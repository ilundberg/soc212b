---
title: "Algorithms for prediction"
format: 
  html:
    fig-height: 3
---

For two sessions, we will cover a few algorithms for prediction. We aim to

* draw connections between algorithms often viewed as classical statistics (e.g., multilevel models) and those often viewed as machine learning (e.g., penalized regression)
* by learning a few new methods, become comfortable with the skills to read about additional new methods on your own in the future

## Data: Baseball salaries {#sec-baseball}

We will explore algorithms for prediction through a simple example dataset. The data contain the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by [USA Today](https://databases.usatoday.com/major-league-baseball-salaries-2023/). After scraping the data and selecting a few variables, I appended each team's win-loss record from 2022. The data are available in [baseball_population.csv](data/baseball_population.csv).

```{r, message = F, warning = F, comment = F}
library(tidyverse)
library(scales)
library(foreach)
```

```{r, echo = F}
label_millions <- scales::label_currency(
  scale = 1e-6, accuracy = .1, suffix = " million"
)
```

```{r, echo = F, message = F, comment = F, warning = F}
# Create data
baseball_population <- read_csv("data/baseball.csv") |>
  left_join(
    read_csv("data/baseball_with_record.csv") |>
      select(team, team_past_record = record) |>
      distinct(),
    by = join_by(team)
  ) |>
  select(player, salary, position, team, team_past_record) |>
  write_csv("data/baseball_population.csv")
```

```{r, eval = F}
baseball_population <- read_csv("https://ilundberg.github.io/soc212b/data/baseball_population.csv")
```

The first rows of the data are depicted below. Each row is a player. The `player` Madison Bumgarner had a `salary` of `r scales::label_dollar()(baseball_population$salary[1])`. His `position` was `LHP` for left-handed pitcher. His `team` was `Arizona`, and this team's record in the previous season was 0.457, meaning that they won 45.7\% of their games.

We will summarize mean salaries. Some useful facts about mean salaries are that they vary substantially across positions and also across teams.

```{r, echo = F}
baseball_population |>
  group_by(position) |>
  summarize(salary = mean(salary)) |>
  mutate(position = fct_reorder(position, -salary)) |>
  ggplot(aes(x = position, y = salary)) +
  geom_bar(stat = "identity") +
  geom_text(
    aes(
      label = scales::label_currency(
        scale = 1e-6, accuracy = .1, suffix = " m"
      )(salary)
    ), 
    y = 0, color = "white", size = 3, fontface = "bold",
    vjust = -1) +
  scale_y_continuous(
    name = "Average Salary",
    labels = scales::label_currency(scale = 1e-6, accuracy = 1, suffix = " million")
  ) +
  scale_x_discrete(
    name = "Position",
    labels = function(x) {
      case_when(
        x == "C" ~ "C\nCatcher",
        x == "RHP" ~ "RHP\nRight-\nHanded\nPitcher",
        x == "LHP" ~ "LHP\nLeft-\nHanded\nPitcher",
        x == "1B" ~ "1B\nFirst\nBase",
        x == "2B" ~ "2B\nSecond\nBase",
        x == "SS" ~ "SS\nShortstop",
        x == "3B" ~ "3B\nThird\nBase",
        x == "OF" ~ "OF\nOutfielder",
        x == "DH" ~ "DH\nDesignated\nHitter"
      )
    }
  ) +
  theme(axis.text.x = element_text(size = 7)) +
  ggtitle("Baseball salaries vary across positions")
```

```{r, echo = F}
baseball_population |>
  group_by(team) |>
  summarize(
    salary = mean(salary),
    team_past_record = unique(team_past_record)
  ) |>
  ggplot(aes(x = team_past_record, y = salary)) +
  geom_point() +
  ggrepel::geom_text_repel(
    aes(label = team),
    size = 2
  ) +
  scale_x_continuous(
    name = "Team Past Record: Proportion Wins in 2022"
  ) +
  scale_y_continuous(
    name = "Team Average Salary in 2023",
    labels = scales::label_currency(scale = 1e-6, accuracy = 1, suffix = " million")
  ) +
  ggtitle("Baseball salaries vary across teams")
```

Each team has about 30 players. As a task, we will often focus on estimating the mean salary of the L.A. Dodgers. Because we have the full population of data, we can calculate the answer directly:

```{r}
true_dodger_mean <- baseball_population |>
  # Restrict to the Dodgers
  filter(team == "L.A. Dodgers") |>
  # Record the mean salary
  summarize(mean_salary = mean(salary)) |>
  # Pull that estimate out of the data frame to just be a number
  pull(mean_salary)
```

The true Dodger mean salary on Opening Day 2023 was `r scales::label_currency()(true_dodger_mean)`. We will imagine that we don't know this number. Instead of having the full population, we will imagine we have

* information on predictors for all players: position, team, team past record
* information on salary for a random sample of 5 players per team

The function `draw_sample` will draw one such sample.

```{r, echo = F}
set.seed(90095)
```
```{r}
baseball_sample <- baseball_population |>
  group_by(team) |>
  slice_sample(n = 5) |>
  ungroup()
```

```{r, echo = F}
dodger_sample_mean <- baseball_sample |> 
  filter(team == "L.A. Dodgers") |>
  summarize(salary = mean(salary)) |>
  pull(salary)
```

In our sample, we observe the salaries of 5 players per team. Our 5 sampled Dodger players have an average salary of `r label_dollar()(dodger_sample_mean)`.

Using our sample of 5 players per team, our task is to predict the mean salary across all `r sum(baseball_population$team == "L.A. Dodgers")` Dodger players in 2023.

```{r}
dodgers_to_predict <- baseball_population |>
  filter(team == "L.A. Dodgers") |>
  select(-salary)
```

<!-- ## James-Stein estimator -->

<!-- ```{r} -->

<!-- ``` -->

<!-- $$ -->
<!-- \hat\mu_i = \bar{Y} + \hat{B}\left(Y_i - \bar{Y}\right) -->
<!-- $$ -->
<!-- where  -->
<!-- $$ -->
<!-- \hat{B} = 1 - \frac{n - 3}{\sum_{i=1}^n(Y_i-\bar{Y})^2} -->
<!-- $$ -->


<!-- ```{r} -->
<!-- repeated_simulations <- foreach(rep = 1:100, .combine = "rbind") %do% { -->

<!--   for_shrinkage <- baseball_population |> -->
<!--     group_by(team) |> -->
<!--     slice_sample(n = 5) |> -->
<!--     group_by(team) |> -->
<!--     summarize(within_mean = mean(salary)) |> -->
<!--     # Calculate grand mean. Works since equal size groups. -->
<!--     mutate(grand_mean = mean(within_mean)) -->

<!--   pooled <- foreach(within_weight_value = seq(0,1,.05), .combine = "rbind") %do% { -->
<!--     for_shrinkage |> -->
<!--       mutate(within_weight = within_weight_value) |> -->
<!--       mutate( -->
<!--         estimate = within_mean * within_weight +  -->
<!--           grand_mean * (1 - within_weight) -->
<!--       ) |> -->
<!--       select(team, within_weight, estimate) -->
<!--   } -->

<!--   return(pooled) -->

<!-- } -->

<!-- truth <- baseball_population |> -->
<!--   group_by(team) |> -->
<!--   summarize(truth = mean(salary)) -->

<!-- # Determine the optimal shrinkage -->
<!-- best_within_weight <- baseball_population |> -->
<!--   group_by(team) |> -->
<!--   summarize( -->
<!--     within_variance = var(salary) / n(), -->
<!--     between_mean = mean(salary) -->
<!--   ) |> -->
<!--   ungroup() |> -->
<!--   summarize(within_variance = mean(within_variance), -->
<!--             between_variance = var(between_mean) / n()) |> -->
<!--   mutate( -->
<!--     grand_weight = within_variance / (within_variance + between_variance), -->
<!--     best_within_weight = 1 - grand_weight -->
<!--   ) |> -->
<!--   pull(best_within_weight) -->

<!-- repeated_simulations |> -->
<!--   left_join(truth, by = "team") |> -->
<!--   group_by(within_weight) |> -->
<!--   summarize(mse = mean((estimate - truth) ^ 2)) |> -->
<!--   ggplot(aes(x = within_weight, y = mse)) + -->
<!--   geom_point() + -->
<!--   geom_line() + -->
<!--   geom_vline(xintercept = best_within_weight) -->
<!-- ``` -->

## Ordinary Least Squares

To walk through the steps of our prediction task, we first consider Ordinary Least Squares. After walking through these steps, we will consider a series of more advanced algorithms for prediction that involve similar steps.

For OLS, we might model `salary` as a function of `team_past_record`. The code below learns this model in our sample.

```{r}
ols <- lm(
  salary ~ team_past_record,
  data = baseball_sample
)
```

We can then make a prediction for every player on the Dodgers. Because our only predictor is a team-level predictor (`team_past_record`), the prediction will be the same for every player. But this may not always be the case, as further down the page when we consider `position` as an additional predictor.

```{r}
ols_predicted <- dodgers_to_predict |>
  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |>
  print(n = 3)
```

Finally, we can average over these predictions to estimate the mean salary on the Dodgers.

```{r}
ols_estimate <- ols_predicted |>
  summarize(ols_estimate = mean(predicted_salary))
```

By OLS prediction, we estimate that the mean Dodger salary was `r label_millions(ols_estimate$ols_estimate)`. Because we estimated in a sample and under some modeling assumptions, this is a bit `r ifelse(ols_estimate$ols_estimate > true_dodger_mean, "higher", "lower")` than the true population mean of `r label_millions(true_dodger_mean)`.

### Performance over repeated samples

Because this is a hypothetical setting, we can consider the performance of our estimator across repeated samples. The chunk below pulls our code into a single function that we call `estimator()`. The estimator takes a sample and returns an estimate.

```{r}
ols_estimator <- function(
    sample = baseball_sample, 
    to_predict = dodgers_to_predict
) {
  # Learn a model in the sample
  ols <- lm(
    salary ~ team_past_record,
    data = sample
  )
  # Predict for our target population
  ols_predicted <- to_predict |>
    mutate(predicted_salary = predict(ols, newdata = to_predict))
  # Average over the target population
  ols_estimate_star <- ols_predicted |>
    summarize(ols_estimate = mean(predicted_salary)) |>
    pull(ols_estimate)
  # Return the estimate
  return(ols_estimate_star)
}
```

We can run the estimator repeatedly, getting one estimate for each repeated sample from the population. This exercise is possible because in this simplified setting we have data on the full population.

```{r}
many_sample_estimates <- foreach(
  repetition = 1:100, .combine = "c"
) %do% {
  # Draw a sample from the population
  baseball_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  # Apply the estimator to the sample
  estimate <- ols_estimator(baseball_sample)
  return(estimate)
}
```

Then we can visualize the performance across repeated samples.

```{r, echo = F, fig.height = 2.5}
tibble(y = many_sample_estimates) |>
  # Random jitter for x
  mutate(x = runif(n(), -.1, .1)) |>
  ggplot(aes(x = x, y = many_sample_estimates)) +
  geom_point() +
  scale_y_continuous(
    name = "Dodger Mean Salary",
    labels = label_millions
  ) +
  theme_minimal() +
  scale_x_continuous(
    breaks = NULL, limits = c(-.5,.5),
    name = "Each dot is an OLS estimate\nin one sample from the population"
  ) +
  geom_hline(yintercept = true_dodger_mean, linetype = "dashed") +
  annotate(geom = "text", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = "True mean in\nDodger subpopulation", size = 3)
```

Across repeated samples, the estimates have a standard deviation of `r label_millions(sd(many_sample_estimates))` and are on average `r label_millions(abs(mean(many_sample_estimates) - true_dodger_mean))` too `r ifelse(mean(many_sample_estimates) > true_dodger_mean,"high","low")`.

### Models with more parameters

An OLS model for salary that is linear in the team past record clearly suffers from model approximation error. If you fit a regression line to the entire population of baseball players you would see that th Dodger's mean salary is below this line.

```{r, echo = F}
population_ols <- lm(salary ~ team_past_record, data = baseball_population)
forplot <- baseball_population |>
  mutate(fitted = predict(population_ols)) |>
  group_by(team) |>
  summarize(
    truth = mean(salary),
    fitted = unique(fitted),
    team_past_record = unique(team_past_record)
  )
forplot_dodgers <- forplot |> filter(team == "L.A. Dodgers")
forplot |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = truth, color = team == "L.A. Dodgers")) +
  geom_line(aes(y = fitted)) +
  geom_segment(
    data = forplot_dodgers,
    aes(
      yend = fitted - 3e5, y = truth + 3e5,
    ), 
    arrow = arrow(length = unit(.05,"in"), ends = "both"), color = "dodgerblue"
  ) +
  geom_text(
    data = forplot_dodgers,
    aes(
      x = team_past_record + .02, 
      y = .5 * (fitted + truth), 
      label = "model\napproximation\nerror"
    ),
    size = 2, color = "dodgerblue", hjust = 0
  ) +
  geom_text(
    data = forplot_dodgers, 
    aes(y = truth, label = "L.A.\nDodgers"),
    color = "dodgerblue",
    vjust = 1.8, size = 2
  ) +
  scale_color_manual(
    values = c("gray","dodgerblue")
  ) +
  scale_y_continuous(name = "Team Mean Salary in 2023", labels = label_millions) +
  scale_x_continuous(limits = c(.3,.8), name = "Team Past Record: Proportion Wins in 2022") +
  theme_classic() +
  theme(legend.position = "none")
```

How can we solve model approximation error? One might replace the linear term `team_past_record` with a series of categories for `team` in the OLS model.

```{r}
ols_team_categories <- lm(
  salary ~ team,
  data = baseball_sample
)
```

But because the sample contains only 5 players per team, these estimates are quite noisy.

```{r, echo = F}
# Create many sample estimates with categorical teams
many_sample_estimates_categories <- foreach(
  repetition = 1:100, .combine = "c"
) %do% {
  # Draw a sample from the population
  baseball_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  # Apply the estimator to the sample# Learn a model in the sample
  ols <- lm(
    salary ~ team,
    data = baseball_sample
  )
  # Predict for our target population
  ols_predicted <- dodgers_to_predict |>
    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))
  # Average over the target population
  ols_estimate <- ols_predicted |>
    summarize(ols_estimate = mean(predicted_salary)) |>
    pull(ols_estimate)
  # Return the estimate
  return(ols_estimate)
}
# Visualize
tibble(x = "OLS linear in\nteam past record", y = many_sample_estimates) |>
  bind_rows(
    tibble(x = "OLS with categorical\nteam indicators", y = many_sample_estimates_categories)
  ) |>
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = .2, height = 0) +
  scale_y_continuous(
    name = "Dodger Mean Salary",
    labels = label_millions
  ) +
  theme_minimal() +
  scale_x_discrete(
    name = "Estimator"
  ) +
  ggtitle(NULL, subtitle = "Each dot is an estimate in one sample from the population") +
  geom_hline(yintercept = true_dodger_mean, linetype = "dashed") +
  annotate(geom = "text", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = "Truth in\npopulation", size = 3)
```

In broad strokes, this is the problem that algorithms for prediction seek to solve:

* avoid bias from model approximation error
* avoid a high-variance estimator

We need a model that strikes a balance between bias and variance.

## A big idea: Regularization

We might want a middle ground between two extreme possibilities:

* our OLS linear prediction: $\hat{Y}^\text{Linear}_\text{Dodgers} = \hat\alpha + \hat\beta\text{(Past Record)}_\text{Dodgers}={}$ `r label_millions(ols_estimate)`
* the mean of the 5 sampled Dodger salaries: $\hat{Y}^\text{Nonparametric}_\text{Dodgers} = {}$ `r label_millions(dodger_sample_mean)`

Our estimator could be a weighted average of these two, with weight $w$ placed on the nonparametric mean and weight $1 - w$ placed on the linear prediction.
$$
\hat\tau = w \hat{Y}^\text{Nonparametric}_\text{Dodgers} + (1 - w)\hat{Y}^\text{Linear}_\text{Dodgers}
$$
The graph below visualizes the resulting estimate at various values of $w$.

```{r, echo = F, warning = F, comment = F}
estimates_by_w <- foreach(w_value = seq(0,1,.05), .combine = "rbind") %do% {
  tibble(
    w = w_value,
    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate
  )
}
estimates_by_w |>
  ggplot(aes(x = w, y = estimate)) +
  geom_point() +
  geom_line() +
  geom_text(
    data = estimates_by_w |>
      filter(w %in% c(0,1)) |>
      mutate(
        w = w + c(1,-1) * .13, 
        hjust = c(0,1),
        label = c("Nonparametric Dodger Samle Mean",
                  "Linear Prediction from OLS")
      ),
    aes(label = label, hjust = hjust)
  ) +
  geom_segment(
     data = estimates_by_w |>
      filter(w %in% c(0,1)) |>
       mutate(x = w + c(1,-1) * .12,
              xend = w + c(1,-1) * .04),
     aes(x = x, xend = xend),
     arrow = arrow(length = unit(.08,"in"))
  ) +
  annotate(
    geom = "text", x = .3, y = estimates_by_w$estimate[12],
    label = "Partial\nPooling\nEstimates",
    vjust = 1
  ) +
  annotate(
    geom = "segment", 
    x = c(.3,.4), 
    xend = c(.3,.55),
    y = estimates_by_w$estimate[c(11,14)],
    yend = estimates_by_w$estimate[c(8,14)],
    arrow = arrow(length = unit(.08,"in"))
  ) +
  scale_x_continuous("Weight: Amount of Shrinkage Toward Linear Fit") +
  scale_y_continuous(
    labels = label_millions,
    name = "Dodger Mean Salary Estimates",
    expand = expansion(mult =.1)
  ) +
  theme_minimal()
```

Partial pooling allows one to avoid choosing an extreme: we can choose somewhere in between estimating by the mean among the sampled Dodgers and estimating by the linear model. The best estimator in fact lies somewhere in between. Let $\hat\mu_\text{Dodgers}$ be an estimator: a function that when applied to a sample $S$ returns an estimate $\hat\mu_\text{Dodgers}(S)$. The squared error of this estimator in a particular sample $S$ is $(\hat\mu_\text{Dodgers}(S) - \mu_\text{Dodgers})^2$. Expected squared error is the expected value of this performance taken across repeated samples $S$ from the population.

$$
\text{Expected Squared Error}(\hat\mu_\text{Dodgers}) = \text{E}_S\left(\left(\hat\mu_\text{Dodgers}(S) - \mu_\text{Dodgers}\right)^2\right)
$$
Below, we visualize a simulation-based estimate of expected squared error. For each of $r = 1,\dots,100$ repetitions, we draw a sample $S_r$ from the population, apply the estimators, and record squared error. We then take the average across simulated samples

$$
\widehat{\text{Expected Squared Error}}(\hat\mu_\text{Dodgers}) = \frac{1}{100}\sum_{r=1}^{100}\left(\hat\mu_\text{Dodgers}(S_r) - \mu_\text{Dodgers}\right)^2
$$


To approximate the expected squared error, we repeatedly take samples of 5 players per MLB team and apply the estimator with each weight value from 0 to 1. Across repeated samples, we estimate the expected squared error of the estimator for the Dodger salaries. Results are visualized below.

```{r, echo = F}
repeated_simulations <- foreach(rep = 1:100, .combine = "rbind") %do% {
  
  a_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  
  ols_fit <- lm(salary ~ team_past_record, data = a_sample)
  
  ols_estimate <- predict(
    ols_fit, 
    newdata = baseball_population |> 
      filter(team == "L.A. Dodgers") |>
      distinct(team_past_record)
  )
  
  nonparametric_estimate <- a_sample |>
    filter(team == "L.A. Dodgers") |>
    summarize(salary = mean(salary)) |>
    pull(salary)
  
  foreach(w_value = seq(0,1,.05), .combine = "rbind") %do% {
    tibble(
      w = w_value,
      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate
    )
  }
}

aggregated <- repeated_simulations |>
  group_by(w) |>
  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |>
  mutate(best = mse == min(mse))

aggregated |>
  ggplot(aes(x = w, y = mse, color = best)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(name = "Expected Squared Error\nfor Dodger Mean Salary") +
  scale_x_continuous(name = "Weight: Amount of Shrinkage Toward Linear Fit") +
  scale_color_manual(values = c("black","dodgerblue")) +
  geom_vline(xintercept = c(0,1), linetype = "dashed") +
  theme_minimal() +
  theme(legend.position = "none") +
  annotate(
    geom = "text", 
    x = c(0.02,.98), 
    y = range(aggregated$mse),
    hjust = c(0,1), vjust = c(0,1),
    size = 3,
    label = c(
      "Nonparametric estimator:\nDodger mean salary",
      "Model-based estimator:\nOLS linear prediction"
    )
  ) +
  annotate(
    geom = "text", 
    x = aggregated$w[aggregated$best], 
    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),
    vjust = -.1,
    label = "Best-Performing\nEstimator",
    size = 3,
    color = "dodgerblue"
  ) +
  annotate(
    geom = "segment",
    x = aggregated$w[aggregated$best], 
    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),
    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),
    arrow = arrow(length = unit(.08,"in")),
    color = "dodgerblue"
  )
```

In this illustration, the best performance is an estimator that puts `r label_percent()(aggregated$w[aggregated$best])` of the weight on the linear fit and `r label_percent()(1 - aggregated$w[aggregated$best])` of the weight on the mean among the 5 sampled Dodgers.

The example above illustrates an idea known as regularization, shrinkage, or partial pooling: we may often want to combine an estimate on a subgroup (the Dodger mean) with an estimate made on the full population (the linear fit). We will consider various methods to accomplish regularization, and we will return at the end to consider connections among them.

## Multilevel models

Multilevel models^[Raudenbush, S. W., and A.S. Bryk. (2002). Hierarchical linear models: Applications and data analysis methods. Advanced Quantitative Techniques in the Social Sciences Series/SAGE.] are an algorithm for prediction that is fully grounded in classical statistics. They are especially powerful for the problem depicted above: making predictions when there are many groups (teams) with a small sample size in each group.

We will first illustrate a multilevel model's performance and then consider the statistics behind this model. We will estimate using the `lme4` package. If you don't have this package, install it with `install.packages("lme4")`.

```{r, message = F, comment = F, warning = F}
library(lme4)
```

In the syntax, the code `(1 | team)` says that our model should have a unique intercept for every team, and that these intercepts should be regularized (more on this soon).
```{r}
multilevel <- lmer(salary ~ team_past_record + (1 | team), data = baseball_sample)
```

We can make predictions from a multilevel model just like we can from OLS. For example, the code below makes predictions for the Dodgers.

```{r}
multilevel_predicted <- dodgers_to_predict |>
  mutate(
    fitted = predict(multilevel, newdata = dodgers_to_predict)
  )
```

### Intuition

The multilevel model is a **partial-pooling estimator**. The figure below displays this visually. For each team, the solid dot is the mean salary among the 5 sampled players. The ends of the arrows are the multilevel model estimates. The multilevel model pools the team-specific estimates toward the model-based prediction.

```{r, echo = F}
p <- baseball_sample |>
  group_by(team) |>
  mutate(nonparametric = mean(salary)) |>
  ungroup() |>
  mutate(
    fitted = predict(multilevel)
  ) |>
  distinct(team, team_past_record, fitted, nonparametric) |>
  ungroup() |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = nonparametric)) +
  geom_abline(
    intercept = fixef(multilevel)[1], 
    slope = fixef(multilevel)[2], 
    linetype = "dashed"
  ) +
  geom_segment(
    aes(y = nonparametric, yend = fitted),
    arrow = arrow(length = unit(.05,"in"))
  ) +
  geom_point(aes(y = nonparametric)) +
  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +
  theme_minimal() +
  scale_y_continuous(name = "Team Mean Salary in 2023", labels = label_millions) +
  scale_x_continuous(name = "Team Past Record: Proportion Wins in 2022") +
  ggtitle("Multilevel model on a sample of 5 players per team",
          subtitle = "Points are nonparametric sample mean estimates.\nArrow ends are multilevel model estimates.\nDashed line is an unregularized fixed effect.")
p
```

The multilevel model only regularizes the team-specific estimates to the degree that they are imprecise. If we repeat the entire process on a sample of 20 players per team, each team-specific estimate becomes more precise and the overall amount of shrinkage is less.

```{r, echo = F}
bigger_sample <- baseball_population |>
  group_by(team) |>
  slice_sample(n = 20) |>
  ungroup()
multilevel_big <- lmer(formula(multilevel), data = bigger_sample)
bigger_sample |>
  group_by(team) |>
  mutate(nonparametric = mean(salary)) |>
  ungroup() |>
  mutate(
    fitted = predict(multilevel_big)
  ) |>
  distinct(team, team_past_record, fitted, nonparametric) |>
  ungroup() |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = nonparametric)) +
  geom_abline(
    intercept = fixef(multilevel)[1], 
    slope = fixef(multilevel)[2], 
    linetype = "dashed"
  ) +
  geom_segment(
    aes(y = nonparametric, yend = fitted),
    arrow = arrow(length = unit(.05,"in"))
  ) +
  geom_point(aes(y = nonparametric)) +
  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +
  theme_minimal() +
  scale_y_continuous(
    name = "Team Mean Salary in 2023", 
    labels = label_millions,
    limits = layer_scales(p)$y$range$range
  ) +
  scale_x_continuous(name = "Team Past Record: Proportion Wins in 2022") +
  ggtitle("Multilevel model on a sample of 20 players per team",
          subtitle = "Points are nonparametric sample mean estimates.\nArrow ends are multilevel model estimates.\nDashed line is an unregularized fixed effect.")
```

### Performance over repeated samples

We previously discussed how an OLS prediction that was linear in the past team record was a biased estimator with low variance. The sample mean within each team was an unbiased estimator with high variance. The multilevel model falls in between these two extremes.

```{r, echo = F, comment = F, warning = F, message = F}
many_sample_estimates_multilevel <- foreach(
  repetition = 1:100, .combine = "c"
) %do% {
  # Draw a sample from the population
  baseball_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  # Apply the estimator to the sample# Learn a model in the sample
  multilevel <- lmer(
    salary ~ team_past_record + (1 | team),
    data = baseball_sample
  )
  # Predict for our target population
  mutilevel_predicted <- dodgers_to_predict |>
    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))
  # Average over the target population
  mutilevel_estimate <- mutilevel_predicted |>
    summarize(mutilevel_estimate = mean(predicted_salary)) |>
    pull(mutilevel_estimate)
  # Return the estimate
  return(mutilevel_estimate)
}
# Visualize
tibble(x = "OLS with\nlinear record", y = many_sample_estimates) |>
  bind_rows(
    tibble(x = "OLS with\nteam indicators", y = many_sample_estimates_categories)
  ) |>
  bind_rows(
    tibble(x = "Multilevel\nmodel", y = many_sample_estimates_multilevel)
  ) |>
  ggplot(aes(x = x, y = y)) +
  geom_jitter(width = .2, height = 0) +
  scale_y_continuous(
    name = "Dodger Mean Salary",
    labels = label_millions
  ) +
  theme_minimal() +
  scale_x_discrete(
    name = "Estimator"
  ) +
  ggtitle(NULL, subtitle = "Each dot is an estimate in one sample from the population") +
  geom_hline(yintercept = true_dodger_mean, linetype = "dashed") +
  annotate(geom = "text", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = "Truth in\npopulation", size = 3)
```

### In math

Mathematically, a multilevel model is a maximum likelihood estimator. For our case, the model assumes that the salary of player $i$ on team $t$ is assumed to be normally distributed around the team mean salary $\mu_t$, with variance $\sigma^2$ which in our case is assumed to be the same across teams.

$$Y_{ti} \sim \text{Normal}\left(\mu_t, \sigma^2\right)$$
The team-specific mean $\mu_t$ involves two components. First, this mean is assumed to be centered at a linear prediction $\alpha + X_t\beta$ where $X_t$ is the win-loss record of team $t$ in the previous year. This is the value toward which team-specific estimates are regularized. Second, the mean for the particular team $i$ is drawn from a normal distribution with standard deviation $\tau^2$, which is the standard deviation of the team-specific mean salary residuals across teams.

$$\mu_t \sim \text{Normal}(\alpha + X_t\beta, \tau^2)$$
By maximizing the log likelihood of the observed data under this model, one comes to maximum likelihood estimates of all of the unknown parameters. The $\mu_t$ estimates will partially pool between two estimators,

1. the sample mean $\bar{Y}_t$ within team $t$
2. the linear model prediction $\hat\alpha + X_t\hat\beta$

where the weight on (1) will depend on the relative precision of this within-team estimate and the weight (2) will depend on the relative precision of the between-team estimate. After explaining ridge regression, we will return to a simplified case where the formula for the multilevel model estimates allows further intuition building.

<!-- The estimates involve partial pooling between two components -->

<!-- $$ -->
<!-- \hat\mu_t^\text{Multilevel} = \frac{\sum_{i}Y_{ti} + \frac{\hat\sigma^2}{\hat\tau^2}\left(\hat\alpha + X_t\hat\beta\right)}{n_t + \frac{\hat\sigma^2}{\hat\tau^2}} -->
<!-- $$ -->

## Ridge regression

While multilevel models are often approached from the standpoint of classical statistics, they are very similar to another approach commonly approached from the standpoint of data science: ridge regression.

### Intuition

Consider our sample of 30 baseball teams with 5 players per team. We might want to fit a linear regression model as follows,

$$
Y_{ij} = \alpha + \beta X_i + \gamma_{i} + \epsilon_{ij}
$$
where $Y_{ij}$ is the salary of player $i$ on team $j$ and $X_i$ is the past win-loss record of that team. In this model, $\gamma_i$ is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term $\epsilon_ij$ is the deviation for player $j$ from their own team's average salary.

The problem with this model is its high variance: with 30 teams, there are 30 different values of $\gamma_i$ to be estimated. And there are only 5 players per team! We might believe that $\gamma_i$ values will generally be small, so we might want to estimate by **penalizing** large values of $\gamma_i$.

We can penalize large values of $\gamma_i$ by an estimator written as it might be written in a data science course:

$$
\{\hat\alpha, \hat\beta, \hat{\vec\gamma}\} = \underset{\alpha,\beta,\vec\gamma}{\text{arg min}} \left[\sum_i\sum_j\left(Y_{ij} - \left(\alpha + \beta X_i + \gamma_{i}\right)\right)^2 + \lambda\sum_i\gamma_i^2\right]
$$

where the estimated values $\{\hat\alpha, \hat\beta, \hat{\vec\gamma}\}$ are chosen to minimize a **loss function** which is the sum over the data of the squared prediction error plus a penalty on large values of $\gamma_i$.

### In code

```{r, comment = F, message = F, warning = F}
library(glmnet)
```

```{r}
ridge <- glmnet(
  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),
  y = baseball_sample$salary,
  penalty.factor = c(0,rep(1,30)),
  # Choose the ridge penalty (alpha = 0).
  # Later, we will learn about the LASSO penalty (alpha = 1)
  alpha = 0
)
```

We can visualize the ridge regression estimates just like the multilevel model estimates. Because the penalty applies to squared values of team deviations from the line, the points furthest from the line are most strongly regularized toward the line.

```{r, echo = F}
baseball_sample |>
  group_by(team) |>
  mutate(nonparametric = mean(salary)) |>
  ungroup() |>
  mutate(
    fitted = predict(
      ridge, 
      s = ridge$lambda[50],
      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
    )
  ) |>
  distinct(team, team_past_record, fitted, nonparametric) |>
  ungroup() |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = nonparametric)) +
  geom_abline(
    intercept = coef(ridge, s = ridge$lambda[20])[1], 
    slope = coef(ridge, s = ridge$lambda[20])[2], 
    linetype = "dashed"
  ) +
  geom_segment(
    aes(y = nonparametric, yend = fitted),
    arrow = arrow(length = unit(.05,"in"))
  ) +
  geom_point(aes(y = nonparametric)) +
  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +
  theme_minimal() +
  scale_y_continuous(name = "Team Mean Salary in 2023", labels = label_millions) +
  scale_x_continuous(name = "Team Past Record: Proportion Wins in 2022") +
  ggtitle("Ridge regression on a sample of 5 players per team",
          subtitle = "Points are nonparametric sample mean estimates.\nArrow ends are ridge regression estimates.\nDashed line is the unregularized component of the model.")
```

The amount of regularization will depend on the chosen value of the penalty parameter $\lambda$. To the degree that $\lambda$ is large, estimates will be more strongly pulled toward the regression line. Below is a visualization at three values of $\lambda$.

```{r, echo = F}
fitted <- predict(
  ridge, 
  s = ridge$lambda[c(20,60,80)],
  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
)
colnames(fitted) <- c("Large Lambda Value","Medium Lambda Value","Small Lambda Value")
baseball_sample |>
  group_by(team) |>
  mutate(nonparametric = mean(salary)) |>
  ungroup() |>
  bind_cols(fitted) |>
  select(team, team_past_record, nonparametric, contains("Lambda")) |>
  pivot_longer(cols = contains('Lambda')) |>
  distinct() |>
  mutate(name = fct_rev(name)) |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = nonparametric), size = .8) +
  geom_segment(
    aes(y = nonparametric, yend = value),
    arrow = arrow(length = unit(.04,"in"))
  ) +
  facet_wrap(~name) +
  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +
  theme_minimal() +
  scale_y_continuous(name = "Team Mean Salary in 2023", labels = label_millions) +
  scale_x_continuous(name = "Team Past Record: Proportion Wins in 2022") +
  ggtitle("Ridge regression on a sample of 5 players per team",
          subtitle = "Points are nonparametric sample mean estimates.\nArrow ends are ridge regression estimates.\nDashed line is the unregularized component of the model.")
```

Focusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter $\lambda$.

```{r, echo = F}
fitted <- predict(
  ridge, 
  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
)
dodgers_sample_mean <- baseball_sample |>
  filter(team == "L.A. Dodgers") |>
  summarize(salary = mean(salary)) |>
  pull(salary)
ols_estimate <- predict(
  lm(salary ~ team_past_record, data = baseball_sample),
  newdata = baseball_sample |> filter(team == "L.A. Dodgers") |> slice_head(n = 1)
)
tibble(estimate = fitted[baseball_sample$team == "L.A. Dodgers",][1,],
       lambda = ridge$lambda) |>
  ggplot(aes(x = lambda, y = estimate)) +
  geom_line() +
  scale_y_continuous(
    name = "Dodger Mean Salary Estimate",
    labels = label_millions,
    expand = expansion(mult = .2)
  ) +
  scale_x_continuous(
    name = "Ridge Regression Penalty Term",
    limits = c(0,1e8)
  ) +
  annotate(
    geom = "point", x = 0, y = dodgers_sample_mean
  ) +
  annotate(geom = "segment", x = .85e7, xend = .2e7, y = dodgers_sample_mean,
           arrow = arrow(length = unit(.05,"in"))) +
  annotate(
    geom = "text", x = 1e7, y = dodgers_sample_mean,
    label = "Mean salary of 5 sampled Dodgers",
    hjust = 0, size = 3
  ) +
  geom_hline(yintercept = ols_estimate, linetype = "dashed") +
  annotate(
    geom = "text",
    x = 0, y = ols_estimate, 
    label = "Dashed line = Prediction that does not allow any team-specific deviations", 
    vjust = -.8, size = 3, hjust = 0
  ) +
  theme_minimal()
```

We will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.

### Performance over repeated samples

The ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter $\lambda$ rises. The biase of the estimates also increases as the value of $\lambda$ increases. The optimal value of $\lambda$ is a problem-specific question that requires one to balance the tradeoff between bias and variance.

```{r, echo = F, comment = F, warning = F, message = F}
many_sample_estimates_ridge <- foreach(
  repetition = 1:100, .combine = "rbind"
) %do% {
  # Draw a sample from the population
  baseball_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  # Apply the estimator to the sample# Learn a model in the sample
  ridge <- glmnet(
    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),
    y = baseball_sample$salary,
    penalty.factor = c(0,rep(1,30)),
    alpha = 0
  )
  # Predict for our target population
  fitted <- predict(
    ridge, 
    s = ridge$lambda[c(20,60,80)],
    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
  )
  colnames(fitted) <- c("Large Lambda Value","Medium Lambda Value","Small Lambda Value")
  
  as_tibble(
    as.matrix(fitted[baseball_sample$team == "L.A. Dodgers",][1,]),
    rownames = "estimator"
  ) |>
    rename(estimate = V1)
}
# Visualize
many_sample_estimates_ridge |>
  mutate(estimator = fct_rev(estimator)) |>
  ggplot(aes(x = estimator, y = estimate)) +
  geom_jitter(width = .2, height = 0) +
  scale_y_continuous(
    name = "Dodger Mean Salary",
    labels = label_millions
  ) +
  theme_minimal() +
  scale_x_discrete(
    name = "Estimator"
  ) +
  ggtitle(NULL, subtitle = "Each dot is an estimate in one sample from the population") +
  geom_hline(yintercept = true_dodger_mean, linetype = "dashed") +
  annotate(geom = "text", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = "Truth in\npopulation", size = 3)
```

### Connections: Multilevel model and ridge regression

Multilevel models and ridge regression are closely connected, and they can yield mathematically equivalent estimates in special cases.  Consider again the multilevel model for the salary $Y_{ti}$ of player $i$ on team $t$.

$$
\begin{aligned}
Y_{ti} &\sim \text{Normal}(\mu_t,\sigma^2) \\
\mu_t &\sim \text{Normal}(\mu_0, \tau^2)
\end{aligned}
$$
For simplicity, suppose we already have estimates $\hat\mu_0$, $\hat\tau^2$, and $\hat\sigma^2$. One can show that the multilevel model estimates of $\mu_t$ are:

$$
\hat\mu_t^\text{Multilevel} = \frac{\sum_{i}Y_{ti} + \frac{\hat\sigma^2}{\hat\tau^2}\hat\mu_0}{n_t + \frac{\hat\sigma^2}{\hat\tau^2}}
$$
This formula can be interpreted as analogous to a sample mean, but with some added observations. The first part of the numerator and denominator corresponds to a sample mean: the sum $\sum_i Y_{ti}$ of salaries of all sampled players in team $t$ in the numerator and the number of such players $n_t$ in the denominator. The right side of the numerator and denominator correspond to pseudo-observations. It is as though in addition to the sampled players, we also saw $\frac{\hat\sigma^2}{\hat\tau^2}$ additional players with exactly the overall baseball mean salary estimate $\hat\mu_0$. This part of the formula partially pools the team-specific mean toward this overall mean. Partial pooling is greater to the degree that there is small within-team variance (small $\sigma^2$) or large across-team variance (large $\hat\tau^2$).

Next, we consider a ridge regression estimator that minimizes an objective function where $\hat\mu_0$ is an unpenalized estimate for the baseball-wide mean salary.^[While we write out $\hat\mu_0$, algorithmic implementations of ridge regression often mean-center $Y$ before applying the algorithm which is equivalent to having an unpenalized $\hat\mu_0$.]

$$
\hat{\vec\mu}^\text{Ridge} = \underset{\vec\mu}{\text{arg min}} \underbrace{\sum_t\sum_i \left(Y_{it}-\mu_t\right)^2}_\text{Sum of Squared Error} + \underbrace{\lambda\sum_t \left(\mu_t - \hat\mu_0\right)^2}_\text{Penalty}
$$
We can gain some additional intuition for this estimate by rearranging to pull the penalty into the main term.

$$
\hat{\vec\mu}^\text{Ridge} = \underset{\vec\mu}{\text{arg min}} \sum_t\left(\sum_i \left(Y_{it}-\mu_t\right)^2 + \lambda\left(\hat\mu_0 - \mu_t\right)^2\right)
$$
The first part of this term is the sum over observed squared errors within team $t$, $\sum_i\left(Y_{it}-\mu_t\right)^2$. The second part is as though we had observed an additional $\lambda$ cases within team $t$ with an outcome value $\mu_0$ equal to the baseball-wide mean. With this intuition, the ridge regression estimator becomes

$$
\hat\mu_t^\text{Ridge} = \frac{\sum_i Y_{it} + \lambda\hat\mu_0}{n_t + \lambda}
$$
which is the same as the multilevel model estimator in the special case when $\lambda = \frac{\hat\sigma^2}{\hat\tau^2}$. Multilevel models and ridge regression are actually accomplishing the same thing!

## LASSO regression

LASSO regression is just like ridge regression except for one key difference: instead of penalizing the sum of squared coefficients, LASSO penalizes the sum of the absolute value of coefficients. As we will see, this change means that some parameters become regularized all the way to zero so that some terms drop out of the model completely.

As with ridge regressiom, our outcome model is

$$
Y_{ij} = \alpha + \beta X_i + \gamma_{i} + \epsilon_{ij}
$$
where $Y_{ij}$ is the salary of player $i$ on team $j$ and $X_i$ is the past win-loss record of that team. In this model, $\gamma_i$ is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term $\epsilon_ij$ is the deviation for player $j$ from their own team's average salary.

To solve the high-variance estimates of $\gamma_i$, LASSO regression uses a different penalty term in its loss function:

$$
\{\hat\alpha, \hat\beta, \hat{\vec\gamma}\} = \underset{\alpha,\beta,\vec\gamma}{\text{arg min}} \left[\sum_i\sum_j\left(Y_{ij} - \left(\alpha + \beta X_i + \gamma_{i}\right)\right)^2 + \lambda\sum_i\lvert\gamma_i\rvert\right]
$$

where $\gamma_i^2$ from the ridge regression penalty has been replaced by the absolute value $\lvert\gamma_i\rvert$. We will first apply this in code and then see how it changes the performance of the estimator.

### In code

```{r, comment = F, message = F, warning = F}
library(glmnet)
```

```{r}
lasso <- glmnet(
  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),
  y = baseball_sample$salary,
  penalty.factor = c(0,rep(1,30)),
  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)
  alpha = 1
)
```

Visualizing the estimates, we see behavior similar to what we have previously seen from multilevel models and ridge regression. All estimates are pulled toward a linear regression fit. There are two key differences, however.

First, the previous estimators most strongly pulled the large deviations toward the linear fit. The LASSO estimates pull all parameter estimates toward the linear fit to a similar degree, regardless of their size. This is because the LASSO estimates penalize the absolute value of $\gamma_i$ instead of the squared value of $\gamma_i$.

Second, the multilevel and ridge estimates never pulled any of the estimates all the way to the line; instead, the estimates asymptoted toward the line as the penalty parameter grew. In LASSO, some estimates are pulled all the way to the line.

```{r, echo = F}
baseball_sample |>
  group_by(team) |>
  mutate(nonparametric = mean(salary)) |>
  ungroup() |>
  mutate(
    fitted = predict(
      lasso, 
      s = lasso$lambda[20],
      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
    )
  ) |>
  distinct(team, team_past_record, fitted, nonparametric) |>
  ungroup() |>
  ggplot(aes(x = team_past_record)) +
  geom_point(aes(y = nonparametric)) +
  geom_abline(
    intercept = coef(lasso, s = lasso$lambda[20])[1], 
    slope = coef(lasso, s = lasso$lambda[20])[2], 
    linetype = "dashed"
  ) +
  geom_segment(
    aes(y = nonparametric, yend = fitted),
    arrow = arrow(length = unit(.05,"in"))
  ) +
  geom_point(aes(y = nonparametric)) +
  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +
  theme_minimal() +
  scale_y_continuous(name = "Team Mean Salary in 2023", labels = label_millions) +
  scale_x_continuous(name = "Team Past Record: Proportion Wins in 2022") +
  ggtitle("LASSO regression on a sample of 5 players per team",
          subtitle = "Points are nonparametric sample mean estimates.\nArrow ends are ridge regression estimates.\nDashed line is the unregularized component of the model.")
```

Focusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter $\lambda$. At a very small penalty, the estimate is approximately the same as the mean among the 5 sampled Dodger players. As the penalty parameter $\lambda$ gets larger, the estimates move around. They generally move toward zero, but not always: as some other team-specific deviations are pulled to zero, the unregularized intercept and slope on team past record move around in response. Ultimately, the penalty becomes so large that the Dodger estimate is regularized all the way to the estimate we would get in a model with no team-specific deviations.

```{r, echo = F}
fitted <- predict(
  lasso, 
  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
)
dodgers_sample_mean <- baseball_sample |>
  filter(team == "L.A. Dodgers") |>
  summarize(salary = mean(salary)) |>
  pull(salary)
ols_estimate <- predict(
  lm(salary ~ team_past_record, data = baseball_sample),
  newdata = baseball_sample |> filter(team == "L.A. Dodgers") |> slice_head(n = 1)
)
tibble(estimate = fitted[baseball_sample$team == "L.A. Dodgers",][1,],
       lambda = lasso$lambda) |>
  ggplot(aes(x = lambda, y = estimate)) +
  geom_line() +
  scale_y_continuous(
    name = "Dodger Mean Salary Estimate",
    labels = label_millions,
    expand = expansion(mult = .2)
  ) +
  scale_x_continuous(
    name = "LASSO Regression Penalty Term",
    limits = c(0,2.5e6)
  ) +
  annotate(
    geom = "point", x = 0, y = dodgers_sample_mean
  ) +
  annotate(geom = "segment", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,
           arrow = arrow(length = unit(.05,"in"))) +
  annotate(
    geom = "text", x = 3e5, y = dodgers_sample_mean,
    label = "Mean salary of 5 sampled Dodgers",
    hjust = 0, size = 3
  ) +
  geom_hline(yintercept = ols_estimate, linetype = "dashed") +
  annotate(
    geom = "text",
    x = 0, y = ols_estimate, 
    label = "Dashed line = Prediction that does not allow any team-specific deviations", 
    vjust = -.8, size = 3, hjust = 0
  ) +
  theme_minimal()
```

As with ridge regression, we will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.

### Performance over repeated samples

Similar to the ridge regression estimator, we can visualize the performance of the LASSO regression estimator across repeated samples from the population.

```{r, echo = F, comment = F, warning = F, message = F}
many_sample_estimates_lasso <- foreach(
  repetition = 1:100, .combine = "rbind"
) %do% {
  # Draw a sample from the population
  baseball_sample <- baseball_population |>
    group_by(team) |>
    slice_sample(n = 5) |>
    ungroup()
  # Apply the estimator to the sample# Learn a model in the sample
  lasso <- glmnet(
    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),
    y = baseball_sample$salary,
    penalty.factor = c(0,rep(1,30)),
    alpha = 1
  )
  # Predict for our target population
  fitted <- predict(
    lasso, 
    s = lasso$lambda[c(15,30,45)],
    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)
  )
  colnames(fitted) <- c("Large Lambda Value","Medium Lambda Value","Small Lambda Value")
  
  as_tibble(
    as.matrix(fitted[baseball_sample$team == "L.A. Dodgers",][1,]),
    rownames = "estimator"
  ) |>
    rename(estimate = V1)
}
# Visualize
many_sample_estimates_lasso |>
  mutate(estimator = fct_rev(estimator)) |>
  ggplot(aes(x = estimator, y = estimate)) +
  geom_jitter(width = .2, height = 0) +
  scale_y_continuous(
    name = "Dodger Mean Salary",
    labels = label_millions
  ) +
  theme_minimal() +
  scale_x_discrete(
    name = "Estimator"
  ) +
  ggtitle(NULL, subtitle = "Each dot is an estimate in one sample from the population") +
  geom_hline(yintercept = true_dodger_mean, linetype = "dashed") +
  annotate(geom = "text", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = "Truth in\npopulation", size = 3)
```

## Trees

## Forests

## Gradient boosting