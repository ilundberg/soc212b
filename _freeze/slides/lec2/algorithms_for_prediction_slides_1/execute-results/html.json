{
  "hash": "6ab5d9f8a64d199755df5b447ca2c7d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Algorithms for prediction\"\nsubtitle: \"Part 1 of 2: Penalized Linear Regression\"\nexecute:\n  echo: TRUE\nformat: \n  revealjs:\n    incremental: true\n    html-math-method: mathjax\n---\n\n\n\n## Problem Set 1: Discussion\n\n|Term|Meaning|\n|---|---|\n| Unit of analysis | A row of your data |\n| Unit-specific quantity | Outcome or potential outcome(s) |\n| Target population | Set of units over which to aggregate |\n\n. . .\n\nWhy? To ask $\\hat{Y}$ questions, not $\\hat\\beta$ questions\n\n## Problem Set 1: Discussion\n\nSome language notes:\n\n* Causal language\n     - X verb Y\n* Non-causal language\n     - differences in Y across X\n\n## Problem Set 1: Discussion\n\nMany submitted models like this:\n\n$$E(Y \\mid X) = \\alpha + \\beta X$$\n\n. . .\n\nWhen $X$ is binary, this model is like taking subgroup means.\n\n. . .\n\n$$\n\\begin{aligned}\nE(Y\\mid X = 0) &= \\alpha \\\\\nE(Y\\mid X = 1) &= \\alpha + \\beta\n\\end{aligned}\n$$\n. . .\n\nIt is not really a \"model\" at all\n\n## Problem Set 1: Discussion\n\nOther things to discuss?\n\n## Goals for today\n\n* review OLS to predict\n* understand regularization to estimate\\\n  many parameters while avoiding high variance\n* walk through three regularized linear model algorithms\n     - multilevel models\n     - ridge regression\n     - LASSO regression\n\n# Data\n\n## Data: Baseball salaries\n\n* All 944 Major League Baseball Players, Opening Day 2023.\n* Compiled by [USA Today](https://databases.usatoday.com/major-league-baseball-salaries-2023/)\n* I appended each team's win-loss record from 2022\n* Available in [baseball_population.csv](../../data/baseball_population.csv)\n\n## Data: Baseball salaries\n\nLoad packages:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n```\n:::\n\n\n\n. . .\n\nLoad data:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbaseball_population <- read_csv(\n  \"https://ilundberg.github.io/soc212b/data/baseball_population.csv\"\n)\n```\n:::\n\n\n\n## Data: Baseball salaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbaseball_population |> print(n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 944 × 5\n  player               salary position team    team_past_record\n  <chr>                 <dbl> <chr>    <chr>              <dbl>\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457\n2 Marte, Ketel       11600000 2B       Arizona            0.457\n3 Ahmed, Nick        10375000 SS       Arizona            0.457\n# ℹ 941 more rows\n```\n\n\n:::\n:::\n\n\n\n## Data: Baseball salaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbaseball_population |>\n  group_by(position) |>\n  summarize(salary = mean(salary)) |>\n  mutate(position = fct_reorder(position, -salary)) |>\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across positions\")\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n## Data: Baseball salaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbaseball_population |>\n  group_by(team) |>\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |>\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across teams\")\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n# Task\n\n## Task: Predict Dodger mean salary\n\n. . .\n\nWe have the full population:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_dodger_mean <- baseball_population |>\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |>\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |>\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n```\n:::\n\n\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196.\n\n\n## Task: Predict Dodger mean salary\n\nImagine we have\n\n* predictors for everyone\n* salary for a sample of 5 players per team\n\n## Task: Predict Dodger mean salary\n\n![](dodgers_estimator1.png)\n\n## Task: Predict Dodger mean salary\n\n![](dodgers_estimator2.png)\n\n## Task: Predict Dodger mean salary\n\n![](dodgers_estimator3.png)\n\n## Task: Predict Dodger mean salary\n\n* learn model on everyone\n* create data to predict: the Dodger population\n* predict the outcome in this target population\n* average over units\n\n## Task: Predict Dodger mean salary\n\nDraw a sample of 5 players per team.\n\n. . .\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbaseball_sample <- baseball_population |>\n  group_by(team) |>\n  slice_sample(n = 5) |>\n  ungroup() |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 150 × 5\n   player                 salary position team    team_past_record\n   <chr>                   <dbl> <chr>    <chr>              <dbl>\n 1 Carroll, Corbin       1625000 OF       Arizona            0.457\n 2 Herrera, Jose          724300 C        Arizona            0.457\n 3 Melancon, Mark*       6000000 RHP      Arizona            0.457\n 4 Ahmed, Nick          10375000 SS       Arizona            0.457\n 5 Robinson, Kristian**   720000 OF       Arizona            0.457\n 6 Arcia, Orlando        2300000 SS       Atlanta            0.623\n 7 Lee, Dylan             730000 LHP      Atlanta            0.623\n 8 Pillar, Kevin         3000000 OF       Atlanta            0.623\n 9 Matzek, Tyler*        1200000 LHP      Atlanta            0.623\n10 Hilliard, Sam          750000 OF       Atlanta            0.623\n# ℹ 140 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Task: Predict Dodger mean salary\n\nOur 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  player           salary position team         team_past_record\n  <chr>             <dbl> <chr>    <chr>                   <dbl>\n1 Phillips, Evan  1300000 RHP      L.A. Dodgers            0.685\n2 Miller, Shelby  1500000 RHP      L.A. Dodgers            0.685\n3 Taylor, Chris  15000000 OF       L.A. Dodgers            0.685\n4 Betts, Mookie  21158692 OF       L.A. Dodgers            0.685\n5 Outman, James    722500 OF       L.A. Dodgers            0.685\n```\n\n\n:::\n:::\n\n\n\n## Task: Create data to predict\n\nWe want to predict salary for all Dodger players:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndodgers_to_predict <- baseball_population |>\n  filter(team == \"L.A. Dodgers\") |>\n  select(-salary)\n```\n:::\n\n\n\n# OLS\n\n## Ordinary Least Squares: Model\n\nModel `salary` as a function of `team_past_record`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)\n```\n:::\n\n\n\n## Ordinary Least Squares: Predict\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_predicted <- dodgers_to_predict |>\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |>\n  print(n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 35 × 5\n  player           position team         team_past_record predicted_salary\n  <chr>            <chr>    <chr>                   <dbl>            <dbl>\n1 Freeman, Freddie 1B       L.A. Dodgers            0.685         5552999.\n2 Heyward, Jason   OF       L.A. Dodgers            0.685         5552999.\n3 Betts, Mookie    OF       L.A. Dodgers            0.685         5552999.\n# ℹ 32 more rows\n```\n\n\n:::\n:::\n\n\n\n## Ordinary Least Squares: Average\n\nAverage predictions over the target population\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_estimate <- ols_predicted |>\n  summarize(ols_estimate = mean(predicted_salary)) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  ols_estimate\n         <dbl>\n1     5552999.\n```\n\n\n:::\n:::\n\n\n\n## Performance of OLS\n\n* We usually have only one sample\n* In this case, we have the population\n\n. . .\n\n* Plan\n    * draw repeated samples\n    * evaluate performance over repeated samples\n    \n## Performance of OLS: Estimator function\n\nA function that\n\n* takes in a sample\n* returns an estimate\n\n## Performance of OLS: Estimator function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_estimator <- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols <- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted <- to_predict |>\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star <- ols_predicted |>\n    summarize(ols_estimate = mean(predicted_salary)) |>\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}\n```\n:::\n\n\n\n## Performance of OLS: Estimator function\n\nApply the estimator in repeated samples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmany_sample_estimates <- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  # Apply the estimator to the sample\n  estimate <- ols_estimator(baseball_sample)\n  return(estimate)\n}\n```\n:::\n\n\n\n## Performance of OLS: Estimator function\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(y = many_sample_estimates) |>\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |>\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n# Approximation Error\n\n## Model approximation error\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npopulation_ols <- lm(salary ~ team_past_record, data = baseball_population)\nforplot <- baseball_population |>\n  mutate(fitted = predict(population_ols)) |>\n  group_by(team) |>\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers <- forplot |> filter(team == \"L.A. Dodgers\")\nforplot |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\", text = element_text(size = 18))\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n## Model approximation error\n\nHow to solve model approximation error? \\pause\n\nA model with **more parameters** \\pause\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_team_categories <- lm(\n  salary ~ team,\n  data = baseball_sample\n)\n```\n:::\n\n\n\n\n## OLS with many parameters\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories <- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols <- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted <- dodgers_to_predict |>\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate <- ols_predicted |>\n    summarize(ols_estimate = mean(predicted_salary)) |>\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |>\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n# Regularization\n\n## Regularization\n\nTwo estimators:\n\n* **nonparametric** mean of 5 sampled players on each team\\\n  (high variance)\\\n  \n* **linear prediction**: linear fit on team past record\\\n  (biased by model approximation error)\n  \n. . .\n  \nWe might **regularize** the nonparametric toward the linear prediction\n\n## Regularization\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nestimates_by_w <- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |>\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |>\n      filter(w %in% c(0,1)) |>\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Samle Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |>\n      filter(w %in% c(0,1)) |>\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n## Regularization: How much?\n\n. . .\n\nWant to minimize expected squared error.\n\n$$\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n$$\n\n## Regularization: How much?\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nrepeated_simulations <- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  \n  ols_fit <- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate <- predict(\n    ols_fit, \n    newdata = baseball_population |> \n      filter(team == \"L.A. Dodgers\") |>\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate <- a_sample |>\n    filter(team == \"L.A. Dodgers\") |>\n    summarize(salary = mean(salary)) |>\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated <- repeated_simulations |>\n  group_by(w) |>\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |>\n  mutate(best = mse == min(mse))\n\naggregated |>\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", text = element_text(size = 18)) +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n# Multilevel models\n\n## Multilevel models: Idea\n\n* many groups\n* a few observations per group\n* partially pool between\n     - group-specific mean estimates (high variance)\n     - a model pooled across groups (potentially biased)\n\n## Multilevel models: Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmultilevel <- lmer(\n  salary ~ team_past_record + (1 | team), \n  data = baseball_sample\n)\n```\n:::\n\n\n\nMake predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultilevel_predicted <- dodgers_to_predict |>\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )\n```\n:::\n\n\n\n## Multilevel models: Intuition\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np <- baseball_sample |>\n  group_by(team) |>\n  mutate(nonparametric = mean(salary)) |>\n  ungroup() |>\n  mutate(\n    fitted = predict(multilevel)\n  ) |>\n  distinct(team, team_past_record, fitted, nonparametric) |>\n  ungroup() |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n## Multilevel models: Intuition\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbigger_sample <- baseball_population |>\n  group_by(team) |>\n  slice_sample(n = 20) |>\n  ungroup()\nmultilevel_big <- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |>\n  group_by(team) |>\n  mutate(nonparametric = mean(salary)) |>\n  ungroup() |>\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |>\n  distinct(team, team_past_record, fitted, nonparametric) |>\n  ungroup() |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel_big)[1], \n    slope = fixef(multilevel_big)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n\n## Multilevel models: Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmany_sample_estimates_multilevel <- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel <- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted <- dodgers_to_predict |>\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate <- mutilevel_predicted |>\n    summarize(mutilevel_estimate = mean(predicted_salary)) |>\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |>\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |>\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## Multilevel models: In math\n\n. . .\n\n**Person-level model.** For player $i$ on team $t$,\n\n\\begin{equation}\nY_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\n\\end{equation}\n\n. . .\n\n**Group-level model.** There is a model for $\\mu_t$, mean salary on team $t$.\n\n$$\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)$$\nwhere $X_t$ is team past record\n\n# Ridge regression\n\n## Ridge regression\n\nOften taught in data science classes\n\n* many coefficients\n* penalize large coefficients\n* minimize a loss function\n\n## Ridge regression\n\n$$\nY_{ij} = \\alpha + \\beta X_i + \\gamma_{i} + \\epsilon_{ij}\n$$\n\nMinimize a loss function:\n\n$$\n\\underbrace{\\sum_i\\sum_j\\left(Y_{ij} - \\left(\\alpha + \\beta X_i + \\gamma_{i}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_i\\gamma_i^2}_\\text{Penalty}\n$$\n\n## Ridge regression: Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nridge <- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)\n```\n:::\n\n\n\n## Ridge regression: Estimates\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_ridge <- baseball_sample |>\n  group_by(team) |>\n  mutate(nonparametric = mean(salary)) |>\n  ungroup() |>\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |>\n  distinct(team, team_past_record, fitted, nonparametric) |>\n  ungroup() |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_ridge\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n\n\n## Ridge regression: Penalty parameter $\\lambda$ controls shrinkage\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfitted <- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) <- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |>\n  group_by(team) |>\n  mutate(nonparametric = mean(salary)) |>\n  ungroup() |>\n  bind_cols(fitted) |>\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |>\n  pivot_longer(cols = contains('Lambda')) |>\n  distinct() |>\n  mutate(name = fct_rev(name)) |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n\n## Ridge regression: Penalty parameter $\\lambda$ controls shrinkage\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfitted <- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean <- baseball_sample |>\n  filter(team == \"L.A. Dodgers\") |>\n  summarize(salary = mean(salary)) |>\n  pull(salary)\nols_estimate <- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |> filter(team == \"L.A. Dodgers\") |> slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |>\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n### Performance over repeated samples\n\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter $\\lambda$ rises. The biase of the estimates also increases as the value of $\\lambda$ increases. The optimal value of $\\lambda$ is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmany_sample_estimates_ridge <- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge <- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted <- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) <- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |>\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |>\n  mutate(estimator = fct_rev(estimator)) |>\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\n# Connections: Multilevel model and ridge regression\n\n## Connections: Multilevel model and ridge regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultilevel <- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nyhat_multilevel <- baseball_sample |>\n  mutate(yhat_multilevel = predict(multilevel)) |>\n  distinct(team, yhat_multilevel)\nlambda_equivalent <- as_tibble(VarCorr(multilevel)) |>\n  select(grp, vcov) |>\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |>\n  mutate(lambda_equivalent = Residual / team) |>\n  pull(lambda_equivalent)\n```\n:::\n\n\n\nI will also fit ridge regression with $\\lambda$ set to 9.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nX <- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered <- baseball_sample$salary - mean(baseball_sample$salary)\nbeta_ridge <- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\nyhat_ridge <- beta_ridge + mean(baseball_sample$salary)\n```\n:::\n\n\n\nFinally, we create a `tibble` with both the ridge regression and multilevel model estimates.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nboth_estimators <- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |>\n  rename(team = rowname) |>\n  mutate(team = str_remove(team,\"team\")) |>\n  left_join(yhat_multilevel, by = join_by(team))\n```\n:::\n\n\n\n## Connections: Multilevel model and ridge regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np <- both_estimators |>\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  geom_point(alpha = 0) +\n  theme(text = element_text(size = 18))\np\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-40-1.png){width=960}\n:::\n:::\n\n\n\n## Connections: Multilevel model and ridge regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n\n\n## Connections: Multilevel model and ridge regression\n\nMultilevel model:\n\n$$\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n$$\nEstimates of $\\mu_t$ given $\\hat\\mu_0$, $\\hat\\tau^2$, and $\\hat\\sigma^2$:\n\n$$\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n$$\n\n## Connections: Multilevel model and ridge regression\n\n$$\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n$$\n\nRearranged to:\n\n$$\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n$$\n\n\n## Connections: Multilevel model and ridge regression\n\n\n$$\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n$$\n\n. . .\n\nResult:\n\n$$\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n$$\n\n## Connections: Multilevel model and ridge regression\n\nMultilevel:\n\n$$\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n$$\n\nRidge:\n\n$$\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n$$\n\n# LASSO\n\n## LASSO regression\n\nOutcome model similar to ridge:\n\n$$\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n$$\n\nLoss function penalty has absolute value:\n\n$$\n\\underbrace{\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t\\lvert\\gamma_t\\rvert}_\\text{Penalty}\n$$\n\n## LASSO: In code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso <- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)\n```\n:::\n\n\n\n## LASSO estimates\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_lasso <- baseball_sample |>\n  group_by(team) |>\n  mutate(nonparametric = mean(salary)) |>\n  ungroup() |>\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |>\n  distinct(team, team_past_record, fitted, nonparametric) |>\n  ungroup() |>\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_lasso\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\n\n## LASSO estimates\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n\n\n\n## LASSO estimates\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfitted <- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean <- baseball_sample |>\n  filter(team == \"L.A. Dodgers\") |>\n  summarize(salary = mean(salary)) |>\n  pull(salary)\nols_estimate <- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |> filter(team == \"L.A. Dodgers\") |> slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |>\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-46-1.png){width=960}\n:::\n:::\n\n\n\n## LASSO: Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmany_sample_estimates_lasso <- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample <- baseball_population |>\n    group_by(team) |>\n    slice_sample(n = 5) |>\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso <- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted <- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) <- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |>\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |>\n  mutate(estimator = fct_rev(estimator)) |>\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n```\n\n::: {.cell-output-display}\n![](algorithms_for_prediction_slides_1_files/figure-revealjs/unnamed-chunk-47-1.png){width=960}\n:::\n:::\n\n\n\n# Recap + Project Connection\n\n## Recap\n\nA recipe for prediction:\n\n* model in learning data\n* predict in target population\n* average\n\n## Recap\n\nPerformance (**expected squared error**) can suffer from two sources:\n\n* bias (model approximation error)\n* variance (too many parameters)\n\n. . .\n\n**Regularization** balances these two concerns\n\n## Recap\n\n* OLS: No regularization\n* Ridge regression: Penalizes sum of squared coefficients\n* LASSO regression: Penalizes sum of absolute coefficients\n\n. . .\n\nMultilevel models regularize group-specific estimates depending on the within-group precision, a special case of ridge regression.\n\n## Reading (optional)\n\n[Efron \\& Hastie Ch 7.3](https://hastie.su.domains/CASI/)\n\n## Connection to project\n\nIn your project, do you have a variable that creates\n\n> * many groups with\n> * few observations per group?\n\nWhere else do you have many parameters to estimate?\n\nHow could penalized regression apply in your project?\n\n## Problem Set 2\n\nDue Monday 9pm\n\n",
    "supporting": [
      "algorithms_for_prediction_slides_1_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}