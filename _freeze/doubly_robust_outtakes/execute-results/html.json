{
  "hash": "11a13098466b61757ed86df63dbf0c96",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Doubly-robust estimation\"\n---\n\n\n\n### TMLE with a binary outcome\n\n> **Issue.** AIPW never predicts an ATE outside of the (0,1) range when the treatment weights are correct and we have no random noise. May need to introduce random noise for this to happen. May \n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nA key advantage of targeted learning over augmented inverse pribability weighting is that targeted learning accomodates generalized linear models such as logistic regression for bounded outcomes. To illustrate this property, we now consider a binary outcome: whether the child's day scores at least 7 on the awesomeness scale. We focus here on the average treatment effect.\n\n$$\n\\tau = \\frac{1}{n}\\sum_i \\left(\\mathbb{I}(Y_i^1\\geq 7) - \\mathbb{I}(Y_i^0\\geq 7)\\right)\n$$\n\n### Non-enforced bounds (ATC)\n\nWe first consider two estimators that do not enforce the bounds: the AIPW estimator and the targeted learning estimator using linear regression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_binary <- tibble(x = rep(c(1,2,5), each = 10)) |>\n  mutate(\n    a = c(rep(c(F,rep(T,9)),2),\n          T,rep(F,9))\n  ) |>\n  mutate(y = case_when(\n    !a ~ F,\n    a & x == 1 ~ F,\n    a & x == 2 ~ T,\n    a & x == 4 ~ F,\n    a & x == 5 ~ F\n    ))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- lm(y ~ x, data = data_binary |> filter(a))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\nWhat is the initial estimate of the outcome under treatment?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |> \n  mutate(yhat1 = predict(fit1, newdata = data)) |>\n  filter(!a) |>\n  summarize(yhat0 = mean(y >= 6), yhat1 = mean(yhat1))\n```\n:::\n\n\n\nFor both estimators, we calculate an initial estimate. For streamlined coding in this comparison, we use the notation of targeted learning and first create a `q0()` function for predictions from our initial outcome model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq0 <- function(a,x) {\n  to_predict <- tibble(a,x)\n  to_predict |>\n    mutate(\n      q0 = case_when(\n        a == 1 ~ predict(fit1, newdata = to_predict),\n        a == 0 ~ predict(fit0, newdata = to_predict)\n      )\n    ) |>\n    pull(q0)\n}\n```\n:::\n\n\n\nThen we use this function to create an initial estimate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_estimate <- data |>\n  transmute(\n    yhat1 = q0(1,x),\n    yhat0 = q0(0,x),\n    effect = yhat1 - yhat0\n  ) |>\n  summarize_all(mean) |>\n  print()\n```\n:::\n\n\n\nWe then calculate the AIPW correction using inverse probability weights.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrection <- data |>\n  mutate(\n    yhat = q0(a,x),\n    weight = case_when(\n      a ~ 1 / pi,\n      !a ~ 1 / (1 - pi)\n    )\n  ) |>\n  group_by(a) |>\n  summarize(error = weighted.mean(yhat - I(y >= 3), w = weight)) |>\n  pivot_wider(names_from = \"a\", values_from = \"error\", names_prefix = \"error_\") |>\n  mutate(error_ATE = error_TRUE - error_FALSE) |>\n  select(error_TRUE, error_FALSE, error_ATE) |>\n  print()\n```\n:::\n\n\n\nThe above shows that the model tends to over-predict the probability of awesomeness exceeding 3 if surfing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_estimate |> pull(effect) - \n  (correction |> pull(error_ATE))\n```\n:::\n\n\n\n\n\n### Non-enforced bounds ($Y^1$ and $Y^0$)\n\nWe first consider two estimators that do not enforce the bounds: the AIPW estimator and the targeted learning estimator using linear regression. For these estimators, our initial fits are linear regression models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0 <- lm(I(y >= 4) ~ x, data = data |> filter(!a))\nfit1 <- lm(I(y >= 4) ~ x, data = data |> filter(a))\n```\n:::\n\n\n\nFor both estimators, we calculate an initial estimate. For streamlined coding in this comparison, we use the notation of targeted learning and first create a `q0()` function for predictions from our initial outcome model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq0 <- function(a,x) {\n  to_predict <- tibble(a,x)\n  to_predict |>\n    mutate(\n      q0 = case_when(\n        a == 1 ~ predict(fit1, newdata = to_predict),\n        a == 0 ~ predict(fit0, newdata = to_predict)\n      )\n    ) |>\n    pull(q0)\n}\n```\n:::\n\n\n\nThen we use this function to create an initial estimate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_estimate <- data |>\n  transmute(\n    yhat1 = q0(1,x),\n    yhat0 = q0(0,x),\n    effect = yhat1 - yhat0\n  ) |>\n  summarize_all(mean) |>\n  print()\n```\n:::\n\n\n\nWe then calculate the AIPW correction using inverse probability weights.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrection <- data |>\n  mutate(\n    yhat = q0(a,x),\n    weight = case_when(\n      a ~ 1 / pi,\n      !a ~ 1 / (1 - pi)\n    )\n  ) |>\n  group_by(a) |>\n  summarize(error = weighted.mean(yhat - I(y >= 3), w = weight)) |>\n  pivot_wider(names_from = \"a\", values_from = \"error\", names_prefix = \"error_\") |>\n  mutate(error_ATE = error_TRUE - error_FALSE) |>\n  select(error_TRUE, error_FALSE, error_ATE) |>\n  print()\n```\n:::\n\n\n\nThe above shows that the model tends to over-predict the probability of awesomeness exceeding 3 if surfing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_estimate |> pull(effect) - \n  (correction |> pull(error_ATE))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}