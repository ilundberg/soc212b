[
  {
    "objectID": "algorithms_for_prediction.html",
    "href": "algorithms_for_prediction.html",
    "title": "Algorithms for prediction",
    "section": "",
    "text": "For two sessions, we will cover a few algorithms for prediction. We aim to",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#sec-baseball",
    "href": "algorithms_for_prediction.html#sec-baseball",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\nWe will explore algorithms for prediction through a simple example dataset. The data contain the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today. After scraping the data and selecting a few variables, I appended each team’s win-loss record from 2022. The data are available in baseball_population.csv.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n\n\nbaseball_population &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nThe first rows of the data are depicted below. Each row is a player. The player Madison Bumgarner had a salary of $21,882,892. His position was LHP for left-handed pitcher. His team was Arizona, and this team’s record in the previous season was 0.457, meaning that they won 45.7% of their games.\n\n\n# A tibble: 944 × 5\n  player               salary position team    team_past_record\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457\n2 Marte, Ketel       11600000 2B       Arizona            0.457\n3 Ahmed, Nick        10375000 SS       Arizona            0.457\n# ℹ 941 more rows\n\n\nWe will summarize mean salaries. Some useful facts about mean salaries are that they vary substantially across positions and also across teams.\n\n\nCode\nbaseball_population |&gt;\n  group_by(position) |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  mutate(position = fct_reorder(position, -salary)) |&gt;\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", size = 3, fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme(axis.text.x = element_text(size = 7)) +\n  ggtitle(\"Baseball salaries vary across positions\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbaseball_population |&gt;\n  group_by(team) |&gt;\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |&gt;\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Baseball salaries vary across teams\")",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "href": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "title": "Algorithms for prediction",
    "section": "Task: Predict the Dodgers’ mean salary",
    "text": "Task: Predict the Dodgers’ mean salary\nAs a task, we will often focus on estimating the mean salary of the L.A. Dodgers. Because we have the full population of data, we can calculate the answer directly:\n\ntrue_dodger_mean &lt;- baseball_population |&gt;\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |&gt;\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |&gt;\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196. We will imagine that we don’t know this number. Instead of having the full population, we will imagine we have\n\ninformation on predictors for all players: position, team, team past record\ninformation on salary for a random sample of 5 players per team\n\nThe function draw_sample will draw one such sample.\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nIn our sample, we observe the salaries of 5 players per team. Our 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n# A tibble: 5 × 5\n  player           salary position team         team_past_record\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;\n1 Phillips, Evan  1300000 RHP      L.A. Dodgers            0.685\n2 Miller, Shelby  1500000 RHP      L.A. Dodgers            0.685\n3 Taylor, Chris  15000000 OF       L.A. Dodgers            0.685\n4 Betts, Mookie  21158692 OF       L.A. Dodgers            0.685\n5 Outman, James    722500 OF       L.A. Dodgers            0.685\n\n\nOur task is to predict the salary for all 35 Dodger players and average those predictions to yield a predicted vaue of the Dodgers’ mean salary on opening day 2023. The data we will use to predict include all variables except the outcome for the Dodger players.\n\ndodgers_to_predict &lt;- baseball_population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  select(-salary)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ordinary-least-squares",
    "href": "algorithms_for_prediction.html#ordinary-least-squares",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nTo walk through the steps of our prediction task, we first consider Ordinary Least Squares. After walking through these steps, we will consider a series of more advanced algorithms for prediction that involve similar steps.\nFor OLS, we might model salary as a function of team_past_record. The code below learns this model in our sample.\n\nols &lt;- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)\n\nWe can then make a prediction for every player on the Dodgers. Because our only predictor is a team-level predictor (team_past_record), the prediction will be the same for every player. But this may not always be the case, as further down the page when we consider position as an additional predictor.\n\nols_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |&gt;\n  print(n = 3)\n\n# A tibble: 35 × 5\n  player           position team         team_past_record predicted_salary\n  &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;            &lt;dbl&gt;\n1 Freeman, Freddie 1B       L.A. Dodgers            0.685         5552999.\n2 Heyward, Jason   OF       L.A. Dodgers            0.685         5552999.\n3 Betts, Mookie    OF       L.A. Dodgers            0.685         5552999.\n# ℹ 32 more rows\n\n\nFinally, we can average over these predictions to estimate the mean salary on the Dodgers.\n\nols_estimate &lt;- ols_predicted |&gt;\n  summarize(ols_estimate = mean(predicted_salary))\n\nBy OLS prediction, we estimate that the mean Dodger salary was $5.6 million. Because we estimated in a sample and under some modeling assumptions, this is a bit lower than the true population mean of $6.2 million.\n\nPerformance over repeated samples\nBecause this is a hypothetical setting, we can consider the performance of our estimator across repeated samples. The chunk below pulls our code into a single function that we call estimator(). The estimator takes a sample and returns an estimate.\n\nols_estimator &lt;- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}\n\nWe can run the estimator repeatedly, getting one estimate for each repeated sample from the population. This exercise is possible because in this simplified setting we have data on the full population.\n\nmany_sample_estimates &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample\n  estimate &lt;- ols_estimator(baseball_sample)\n  return(estimate)\n}\n\nThen we can visualize the performance across repeated samples.\n\n\nCode\ntibble(y = many_sample_estimates) |&gt;\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |&gt;\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nAcross repeated samples, the estimates have a standard deviation of $1.3 million and are on average $1.2 million too high.\n\n\nModel approximation error\nAn OLS model for salary that is linear in the team past record clearly suffers from model approximation error. If you fit a regression line to the entire population of baseball players you would see that th Dodger’s mean salary is below this line.\n\n\nCode\npopulation_ols &lt;- lm(salary ~ team_past_record, data = baseball_population)\nforplot &lt;- baseball_population |&gt;\n  mutate(fitted = predict(population_ols)) |&gt;\n  group_by(team) |&gt;\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers &lt;- forplot |&gt; filter(team == \"L.A. Dodgers\")\nforplot |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHow can we solve model approximation error? One might replace the linear term team_past_record with a series of categories for team in the OLS model.\n\nols_team_categories &lt;- lm(\n  salary ~ team,\n  data = baseball_sample\n)\n\nBut because the sample contains only 5 players per team, these estimates are quite noisy.\n\n\nCode\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nWe would like to make the model more flexible to reduce model approximation error, while also avoiding high variance. To balance these competing objectives, we need a new strategy from machine learning.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#a-big-idea-regularization",
    "href": "algorithms_for_prediction.html#a-big-idea-regularization",
    "title": "Algorithms for prediction",
    "section": "A big idea: Regularization",
    "text": "A big idea: Regularization\nRegularization encompasses a broad class of approaches designed for models that have many parameters, such as a unique intercept for every team in Major League Baseball. Regularization allows us to estimate many parameters while pulling them toward some common value in order to reduce the high variance that tends to results.\nAs one concrete example of regularization, we might want a middle ground between two extremes:\n\nour OLS linear prediction: \\(\\hat{Y}^\\text{Linear}_\\text{Dodgers} = \\hat\\alpha + \\hat\\beta\\text{(Past Record)}_\\text{Dodgers}={}\\) $3.9 million\nthe mean of the 5 sampled Dodger salaries: \\(\\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} = {}\\) $7.9 million\n\nA regularized estimator could be a weighted average of these two, with weight \\(w\\) placed on the nonparametric mean and weight \\(1 - w\\) placed on the linear prediction.\n\\[\n\\hat\\tau = w \\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} + (1 - w)\\hat{Y}^\\text{Linear}_\\text{Dodgers}\n\\] The graph below visualizes the resulting estimate at various values of \\(w\\). The regularized estimates are partially pooled toward the linear model prediction, with the amount of pooling controlled by \\(w\\).\n\n\nCode\nestimates_by_w &lt;- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |&gt;\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Samle Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPartial pooling is one way to navigate the bias-variance tradeoff: it allows us to have a flexible model while reducing the high amount of variance that such a model can create. In our case, the best estimator lies somewhere between the fully-pooled estimator (linear regression) and the fully-separate estimator (unique intercepts for each team).\nOne way to formally investigate the properties of our estimator is by defining a concept known as expected squared error. Let \\(\\hat\\mu_\\text{Dodgers}\\) be an estimator: a function that when applied to a sample \\(S\\) returns an estimate \\(\\hat\\mu_\\text{Dodgers}(S)\\). The squared error of this estimator in a particular sample \\(S\\) is \\((\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers})^2\\). Expected squared error is the expected value of this performance taken across repeated samples \\(S\\) from the population.\n\\[\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n\\]\nOrdinarily, one has only one sample and cannot directly calculate expected squared error. But our setting is useful for pedagogical purposes because we have the full population of baseball players and can repeatedly draw samples to evaluate performance. Below, we simulate \\(r = 1,\\dots,100\\) repeated samples and estimated expected squared error by the mean squared error of the estimates \\(\\hat\\mu_\\text{Dodgers}(S_r)\\) that we get from each simulated sample \\(S_r\\).\n\\[\n\\widehat{\\text{Expected Squared Error}}(\\hat\\mu_\\text{Dodgers}) = \\frac{1}{100}\\sum_{r=1}^{100}\\left(\\hat\\mu_\\text{Dodgers}(S_r) - \\mu_\\text{Dodgers}\\right)^2\n\\]\n\n\nCode\nrepeated_simulations &lt;- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  \n  ols_fit &lt;- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate &lt;- predict(\n    ols_fit, \n    newdata = baseball_population |&gt; \n      filter(team == \"L.A. Dodgers\") |&gt;\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate &lt;- a_sample |&gt;\n    filter(team == \"L.A. Dodgers\") |&gt;\n    summarize(salary = mean(salary)) |&gt;\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated &lt;- repeated_simulations |&gt;\n  group_by(w) |&gt;\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |&gt;\n  mutate(best = mse == min(mse))\n\naggregated |&gt;\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )\n\n\n\n\n\n\n\n\n\nIn this illustration, the best performance is an estimator that puts 75% of the weight on the linear fit and 25% of the weight on the mean among the 5 sampled Dodgers.\nThe example above illustrates an idea known as regularization, shrinkage, or partial pooling: we may often want to combine an estimate on a subgroup (the Dodger mean) with an estimate made on the full population (the linear fit). We will consider various methods to accomplish regularization, and we will return at the end to consider connections among them.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#multilevel-models",
    "href": "algorithms_for_prediction.html#multilevel-models",
    "title": "Algorithms for prediction",
    "section": "Multilevel models",
    "text": "Multilevel models\nMultilevel models1 are an algorithm for prediction that is fully grounded in classical statistics. They are especially powerful for the problem depicted above: making predictions when there are many groups (teams) with a small sample size in each group.\nWe will first illustrate a multilevel model’s performance and then consider the statistics behind this model. We will estimate using the lme4 package. If you don’t have this package, install it with install.packages(\"lme4\").\n\nlibrary(lme4)\n\nIn the syntax, the code (1 | team) says that our model should have a unique intercept for every team, and that these intercepts should be regularized (more on this soon).\n\nmultilevel &lt;- lmer(salary ~ team_past_record + (1 | team), data = baseball_sample)\n\nWe can make predictions from a multilevel model just like we can from OLS. For example, the code below makes predictions for the Dodgers.\n\nmultilevel_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )\n\n\nIntuition\nThe multilevel model is a partial-pooling estimator. The figure below displays this visually. For each team, the solid dot is the mean salary among the 5 sampled players. The ends of the arrows are the multilevel model estimates. The multilevel model pools the team-specific estimates toward the model-based prediction.\n\n\nCode\np &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np\n\n\n\n\n\n\n\n\n\nThe multilevel model only regularizes the team-specific estimates to the degree that they are imprecise. If we repeat the entire process on a sample of 20 players per team, each team-specific estimate becomes more precise and the overall amount of shrinkage is less.\n\n\nCode\nbigger_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 20) |&gt;\n  ungroup()\nmultilevel_big &lt;- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\n\n\n\n\n\n\n\n\n\n\n\nPerformance over repeated samples\nWe previously discussed how an OLS prediction that was linear in the past team record was a biased estimator with low variance. The sample mean within each team was an unbiased estimator with high variance. The multilevel model falls in between these two extremes.\n\n\nCode\nmany_sample_estimates_multilevel &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel &lt;- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate &lt;- mutilevel_predicted |&gt;\n    summarize(mutilevel_estimate = mean(predicted_salary)) |&gt;\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nIn math\nMathematically, a multilevel model is a maximum likelihood estimator. For our case, the model assumes that the salary of player \\(i\\) on team \\(t\\) is assumed to be normally distributed around the team mean salary \\(\\mu_t\\), with variance \\(\\sigma^2\\) which in our case is assumed to be the same across teams.\n\\[Y_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\\] The team-specific mean \\(\\mu_t\\) involves two components. First, this mean is assumed to be centered at a linear prediction \\(\\alpha + X_t\\beta\\) where \\(X_t\\) is the win-loss record of team \\(t\\) in the previous year. This is the value toward which team-specific estimates are regularized. Second, the mean for the particular team \\(i\\) is drawn from a normal distribution with standard deviation \\(\\tau^2\\), which is the standard deviation of the team-specific mean salary residuals across teams.\n\\[\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)\\] By maximizing the log likelihood of the observed data under this model, one comes to maximum likelihood estimates of all of the unknown parameters. The \\(\\mu_t\\) estimates will partially pool between two estimators,\n\nthe sample mean \\(\\bar{Y}_t\\) within team \\(t\\)\nthe linear model prediction \\(\\hat\\alpha + X_t\\hat\\beta\\)\n\nwhere the weight on (1) will depend on the relative precision of this within-team estimate and the weight (2) will depend on the relative precision of the between-team estimate. After explaining ridge regression, we will return to a simplified case where the formula for the multilevel model estimates allows further intuition building.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ridge-regression",
    "href": "algorithms_for_prediction.html#ridge-regression",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\nWhile multilevel models are often approached from the standpoint of classical statistics, they are very similar to another approach commonly approached from the standpoint of data science: ridge regression.\n\nIntuition\nConsider our sample of 30 baseball teams with 5 players per team. We might want to fit a linear regression model as follows,\n\\[\nY_{ij} = \\alpha + \\beta X_i + \\gamma_{i} + \\epsilon_{ij}\n\\] where \\(Y_{ij}\\) is the salary of player \\(i\\) on team \\(j\\) and \\(X_i\\) is the past win-loss record of that team. In this model, \\(\\gamma_i\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_ij\\) is the deviation for player \\(j\\) from their own team’s average salary.\nThe problem with this model is its high variance: with 30 teams, there are 30 different values of \\(\\gamma_i\\) to be estimated. And there are only 5 players per team! We might believe that \\(\\gamma_i\\) values will generally be small, so we might want to estimate by penalizing large values of \\(\\gamma_i\\).\nWe can penalize large values of \\(\\gamma_i\\) by an estimator written as it might be written in a data science course:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_i\\sum_j\\left(Y_{ij} - \\left(\\alpha + \\beta X_i + \\gamma_{i}\\right)\\right)^2 + \\lambda\\sum_i\\gamma_i^2\\right]\n\\]\nwhere the estimated values \\(\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\}\\) are chosen to minimize a loss function which is the sum over the data of the squared prediction error plus a penalty on large values of \\(\\gamma_i\\).\n\n\nIn code\n\nlibrary(glmnet)\n\n\nridge &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)\n\nWe can visualize the ridge regression estimates just like the multilevel model estimates. Because the penalty applies to squared values of team deviations from the line, the points furthest from the line are most strongly regularized toward the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nThe amount of regularization will depend on the chosen value of the penalty parameter \\(\\lambda\\). To the degree that \\(\\lambda\\) is large, estimates will be more strongly pulled toward the regression line. Below is a visualization at three values of \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  bind_cols(fitted) |&gt;\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |&gt;\n  pivot_longer(cols = contains('Lambda')) |&gt;\n  distinct() |&gt;\n  mutate(name = fct_rev(name)) |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter \\(\\lambda\\) rises. The biase of the estimates also increases as the value of \\(\\lambda\\) increases. The optimal value of \\(\\lambda\\) is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\nCode\nmany_sample_estimates_ridge &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nConnections: Multilevel model and ridge regression\nMultilevel models and ridge regression are closely connected, and they can yield mathematically equivalent estimates in special cases. Consider again the multilevel model for the salary \\(Y_{ti}\\) of player \\(i\\) on team \\(t\\).\n\\[\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n\\] For simplicity, suppose we already have estimates \\(\\hat\\mu_0\\), \\(\\hat\\tau^2\\), and \\(\\hat\\sigma^2\\). One can show that the multilevel model estimates of \\(\\mu_t\\) are:\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\] This formula can be interpreted as analogous to a sample mean, but with some added observations. The first part of the numerator and denominator corresponds to a sample mean: the sum \\(\\sum_i Y_{ti}\\) of salaries of all sampled players in team \\(t\\) in the numerator and the number of such players \\(n_t\\) in the denominator. The right side of the numerator and denominator correspond to pseudo-observations. It is as though in addition to the sampled players, we also saw \\(\\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\) additional players with exactly the overall baseball mean salary estimate \\(\\hat\\mu_0\\). This part of the formula partially pools the team-specific mean toward this overall mean. Partial pooling is greater to the degree that there is small within-team variance (small \\(\\sigma^2\\)) or large across-team variance (large \\(\\hat\\tau^2\\)).\nNext, we consider a ridge regression estimator that minimizes an objective function where \\(\\hat\\mu_0\\) is an unpenalized estimate for the baseball-wide mean salary.2\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n\\] We can gain some additional intuition for this estimate by rearranging to pull the penalty into the main term.\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\] The first part of this term is the sum over observed squared errors within team \\(t\\), \\(\\sum_i\\left(Y_{it}-\\mu_t\\right)^2\\). The second part is as though we had observed an additional \\(\\lambda\\) cases within team \\(t\\) with an outcome value \\(\\mu_0\\) equal to the baseball-wide mean. With this intuition, the ridge regression estimator becomes\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\] which is the same as the multilevel model estimator in the special case when \\(\\lambda = \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\). Multilevel models and ridge regression are actually accomplishing the same thing!\nBelow, we use code to check that these two give the same thing. We first fit a multilevel model,\n\nmultilevel &lt;- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n\nThen we make predictions,\n\nyhat_multilevel &lt;- baseball_sample |&gt;\n  mutate(yhat_multilevel = predict(multilevel)) |&gt;\n  distinct(team, yhat_multilevel)\n\nand extract the implied value of \\(\\lambda\\) for an equivalent ridge regression (from the math above).\n\nlambda_equivalent &lt;- as_tibble(VarCorr(multilevel)) |&gt;\n  select(grp, vcov) |&gt;\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |&gt;\n  mutate(lambda_equivalent = Residual / team) |&gt;\n  pull(lambda_equivalent)\n\nNow we estimate ridge regression with that \\(\\lambda\\) value. Because glmnet internally rescales variables, it is difficult to carry out this check with glmnet. Instead, we will write our own ridge regression estimator. We first define our predictor matrix X and a mean-centered outcome vector y_centered. The reason to mean-center the outcome is to allow an unpenalized grand mean (\\(\\mu_0\\) in the math above). We will add this value back to predictions later.\n\nX &lt;- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered &lt;- baseball_sample$salary - mean(baseball_sample$salary)\n\nThe ridge regression estimator can be written in matrix form as follows:\n\\[\nY - \\hat\\mu_0 = \\mathbf{X}\\vec\\beta + \\epsilon\n\\] with \\(\\hat{\\vec\\beta}_\\text{Ridge} = (\\mathbf{X}'\\mathbf{X} + \\text{diag}(\\lambda))^{-1}\\mathbf{X}'\\vec{Y}\\). The code below estimates \\(\\vec\\beta\\)\n\nbeta_ridge &lt;- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\n\nBecause there is one \\(\\beta\\) value for each team, we convert to predicted values for each team by adding the grand mean to the estimated coefficients.\n\nyhat_ridge &lt;- beta_ridge + mean(baseball_sample$salary)\n\nFinally, we create a tibble with both the ridge regression and multilevel model estimates.\n\nboth_estimators &lt;- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |&gt;\n  rename(team = rowname) |&gt;\n  mutate(team = str_remove(team,\"team\")) |&gt;\n  left_join(yhat_multilevel, by = join_by(team))\n\nWe can confirm in code that the two predictions are numerically equal.\n\nboth_estimators |&gt;\n  # remove names; it is ok if names are unequal\n  mutate_all(unname) |&gt;\n  # summarize whether the two columns are equal\n  summarize(numerically_equal = all.equal(yhat_ridge, yhat_multilevel))\n\n# A tibble: 1 × 1\n  numerically_equal\n  &lt;lgl&gt;            \n1 TRUE             \n\n\nWe produce a plot with one point for each team, with ridge regression predictions on the \\(x\\)-axis and multilevel model predictions on the \\(y\\)-axis. We can see that these two estimators are equivalent.\n\n\nCode\nboth_estimators |&gt;\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_point() +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Ridge regression and multilevel models can yield\\nequal estimates for the mean salary on each team\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe equivalency of ridge regression and multilevel models may be surprising. Ridge regression is often motivated from a loss function (squared error + penalty), using terms that are common in data science and machine learning. Multilevel models are often motivated from sociological examples where units are clustered in groups, with terminology more common in statistics. Yet the two are mathematically related. An important difference is that multilevel models learn the amount of regularization from the data, whereas ridge regression needs an additional step to learn the penalty parameter (to be discussed in a future class session on data-driven estimator selection).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#lasso-regression",
    "href": "algorithms_for_prediction.html#lasso-regression",
    "title": "Algorithms for prediction",
    "section": "LASSO regression",
    "text": "LASSO regression\nLASSO regression is just like ridge regression except for one key difference: instead of penalizing the sum of squared coefficients, LASSO penalizes the sum of the absolute value of coefficients. As we will see, this change means that some parameters become regularized all the way to zero so that some terms drop out of the model completely.\nAs with ridge regressiom, our outcome model is\n\\[\nY_{ij} = \\alpha + \\beta X_i + \\gamma_{i} + \\epsilon_{ij}\n\\] where \\(Y_{ij}\\) is the salary of player \\(i\\) on team \\(j\\) and \\(X_i\\) is the past win-loss record of that team. In this model, \\(\\gamma_i\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_ij\\) is the deviation for player \\(j\\) from their own team’s average salary.\nTo solve the high-variance estimates of \\(\\gamma_i\\), LASSO regression uses a different penalty term in its loss function:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_i\\sum_j\\left(Y_{ij} - \\left(\\alpha + \\beta X_i + \\gamma_{i}\\right)\\right)^2 + \\lambda\\sum_i\\lvert\\gamma_i\\rvert\\right]\n\\]\nwhere \\(\\gamma_i^2\\) from the ridge regression penalty has been replaced by the absolute value \\(\\lvert\\gamma_i\\rvert\\). We will first apply this in code and then see how it changes the performance of the estimator.\n\nIn code\n\nlibrary(glmnet)\n\n\nlasso &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)\n\nVisualizing the estimates, we see behavior similar to what we have previously seen from multilevel models and ridge regression. All estimates are pulled toward a linear regression fit. There are two key differences, however.\nFirst, the previous estimators most strongly pulled the large deviations toward the linear fit. The LASSO estimates pull all parameter estimates toward the linear fit to a similar degree, regardless of their size. This is because the LASSO estimates penalize the absolute value of \\(\\gamma_i\\) instead of the squared value of \\(\\gamma_i\\).\nSecond, the multilevel and ridge estimates never pulled any of the estimates all the way to the line; instead, the estimates asymptoted toward the line as the penalty parameter grew. In LASSO, some estimates are pulled all the way to the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\). At a very small penalty, the estimate is approximately the same as the mean among the 5 sampled Dodger players. As the penalty parameter \\(\\lambda\\) gets larger, the estimates move around. They generally move toward zero, but not always: as some other team-specific deviations are pulled to zero, the unregularized intercept and slope on team past record move around in response. Ultimately, the penalty becomes so large that the Dodger estimate is regularized all the way to the estimate we would get in a model with no team-specific deviations.\n\n\nCode\nfitted &lt;- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs with ridge regression, we will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nSimilar to the ridge regression estimator, we can visualize the performance of the LASSO regression estimator across repeated samples from the population.\n\n\nCode\nmany_sample_estimates_lasso &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#trees",
    "href": "algorithms_for_prediction.html#trees",
    "title": "Algorithms for prediction",
    "section": "Trees",
    "text": "Trees",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#forests",
    "href": "algorithms_for_prediction.html#forests",
    "title": "Algorithms for prediction",
    "section": "Forests",
    "text": "Forests",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#gradient-boosting",
    "href": "algorithms_for_prediction.html#gradient-boosting",
    "title": "Algorithms for prediction",
    "section": "Gradient boosting",
    "text": "Gradient boosting",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#footnotes",
    "href": "algorithms_for_prediction.html#footnotes",
    "title": "Algorithms for prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRaudenbush, S. W., and A.S. Bryk. (2002). Hierarchical linear models: Applications and data analysis methods. Advanced Quantitative Techniques in the Social Sciences Series/SAGE.↩︎\nWhile we write out \\(\\hat\\mu_0\\), algorithmic implementations of ridge regression often mean-center \\(Y\\) before applying the algorithm which is equivalent to having an unpenalized \\(\\hat\\mu_0\\).↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "yhat_regression.html",
    "href": "yhat_regression.html",
    "title": "Regression for Y-hat",
    "section": "",
    "text": "slides\nThis class is about regression as a tool to approximate a conditional expectation function. From this perspective, the \\(\\hat\\beta\\) estimates are only a step toward the broader purpose of regression to produce \\(\\hat{Y}\\) values that achieve this approximation well. This perspective will ultimately allow us to consider machine learning estimators beyond regression.\nSome of this class relies on an ongoing project on description: ilundberg.github.io/description",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Regression for Y-hat"
    ]
  },
  {
    "objectID": "weighting_for_inference.html",
    "href": "weighting_for_inference.html",
    "title": "Weighting for causal and population inference",
    "section": "",
    "text": "This session will be about weighting to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by weighting"
    ]
  },
  {
    "objectID": "resampling.html",
    "href": "resampling.html",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "",
    "text": "As researchers adopt algorithmic estimation methods for which analytical standard errors do not exist, methods to produce standard errors by resampling become all the more important. We will discuss the bootstrap for simple random samples and extensions to allow resampling-based standard error estimates in complex survey samples.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "prediction_for_inference.html",
    "href": "prediction_for_inference.html",
    "title": "Prediction for causal and population inference",
    "section": "",
    "text": "This session will be about prediction to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators. If you have a Census with features \\(\\vec{X}\\), ignorable sampling conditional on \\(\\vec{X}\\), and a good sample estimator of \\(E(Y\\mid\\vec{X})\\) then you can predict \\(E(Y\\mid\\vec{X})\\) and aggregate over the Census-known population distribution of \\(\\vec{X}\\). For causal inference, being assigned to treatment is analogous to being sampled to observe the potential outcome under treatment.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "nonparametric_identification.html",
    "href": "nonparametric_identification.html",
    "title": "Nonparametric Identification",
    "section": "",
    "text": "This page will cover nonparametric identification of causal and population parameters. To be written.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "Mediation",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Mediation"
    ]
  },
  {
    "objectID": "doubly_robust.html",
    "href": "doubly_robust.html",
    "title": "Doubly-robust estimation",
    "section": "",
    "text": "This session will combine prediction and weighting methods for an approach with properties superior to either on its own.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Doubly-robust estimation"
    ]
  },
  {
    "objectID": "asking_a_research_question.html",
    "href": "asking_a_research_question.html",
    "title": "Asking a research question",
    "section": "",
    "text": "slides\nThis class is about how to ask a quantitative research question. The focus will be on summarizing outcomes over well-defined populations, for which a regression coefficient \\(\\hat\\beta\\) may or may not be a meaningful summary. We will focus on summarizing subgroups by the average value of \\(Y\\), or in small samples by summarizing subgroup means using predicted values (\\(\\hat{Y}\\)) from regression models.",
    "crumbs": [
      "Problem Sets",
      "Asking a Research Question"
    ]
  },
  {
    "objectID": "data_driven_selection.html",
    "href": "data_driven_selection.html",
    "title": "Data-driven selection of a prediction function",
    "section": "",
    "text": "To be written.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to UCLA SOCIOL 212B (Winter 2025). See the syllabus.\nW 9–11:50am. Powell 320B.\nThis course is about answering social science questions using quantitative data. We will especially focus on how computational power is transforming the ways we can carry out quantitative research, covering both statistical and machine learning tools from the perspective of social science applications. The course especially emphasizes how to translate social science theories into quantities that can be estimated by algorithms designed for prediction. We will consider prediction in the service of both description and causal inference, building on ideas from SOCIOL 212A. The end product of the course is an extended abstract containing data analysis using the ideas from the course. For students continuing to 212C, the abstract can serve as the basis for the research project in that course. Students will leave the course prepared to connect social science theories to empirical evidence that can be produced by algorithms designed for prediction."
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Welcome!",
    "section": "Learning goals",
    "text": "Learning goals\nStudents will learn to\n\ndefine a precise quantitative research question\nconnect that question to predictions that can be made by statistical or machine learning algorithms\nmake a principled argument for the choice of a particular learning approach"
  },
  {
    "objectID": "index.html#schedule-of-topics-tentative",
    "href": "index.html#schedule-of-topics-tentative",
    "title": "Welcome!",
    "section": "Schedule of topics (tentative)",
    "text": "Schedule of topics (tentative)\nPart 1. Descriptive data science with probability samples.\n\nJan 8. Asking research questions without \\(\\hat\\beta\\) and regression for \\(\\hat{Y}\\)\nJan 15 and 22. Algorithms for prediction\nJan 29. Data-driven selection of an estimator\nFeb 5. Panel data (actually a Part 2 topic, presented out of order to align with afternoon CCPR workshop)\nFeb 12. Statistical uncertainty by resampling\n\nPart 2. Non-probability samples and observational causal inference\n\nFeb 19. Nonparametric identification\nFeb 26. Estimation by prediction\nMar 5. Estimation by weighting\nMar 12. Doubly-robust estimation"
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "Welcome!",
    "section": "Who should take this course?",
    "text": "Who should take this course?\nThe course is designed to support the development of quantitative social science research projects. The course is a good fit for PhD students in sociology, statistics, political science, economics, and other social sciences. PhD students from disciplines other than sociology should request a code from the instructor to enroll."
  },
  {
    "objectID": "index.html#prerequisite",
    "href": "index.html#prerequisite",
    "title": "Welcome!",
    "section": "Prerequisite",
    "text": "Prerequisite\nFamiliarity with basic probability and statistics (e.g., random variables, expectation, confidence intervals). Soc 212A is formally a prerequisite, but students who did not take Soc 212A are welcome to talk with me about whether Soc 212B would be a good fit for them."
  },
  {
    "objectID": "index.html#instructional-format",
    "href": "index.html#instructional-format",
    "title": "Welcome!",
    "section": "Instructional format",
    "text": "Instructional format\nLecture with in-class exercises. Bring computers to class."
  },
  {
    "objectID": "index.html#course-readings",
    "href": "index.html#course-readings",
    "title": "Welcome!",
    "section": "Course readings",
    "text": "Course readings\nReadings will be available online for free. See the course website for an updated schedule of readings and topics.\nMany readings from books with free PDFs available online:\n\nEfron, B., & T Hastie. 2016. Computer Age Statistical Inference: Algorithms, Evidence and Data Science. Cambridge: Cambridge University Press.\nHastie, T., R. Tibshirani, & J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer.\nHernán, M.A., & J.M. Robins. 2024. Causal Inference: What If? Boca Raton: Chapman & Hall / CRC."
  },
  {
    "objectID": "index.html#statistical-software",
    "href": "index.html#statistical-software",
    "title": "Welcome!",
    "section": "Statistical software",
    "text": "Statistical software\nYou can use any statistical software you prefer. I use R and will best be able to support you in R. In addition to R, we will attempt to provide Stata support where possible. Not all algorithms are available in Stata. If you are fluent in another software, you are welcome to use that. The focus of this course is on conceptual ideas, not a programming language."
  },
  {
    "objectID": "missing_data.html",
    "href": "missing_data.html",
    "title": "Missing data",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Missing data"
    ]
  },
  {
    "objectID": "panel_data.html",
    "href": "panel_data.html",
    "title": "Panel data",
    "section": "",
    "text": "This page will cover forecasting, interrupted time series, difference in difference, and fixed effects.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Panel data"
    ]
  },
  {
    "objectID": "problem_sets.html",
    "href": "problem_sets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "This page will contain a problem set corresponding to each class meeting. The problem sets are very open-ended and are designed to connect the material from class to your ongoing project.\nFor every problem set, submit a PDF. If your code is not embedded in your PDF, then also submit a code file.\nWe chose together the following weekly pattern for problem sets:\nWe will be using identified peer reviews. The reason these are not anonymous is that we are a small class, and we will get to know one another’s projects. Your peer reviewer will not be grading you or assigning point values, but they will be commenting on your work. A good peer review is a short paragraph that comments on promising aspects of your peer’s work as well as offering suggestions for improvement or future directions."
  },
  {
    "objectID": "problem_sets.html#problem-set-1",
    "href": "problem_sets.html#problem-set-1",
    "title": "Problem Sets",
    "section": "Problem Set 1",
    "text": "Problem Set 1\n\n1. Your ongoing paper.\n1.1. (15 points) Write an abstract of your research paper.\n1.2. (5 points) What is one unit-specific quantity in your paper?\n1.3. (5 points) What is one target population in your paper?\n\n\n2. Regression for a conditional mean\n(25 points)\nIn class, we used OLS regression to predict the mean outcome in a subgroup. Do the same thing in a dataset of your choosing, which might be the dataset from your project. Define the outcome variable and the population subgroup you are describing."
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "Scale construction",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Scale construction"
    ]
  },
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team!\n\n\n\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy! I am excited about causal inference and finding new research questions and ways to answer them. Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nSoonhong Cho\ntnsehdtm@gmail.com\n(he / him)\nSoonhong is a PhD candidate in political science at UCLA."
  }
]