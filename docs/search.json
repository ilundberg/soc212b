[
  {
    "objectID": "algorithms_for_prediction.html",
    "href": "algorithms_for_prediction.html",
    "title": "Algorithms for prediction",
    "section": "",
    "text": "For two sessions, we will cover a few algorithms for prediction. We aim to\nThis page contains embedded R code. If you are a Stata user, you can download do files that illustrate Ordinary Least Squares, multilevel models, ridge regression, LASSO regression, and trees and forests.\nA version of this page in slide format is available here:\nHere is a PDF of this page for those taking notes by hand.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#sec-baseball",
    "href": "algorithms_for_prediction.html#sec-baseball",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\nWe will explore algorithms for prediction through a simple example dataset. The data contain the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today. After scraping the data and selecting a few variables, I appended each team’s win-loss record from 2022. The data are available in baseball_population.csv.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n\n\nbaseball_population &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nThe first rows of the data are depicted below. Each row is a player. The player Madison Bumgarner had a salary of $21,882,892. His position was LHP for left-handed pitcher. His team was Arizona, and this team’s record in the previous season was 0.457, meaning that they won 45.7% of their games.\n\n\n# A tibble: 944 × 6\n  player               salary position team    team_past_record team_past_salary\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457         2794887.\n2 Marte, Ketel       11600000 2B       Arizona            0.457         2794887.\n3 Ahmed, Nick        10375000 SS       Arizona            0.457         2794887.\n# ℹ 941 more rows\n\n\nWe will summarize mean salaries. Some useful facts about mean salaries are that they vary substantially across positions and also across teams.\n\n\nCode\nbaseball_population |&gt;\n  group_by(position) |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  mutate(position = fct_reorder(position, -salary)) |&gt;\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", size = 3, fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme(axis.text.x = element_text(size = 7)) +\n  ggtitle(\"Baseball salaries vary across positions\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbaseball_population |&gt;\n  group_by(team) |&gt;\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |&gt;\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Baseball salaries vary across teams\")",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "href": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "title": "Algorithms for prediction",
    "section": "Task: Predict the Dodgers’ mean salary",
    "text": "Task: Predict the Dodgers’ mean salary\nAs a task, we will often focus on estimating the mean salary of the L.A. Dodgers. Because we have the full population of data, we can calculate the answer directly:\n\ntrue_dodger_mean &lt;- baseball_population |&gt;\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |&gt;\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |&gt;\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196. We will imagine that we don’t know this number. Instead of having the full population, we will imagine we have\n\ninformation on predictors for all players: position, team, team past record\ninformation on salary for a random sample of 5 players per team\n\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nIn our sample, we observe the salaries of 5 players per team. Our 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n# A tibble: 5 × 6\n  player           salary position team        team_past_record team_past_salary\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Phillips, Evan  1300000 RHP      L.A. Dodge…            0.685         8388736.\n2 Miller, Shelby  1500000 RHP      L.A. Dodge…            0.685         8388736.\n3 Taylor, Chris  15000000 OF       L.A. Dodge…            0.685         8388736.\n4 Betts, Mookie  21158692 OF       L.A. Dodge…            0.685         8388736.\n5 Outman, James    722500 OF       L.A. Dodge…            0.685         8388736.\n\n\nOur task is to predict the salary for all 35 Dodger players and average those predictions to yield a predicted vaue of the Dodgers’ mean salary on opening day 2023. The data we will use to predict include all variables except the outcome for the Dodger players.\n\ndodgers_to_predict &lt;- baseball_population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  select(-salary)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ordinary-least-squares",
    "href": "algorithms_for_prediction.html#ordinary-least-squares",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nTo walk through the steps of our prediction task, we first consider Ordinary Least Squares. After walking through these steps, we will consider a series of more advanced algorithms for prediction that involve similar steps.\nFor OLS, we might model salary as a function of team_past_record. The code below learns this model in our sample.\n\nols &lt;- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)\n\nWe can then make a prediction for every player on the Dodgers. Because our only predictor is a team-level predictor (team_past_record), the prediction will be the same for every player. But this may not always be the case, as further down the page when we consider position as an additional predictor.\n\nols_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |&gt;\n  print(n = 3)\n\n# A tibble: 35 × 6\n  player       position team  team_past_record team_past_salary predicted_salary\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Freeman, Fr… 1B       L.A.…            0.685         8388736.         5552999.\n2 Heyward, Ja… OF       L.A.…            0.685         8388736.         5552999.\n3 Betts, Mook… OF       L.A.…            0.685         8388736.         5552999.\n# ℹ 32 more rows\n\n\nFinally, we can average over these predictions to estimate the mean salary on the Dodgers.\n\nols_estimate &lt;- ols_predicted |&gt;\n  summarize(ols_estimate = mean(predicted_salary))\n\nBy OLS prediction, we estimate that the mean Dodger salary was $5.6 million. Because we estimated in a sample and under some modeling assumptions, this is a bit lower than the true population mean of $6.2 million.\n\nPerformance over repeated samples\nBecause this is a hypothetical setting, we can consider the performance of our estimator across repeated samples. The chunk below pulls our code into a single function that we call estimator(). The estimator takes a sample and returns an estimate.\n\nols_estimator &lt;- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}\n\nWe can run the estimator repeatedly, getting one estimate for each repeated sample from the population. This exercise is possible because in this simplified setting we have data on the full population.\n\nmany_sample_estimates &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample\n  estimate &lt;- ols_estimator(baseball_sample)\n  return(estimate)\n}\n\nThen we can visualize the performance across repeated samples.\n\n\nCode\ntibble(y = many_sample_estimates) |&gt;\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |&gt;\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nAcross repeated samples, the estimates have a standard deviation of $1.3 million and are on average $1.2 million too high.\n\n\nModel approximation error\nAn OLS model for salary that is linear in the team past record clearly suffers from model approximation error. If you fit a regression line to the entire population of baseball players you would see that th Dodger’s mean salary is below this line.\n\n\nCode\npopulation_ols &lt;- lm(salary ~ team_past_record, data = baseball_population)\nforplot &lt;- baseball_population |&gt;\n  mutate(fitted = predict(population_ols)) |&gt;\n  group_by(team) |&gt;\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers &lt;- forplot |&gt; filter(team == \"L.A. Dodgers\")\nforplot |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHow can we solve model approximation error? One might replace the linear term team_past_record with a series of categories for team in the OLS model.\n\nols_team_categories &lt;- lm(\n  salary ~ team,\n  data = baseball_sample\n)\n\nBut because the sample contains only 5 players per team, these estimates are quite noisy.\n\n\nCode\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nWe would like to make the model more flexible to reduce model approximation error, while also avoiding high variance. To balance these competing objectives, we need a new strategy from machine learning.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#a-big-idea-regularization",
    "href": "algorithms_for_prediction.html#a-big-idea-regularization",
    "title": "Algorithms for prediction",
    "section": "A big idea: Regularization",
    "text": "A big idea: Regularization\nRegularization encompasses a broad class of approaches designed for models that have many parameters, such as a unique intercept for every team in Major League Baseball. Regularization allows us to estimate many parameters while pulling them toward some common value in order to reduce the high variance that tends to results.\nAs one concrete example of regularization, we might want a middle ground between two extremes:\n\nour OLS linear prediction: \\(\\hat{Y}^\\text{Linear}_\\text{Dodgers} = \\hat\\alpha + \\hat\\beta\\text{(Past Record)}_\\text{Dodgers}={}\\) $3.9 million\nthe mean of the 5 sampled Dodger salaries: \\(\\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} = {}\\) $7.9 million\n\nA regularized estimator could be a weighted average of these two, with weight \\(w\\) placed on the nonparametric mean and weight \\(1 - w\\) placed on the linear prediction.\n\\[\n\\hat\\tau = w \\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} + (1 - w)\\hat{Y}^\\text{Linear}_\\text{Dodgers}\n\\] The graph below visualizes the resulting estimate at various values of \\(w\\). The regularized estimates are partially pooled toward the linear model prediction, with the amount of pooling controlled by \\(w\\).\n\n\nCode\nestimates_by_w &lt;- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |&gt;\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Sample Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPartial pooling is one way to navigate the bias-variance tradeoff: it allows us to have a flexible model while reducing the high amount of variance that such a model can create. In our case, the best estimator lies somewhere between the fully-pooled estimator (linear regression) and the fully-separate estimator (unique intercepts for each team).\nOne way to formally investigate the properties of our estimator is by defining a concept known as expected squared error. Let \\(\\hat\\mu_\\text{Dodgers}\\) be an estimator: a function that when applied to a sample \\(S\\) returns an estimate \\(\\hat\\mu_\\text{Dodgers}(S)\\). The squared error of this estimator in a particular sample \\(S\\) is \\((\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers})^2\\). Expected squared error is the expected value of this performance taken across repeated samples \\(S\\) from the population.\n\\[\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n\\]\nOrdinarily, one has only one sample and cannot directly calculate expected squared error. But our setting is useful for pedagogical purposes because we have the full population of baseball players and can repeatedly draw samples to evaluate performance. Below, we simulate \\(r = 1,\\dots,100\\) repeated samples and estimated expected squared error by the mean squared error of the estimates \\(\\hat\\mu_\\text{Dodgers}(S_r)\\) that we get from each simulated sample \\(S_r\\).\n\\[\n\\widehat{\\text{Expected Squared Error}}(\\hat\\mu_\\text{Dodgers}) = \\frac{1}{100}\\sum_{r=1}^{100}\\left(\\hat\\mu_\\text{Dodgers}(S_r) - \\mu_\\text{Dodgers}\\right)^2\n\\]\n\n\nCode\nrepeated_simulations &lt;- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  \n  ols_fit &lt;- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate &lt;- predict(\n    ols_fit, \n    newdata = baseball_population |&gt; \n      filter(team == \"L.A. Dodgers\") |&gt;\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate &lt;- a_sample |&gt;\n    filter(team == \"L.A. Dodgers\") |&gt;\n    summarize(salary = mean(salary)) |&gt;\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated &lt;- repeated_simulations |&gt;\n  group_by(w) |&gt;\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |&gt;\n  mutate(best = mse == min(mse))\n\naggregated |&gt;\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )\n\n\n\n\n\n\n\n\n\nIn this illustration, the best performance is an estimator that puts 75% of the weight on the linear fit and 25% of the weight on the mean among the 5 sampled Dodgers.\nThe example above illustrates an idea known as regularization, shrinkage, or partial pooling: we may often want to combine an estimate on a subgroup (the Dodger mean) with an estimate made on the full population (the linear fit). We will consider various methods to accomplish regularization, and we will return at the end to consider connections among them.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#multilevel-models",
    "href": "algorithms_for_prediction.html#multilevel-models",
    "title": "Algorithms for prediction",
    "section": "Multilevel models",
    "text": "Multilevel models\nMultilevel models1 are an algorithm for prediction that is fully grounded in classical statistics. They are especially powerful for the problem depicted above: making predictions when there are many groups (teams) with a small sample size in each group.\nWe will first illustrate a multilevel model’s performance and then consider the statistics behind this model. We will estimate using the lme4 package. If you don’t have this package, install it with install.packages(\"lme4\").\n\nlibrary(lme4)\n\nIn the syntax, the code (1 | team) says that our model should have a unique intercept for every team, and that these intercepts should be regularized (more on this soon).\n\nmultilevel &lt;- lmer(salary ~ team_past_record + (1 | team), data = baseball_sample)\n\nWe can make predictions from a multilevel model just like we can from OLS. For example, the code below makes predictions for the Dodgers.\n\nmultilevel_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )\n\n\nIntuition\nThe multilevel model is a partial-pooling estimator. The figure below displays this visually. For each team, the solid dot is the mean salary among the 5 sampled players. The ends of the arrows are the multilevel model estimates. The multilevel model pools the team-specific estimates toward the model-based prediction.\n\n\nCode\np &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np\n\n\n\n\n\n\n\n\n\nThe multilevel model only regularizes the team-specific estimates to the degree that they are imprecise. If we repeat the entire process on a sample of 20 players per team, each team-specific estimate becomes more precise and the overall amount of shrinkage is less.\n\n\nCode\nbigger_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 20) |&gt;\n  ungroup()\nmultilevel_big &lt;- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\n\n\n\n\n\n\n\n\n\n\n\nPerformance over repeated samples\nWe previously discussed how an OLS prediction that was linear in the past team record was a biased estimator with low variance. The sample mean within each team was an unbiased estimator with high variance. The multilevel model falls in between these two extremes.\n\n\nCode\nmany_sample_estimates_multilevel &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel &lt;- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate &lt;- mutilevel_predicted |&gt;\n    summarize(mutilevel_estimate = mean(predicted_salary)) |&gt;\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nIn math\nMathematically, a multilevel model is a maximum likelihood estimator. For our case, the model assumes that the salary of player \\(i\\) on team \\(t\\) is assumed to be normally distributed around the team mean salary \\(\\mu_t\\), with variance \\(\\sigma^2\\) which in our case is assumed to be the same across teams.\n\\[Y_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\\] The team-specific mean \\(\\mu_t\\) involves two components. First, this mean is assumed to be centered at a linear prediction \\(\\alpha + X_t\\beta\\) where \\(X_t\\) is the win-loss record of team \\(t\\) in the previous year. This is the value toward which team-specific estimates are regularized. Second, the mean for the particular team \\(i\\) is drawn from a normal distribution with standard deviation \\(\\tau^2\\), which is the standard deviation of the team-specific mean salary residuals across teams.\n\\[\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)\\] By maximizing the log likelihood of the observed data under this model, one comes to maximum likelihood estimates of all of the unknown parameters. The \\(\\mu_t\\) estimates will partially pool between two estimators,\n\nthe sample mean \\(\\bar{Y}_t\\) within team \\(t\\)\nthe linear model prediction \\(\\hat\\alpha + X_t\\hat\\beta\\)\n\nwhere the weight on (1) will depend on the relative precision of this within-team estimate and the weight (2) will depend on the relative precision of the between-team estimate. After explaining ridge regression, we will return to a simplified case where the formula for the multilevel model estimates allows further intuition building.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ridge-regression",
    "href": "algorithms_for_prediction.html#ridge-regression",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\nWhile multilevel models are often approached from the standpoint of classical statistics, they are very similar to another approach commonly approached from the standpoint of data science: ridge regression.\n\nIntuition\nConsider our sample of 30 baseball teams with 5 players per team. We might want to fit a linear regression model as follows,\n\\[\nY_{ij} = \\alpha + \\beta X_i + \\gamma_{i} + \\epsilon_{ij}\n\\] where \\(Y_{ij}\\) is the salary of player \\(i\\) on team \\(j\\) and \\(X_i\\) is the past win-loss record of that team. In this model, \\(\\gamma_i\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_ij\\) is the deviation for player \\(j\\) from their own team’s average salary.\nThe problem with this model is its high variance: with 30 teams, there are 30 different values of \\(\\gamma_i\\) to be estimated. And there are only 5 players per team! We might believe that \\(\\gamma_i\\) values will generally be small, so we might want to estimate by penalizing large values of \\(\\gamma_i\\).\nWe can penalize large values of \\(\\gamma_i\\) by an estimator written as it might be written in a data science course:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_i\\sum_j\\left(Y_{ij} - \\left(\\alpha + \\beta X_i + \\gamma_{i}\\right)\\right)^2 + \\lambda\\sum_i\\gamma_i^2\\right]\n\\]\nwhere the estimated values \\(\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\}\\) are chosen to minimize a loss function which is the sum over the data of the squared prediction error plus a penalty on large values of \\(\\gamma_i\\).\n\n\nIn code\n\nlibrary(glmnet)\n\n\nridge &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)\n\nWe can visualize the ridge regression estimates just like the multilevel model estimates. Because the penalty applies to squared values of team deviations from the line, the points furthest from the line are most strongly regularized toward the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nThe amount of regularization will depend on the chosen value of the penalty parameter \\(\\lambda\\). To the degree that \\(\\lambda\\) is large, estimates will be more strongly pulled toward the regression line. Below is a visualization at three values of \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  bind_cols(fitted) |&gt;\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |&gt;\n  pivot_longer(cols = contains('Lambda')) |&gt;\n  distinct() |&gt;\n  mutate(name = fct_rev(name)) |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter \\(\\lambda\\) rises. The biase of the estimates also increases as the value of \\(\\lambda\\) increases. The optimal value of \\(\\lambda\\) is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\nCode\nmany_sample_estimates_ridge &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nConnections: Multilevel model and ridge regression\nMultilevel models and ridge regression are closely connected, and they can yield mathematically equivalent estimates in special cases. Consider again the multilevel model for the salary \\(Y_{ti}\\) of player \\(i\\) on team \\(t\\).\n\\[\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n\\] For simplicity, suppose we already have estimates \\(\\hat\\mu_0\\), \\(\\hat\\tau^2\\), and \\(\\hat\\sigma^2\\). One can show that the multilevel model estimates of \\(\\mu_t\\) are:\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\] This formula can be interpreted as analogous to a sample mean, but with some added observations. The first part of the numerator and denominator corresponds to a sample mean: the sum \\(\\sum_i Y_{ti}\\) of salaries of all sampled players in team \\(t\\) in the numerator and the number of such players \\(n_t\\) in the denominator. The right side of the numerator and denominator correspond to pseudo-observations. It is as though in addition to the sampled players, we also saw \\(\\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\) additional players with exactly the overall baseball mean salary estimate \\(\\hat\\mu_0\\). This part of the formula partially pools the team-specific mean toward this overall mean. Partial pooling is greater to the degree that there is small within-team variance (small \\(\\sigma^2\\)) or large across-team variance (large \\(\\hat\\tau^2\\)).\nNext, we consider a ridge regression estimator that minimizes an objective function where \\(\\hat\\mu_0\\) is an unpenalized estimate for the baseball-wide mean salary.2\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n\\] We can gain some additional intuition for this estimate by rearranging to pull the penalty into the main term.\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\] The first part of this term is the sum over observed squared errors within team \\(t\\), \\(\\sum_i\\left(Y_{it}-\\mu_t\\right)^2\\). The second part is as though we had observed an additional \\(\\lambda\\) cases within team \\(t\\) with an outcome value \\(\\mu_0\\) equal to the baseball-wide mean. With this intuition, the ridge regression estimator becomes\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\] which is the same as the multilevel model estimator in the special case when \\(\\lambda = \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\). Multilevel models and ridge regression are actually accomplishing the same thing!\nBelow, we use code to check that these two give the same thing. We first fit a multilevel model,\n\nmultilevel &lt;- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n\nThen we make predictions,\n\nyhat_multilevel &lt;- baseball_sample |&gt;\n  mutate(yhat_multilevel = predict(multilevel)) |&gt;\n  distinct(team, yhat_multilevel)\n\nand extract the implied value of \\(\\lambda\\) for an equivalent ridge regression (from the math above).\n\nlambda_equivalent &lt;- as_tibble(VarCorr(multilevel)) |&gt;\n  select(grp, vcov) |&gt;\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |&gt;\n  mutate(lambda_equivalent = Residual / team) |&gt;\n  pull(lambda_equivalent)\n\nNow we estimate ridge regression with that \\(\\lambda\\) value. Because glmnet internally rescales variables, it is difficult to carry out this check with glmnet. Instead, we will write our own ridge regression estimator. We first define our predictor matrix X and a mean-centered outcome vector y_centered. The reason to mean-center the outcome is to allow an unpenalized grand mean (\\(\\mu_0\\) in the math above). We will add this value back to predictions later.\n\nX &lt;- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered &lt;- baseball_sample$salary - mean(baseball_sample$salary)\n\nThe ridge regression estimator can be written in matrix form as follows:\n\\[\nY - \\hat\\mu_0 = \\mathbf{X}\\vec\\beta + \\epsilon\n\\] with \\(\\hat{\\vec\\beta}_\\text{Ridge} = (\\mathbf{X}'\\mathbf{X} + \\text{diag}(\\lambda))^{-1}\\mathbf{X}'\\vec{Y}\\). The code below estimates \\(\\vec\\beta\\)\n\nbeta_ridge &lt;- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\n\nBecause there is one \\(\\beta\\) value for each team, we convert to predicted values for each team by adding the grand mean to the estimated coefficients.\n\nyhat_ridge &lt;- beta_ridge + mean(baseball_sample$salary)\n\nFinally, we create a tibble with both the ridge regression and multilevel model estimates.\n\nboth_estimators &lt;- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |&gt;\n  rename(team = rowname) |&gt;\n  mutate(team = str_remove(team,\"team\")) |&gt;\n  left_join(yhat_multilevel, by = join_by(team))\n\nWe can confirm in code that the two predictions are numerically equal.\n\nboth_estimators |&gt;\n  # remove names; it is ok if names are unequal\n  mutate_all(unname) |&gt;\n  # summarize whether the two columns are equal\n  summarize(numerically_equal = all.equal(yhat_ridge, yhat_multilevel))\n\n# A tibble: 1 × 1\n  numerically_equal\n  &lt;lgl&gt;            \n1 TRUE             \n\n\nWe produce a plot with one point for each team, with ridge regression predictions on the \\(x\\)-axis and multilevel model predictions on the \\(y\\)-axis. We can see that these two estimators are equivalent.\n\n\nCode\nboth_estimators |&gt;\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_point() +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Ridge regression and multilevel models can yield\\nequal estimates for the mean salary on each team\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe equivalency of ridge regression and multilevel models may be surprising. Ridge regression is often motivated from a loss function (squared error + penalty), using terms that are common in data science and machine learning. Multilevel models are often motivated from sociological examples where units are clustered in groups, with terminology more common in statistics. Yet the two are mathematically related. An important difference is that multilevel models learn the amount of regularization from the data, whereas ridge regression needs an additional step to learn the penalty parameter (to be discussed in a future class session on data-driven estimator selection).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#lasso-regression",
    "href": "algorithms_for_prediction.html#lasso-regression",
    "title": "Algorithms for prediction",
    "section": "LASSO regression",
    "text": "LASSO regression\nLASSO regression is just like ridge regression except for one key difference: instead of penalizing the sum of squared coefficients, LASSO penalizes the sum of the absolute value of coefficients. As we will see, this change means that some parameters become regularized all the way to zero so that some terms drop out of the model completely.\nAs with ridge regression, our outcome model is\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\] where \\(Y_{ti}\\) is the salary of player \\(i\\) on team \\(t\\) and \\(X_t\\) is the past win-loss record of that team. In this model, \\(\\gamma_t\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_{ti}\\) is the deviation for player \\(i\\) from their own team’s average salary.\nTo solve the high-variance estimates of \\(\\gamma_t\\), LASSO regression uses a different penalty term in its loss function:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2 + \\lambda\\sum_t\\lvert\\gamma_t\\rvert\\right]\n\\]\nwhere \\(\\gamma_t^2\\) from the ridge regression penalty has been replaced by the absolute value \\(\\lvert\\gamma_t\\rvert\\). We will first apply this in code and then see how it changes the performance of the estimator.\n\nIn code\n\nlibrary(glmnet)\n\n\nlasso &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)\n\nVisualizing the estimates, we see behavior similar to what we have previously seen from multilevel models and ridge regression. All estimates are pulled toward a linear regression fit. There are two key differences, however.\nFirst, the previous estimators most strongly pulled the large deviations toward the linear fit. The LASSO estimates pull all parameter estimates toward the linear fit to a similar degree, regardless of their size. This is because the LASSO estimates penalize the absolute value of \\(\\gamma_i\\) instead of the squared value of \\(\\gamma_i\\).\nSecond, the multilevel and ridge estimates never pulled any of the estimates all the way to the line; instead, the estimates asymptoted toward the line as the penalty parameter grew. In LASSO, some estimates are pulled all the way to the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\). At a very small penalty, the estimate is approximately the same as the mean among the 5 sampled Dodger players. As the penalty parameter \\(\\lambda\\) gets larger, the estimates move around. They generally move toward zero, but not always: as some other team-specific deviations are pulled to zero, the unregularized intercept and slope on team past record move around in response. Ultimately, the penalty becomes so large that the Dodger estimate is regularized all the way to the estimate we would get in a model with no team-specific deviations.\n\n\nCode\nfitted &lt;- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs with ridge regression, we will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nSimilar to the ridge regression estimator, we can visualize the performance of the LASSO regression estimator across repeated samples from the population.\n\n\nCode\nmany_sample_estimates_lasso &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#trees",
    "href": "algorithms_for_prediction.html#trees",
    "title": "Algorithms for prediction",
    "section": "Trees",
    "text": "Trees\n\nTo read more on this topic, see Ch 8.4 of Efron & Hastie (2016)\n\nPenalized regression performs well when the the response surface \\(E(Y\\mid\\vec{X})\\) is well-approximated by the functional form of a particular assumed model. In some settings, however, the response surface may be more complex, with nonlinearities and interaction terms that the researcher may not know about in advance. In these settings, one might desire an estimator that adaptively learns the functional form from the data.\n\nA simulation to illustrate trees\nAs an example, the figure below presents some hypothetical data with a binary predictor \\(Z\\), a numeric predictor \\(X\\), and a numeric outcome \\(Y\\).\n\n\nCode\ntrue_conditional_mean &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\np_no_points &lt;- true_conditional_mean |&gt;\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\np &lt;- p_no_points +\n  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)\np\n\n\n\n\n\n\n\n\n\nIf we tried to approximate these conditional means with an additive linear model, \\[\\hat{E}_\\text{Linear}(Y\\mid X,Z) = \\hat\\alpha + \\hat\\beta X + \\hat\\gamma Z\\] then the model approximation error would be very large.\n\n\nCode\nbest_linear_fit &lt;- lm(mu ~ x + z, data = true_conditional_mean)\np +\n  geom_line(\n    data = true_conditional_mean |&gt;\n      mutate(mu = predict(best_linear_fit))\n  ) +\n  theme_bw() +\n  ggtitle(\"An additive linear model (solid lines) poorly approximates\\nthe true conditional mean function (dashed lines)\")\n\n\n\n\n\n\n\n\n\n\n\nTrees repeatedly split the data\nRegression trees begin from a radically different place. With no model at all, suppose we were to split the sample into two subgroups. For example, we might choose to split on \\(Z\\) and say that all units with z = TRUE are one subgroup while all units with z = FALSE are another subgroup. Or we might split on \\(X\\) and say that all units with x &lt;= .23 are one subgroup and all units with x &gt; .23 are another subgroup. After choosing a way to split the dataset into two subgroups, we would then make a prediction rule: for each unit, predict the mean value of all sampled units who fall in their subgroup. This rule would produce only two predicted values: one prediction per resulting subgroup.\nIf you were designing an algorithm to predict this way, how would you choose to define the split?\nIn regression trees to estimate conditional means, the split is often chosen to minimize the resulting sum of squared prediction errors. Suppose we choose this rule. Suppose we consider splitting on \\(X\\) being above or below each decile of its empirical distribution. Suppose we consider splitting on \\(Z\\) being FALSE or TRUE. The graph below shows the sum of squared prediction error resulting from each rule.\n\n\nCode\nx_split_candidates &lt;- quantile(simulated_data$x, seq(.1,.9,.1))\nz_split_candidates &lt;- .5\nby_z &lt;- simulated_data |&gt;\n  group_by(z) |&gt;\n  mutate(yhat = mean(y)) |&gt;\n  ungroup() |&gt;\n  summarize(sum_squared_error = sum((yhat - y) ^ 2))\nby_x &lt;- foreach(x_split = x_split_candidates, .combine = \"rbind\") %do% {\n  simulated_data |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n}\n\nby_x |&gt;\n  mutate(split = \"If Splitting on X\") |&gt;\n  rename(split_value = x_split) |&gt;\n  bind_rows(\n    by_z |&gt;\n      mutate(split = \"If Splitting on Z\") |&gt;\n      mutate(split_value = .5)\n  ) |&gt;\n  ggplot(aes(x = split_value, y = sum_squared_error)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~split) +\n  labs(\n    x = \"Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWith the results above, we would choose to split on \\(Z\\), creating a subpopulation with \\(Z \\leq .5\\) and a subgroup with \\(Z\\geq .5\\). Our prediction function would look like this. Our split very well approximates the true conditional mean function when Z = FALSE, but is still a poor approximator when Z = TRUE.\n\n\nCode\np +\n  geom_line(\n    data = simulated_data |&gt;\n      group_by(z) |&gt;\n      mutate(mu = mean(y))\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter one split on Z\")\n\n\n\n\n\n\n\n\n\nWhat if we make a second split? A regression tree repeats the process and considers making a further split within each subpopulation. The graph below shows the sum of squared error in the each subpopulation of Z when further split at various candidate values of X.\n\n\nCode\n# Split 2: After splitting by Z, only X remains on which to split\nleft_side &lt;- simulated_data |&gt; filter(!z)\nright_side &lt;- simulated_data |&gt; filter(z)\n\nleft_split_candidates &lt;- quantile(left_side$x, seq(.1,.9,.1))\nright_split_candidates &lt;- quantile(right_side$x, seq(.1,.9,.1))\n\nleft_split_results &lt;- foreach(x_split = left_split_candidates, .combine = \"rbind\") %do% {\n  left_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nright_split_results &lt;- foreach(x_split = right_split_candidates, .combine = \"rbind\") %do% {\n  right_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nsplit2_results &lt;- left_split_results |&gt; mutate(split1 = \"Among Z = FALSE\") |&gt;\n  bind_rows(right_split_results |&gt; mutate(split1 = \"Among Z = TRUE\"))\n\nsplit2_results |&gt;\n  ggplot(aes(x = x_split, y = sum_squared_error)) +\n  geom_line(color = 'gray') +\n  geom_point(aes(color = chosen)) +\n  scale_color_manual(values = c(\"gray\",\"blue\")) +\n  facet_wrap(~split1) +\n  theme_bw() +\n  labs(\n    x = \"X Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  )\n\n\n\n\n\n\n\n\n\nThe resulting prediction function is a step function that begins to more closely approximate the truth.\n\n\nCode\nsplit2_for_graph &lt;- split2_results |&gt;\n  filter(chosen) |&gt;\n  mutate(z = as.logical(str_remove(split1,\"Among Z = \"))) |&gt;\n  select(z, x_split) |&gt;\n  right_join(simulated_data, by = join_by(z)) |&gt;\n  mutate(x_left = x &lt;= x_split) |&gt;\n  group_by(z, x_left) |&gt;\n  mutate(yhat = mean(y))\n\np +\n  geom_line(\n    data = split2_for_graph,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter two splits on (Z,X)\")\n\n\n\n\n\n\n\n\n\nHaving made one and then two splits, the figure below shows what happens when each subgroup is the created by 4 sequential splits of the data.\n\n\nCode\nlibrary(rpart)\nrpart.out &lt;- rpart(\n  y ~ x + z, data = simulated_data, \n  control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n)\np +\n  geom_step(\n    data = true_conditional_mean |&gt;\n      mutate(mu_hat = predict(rpart.out, newdata = true_conditional_mean)),\n    aes(y = mu_hat)\n  ) +\n  ggtitle(\"Prediction from regression tree grown to depth 4\")\n\n\n\n\n\n\n\n\n\nThis prediction function is called a regression tree because of how it looks when visualized a different way. One begins with a full sample which then “branches” into a left and right part, which further “branch” off in subsequent splits. The terminal nodes of the tree—subgroups defined by all prior splits—are referred to as “leaves.” Below is the prediction function from above, visualized as a tree. This visualization is made possible with the rpart.plot package which we practice further down the page.\n\n\nCode\nlibrary(rpart.plot)\nrpart.plot::rpart.plot(rpart.out)\n\n\n\n\n\n\n\n\n\n\n\nYour turn: Fit a regression tree\nUsing the rpart package, fit a regression tree like the one above. First, load the package.\n\nlibrary(rpart)\n\nThen use this code to simulate data. If you are a Stata user, download this simulated data file from the website.\n\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\n\nUse the rpart function to grow a tree.\n\nrpart.out &lt;- rpart(y ~ x + z, data = simulated_data)\n\nFinally, we can define a series of predictor values at which to make predictions,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nand then make predictions\n\npredicted &lt;- predict(rpart.out, newdata = to_predict)\n\nand visualize in a plot.\n\nto_predict |&gt;\n  mutate(yhat = predicted) |&gt;\n  ggplot(aes(x = x, y = yhat, color = z)) +\n  geom_step()\n\n\n\n\n\n\n\n\nWhen you succeed, there are a few things you can try:\n\nVisualize the tree using the rpart.plot() function applied to your rpart.out object\nAttempt a regression tree using the baseball_population.csv data\nTry different specifications of the tuning parameters. See the control argument of rpart, explained at ?rpart.control. To produce a model with depth 4, we previously used the argument control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4).\n\n\n\nChoosing the depth of the tree\nHow deep should one make a tree? Recall that the depth of the tree is the number of sequential splits that define a leaf. The figure below shows relatively shallow trees (depth = 2) and relatively deep trees (depth = 4) learned over repeated samples. What do you notice about performance with each choice?\n\n\nCode\nestimator &lt;- function(maxdepth) {\n  foreach(rep = 1:3, .combine = \"rbind\") %do% {\n    this_sample &lt;- simulate(100)\n    rpart.out &lt;- rpart(y ~ x + z, data = this_sample, control = rpart.control(minsplit = 2, cp = 0, maxdepth = maxdepth))\n    true_conditional_mean |&gt;\n      mutate(yhat = predict(rpart.out, newdata = true_conditional_mean),\n             maxdepth = maxdepth,\n             rep = rep)\n  }\n}\nresults &lt;- foreach(maxdepth_value = c(2,5), .combine = \"rbind\") %do% estimator(maxdepth = maxdepth_value)\np_no_points +\n  geom_line(\n    data = results |&gt; mutate(maxdepth = case_when(maxdepth == 2 ~ \"Shallow Trees\\nDepth = 2\", maxdepth == 5 ~ \"Deep Trees\\nDepth = 5\")),\n    aes(group = interaction(z,rep), y = yhat)\n  ) +\n  facet_wrap(\n    ~maxdepth\n  )\n\n\n\n\n\n\n\n\n\nShallow trees yield predictions that tend to be more biased because the terminal nodes are large. At the far right when z = TRUE and x is large, the predictions from the shallow trees are systematically lower than the true conditional mean.\nDeep trees yield predictions that tend to be high variance because the terminal nodes are small. While the flexibility of deep trees yields predictions that are less biased, the high variance can make deep trees poor predictors.\nThe balance between shallow and deep trees can be chosen by various rules of thumb or out-of-sample performance metrics, many of which are built into functions like rpart. Another way out is to move beyond trees to forests, which involve a simple extension that yields substantial improvements in performance.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#forests",
    "href": "algorithms_for_prediction.html#forests",
    "title": "Algorithms for prediction",
    "section": "Forests",
    "text": "Forests\n\nTo read more on this topic, see Ch 17.1 of Efron & Hastie (2016)\n\nWe saw previously that a deep tree is a highly flexible learner, but one that may have poor predictive performance due to its high sampling variance. Random forests (Breiman 2001) resolve this problem in a simple but powerful way: reduce the variance by averaging the predictions from many trees. The forest is the average of the trees.\nIf one simply estimated a regression tree many times on the same data, every tree would be the same. Instead, each time a random forest grows a tree it proceeds by:\n\nbootstrap a sample \\(n\\) of the \\(n\\) observations chosen with replacement\nrandomly sample some number \\(m\\) of the variables to consider for splitting\n\nThere is an art to selection of the tuning parameter \\(m\\), as well as the parameters of the tree-growing algorithm. But most packages can select these tuning parameters automatically. The more trees you grow, the less the forest-based predictions will be sensitive to the stochastic variability that comes from the random sampling of data for each tree.\n\nIllustration with bagged forest\nFor illustration, we will first consider a simple version of random forest that is a bagging estimator: all predictors are included in every tree and variance is created through bagging, or bootstrap aggregating. The code below builds intuition, and the code later using the regression_forest function from the grf package is one way we would actually recommend learning a forest in practice.\n\ntree_estimates &lt;- foreach(tree_index = 1:100, .combine = \"rbind\") %do% {\n  # Draw a bootstrap sample of the data\n  simulated_data_star &lt;- simulated_data |&gt;\n    slice_sample(prop = 1, replace = T)\n  # Learn the tree\n  rpart.out &lt;- rpart(\n    y ~ x + z, data = simulated_data_star, \n    # Set tuning parameters to grow a deep tree\n    control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n  )\n  # Define data to predict\n  to_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n  # Make predictions\n  predicted &lt;- to_predict |&gt;\n    mutate(\n      yhat = predict(rpart.out, newdata = to_predict),\n      tree_index = tree_index\n    )\n  return(predicted)\n}\n\nWe can then aggregate the tree estimates into a forest prediction by averaging over trees.\n\nforest_estimate &lt;- tree_estimates |&gt;\n  group_by(z,x) |&gt;\n  summarize(yhat = mean(yhat), .groups = \"drop\")\n\nThe forest is very good at approximating the true conditional mean.\n\n\nCode\np_no_points +\n  geom_line(\n    data = forest_estimate,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are forest predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\n\n\nYour turn: A random forest with grf\nIn practice, it is helpful to work with a function that can choose the tuning parameters of the forest for you. One such function is the regression_forest() function in the grf package.\n\nlibrary(grf)\n\nTo illustrate its use, we first produce a matrix X of predictors and a vector Y of outcome values.\n\nX &lt;- model.matrix(~ x + z, data = simulated_data)\nY &lt;- simulated_data |&gt; pull(y)\n\nWe then estimate the forest with the regression_forest() function, here using the tune.parameters = \"all\" argument to allow automated tuning of all parameters.\n\nforest &lt;- regression_forest(\n  X = X, Y = Y, tune.parameters = \"all\"\n)\n\nWe can extract one tree from the forest with the get_tree() function and then visualize with the plot() function.\n\nfirst_tree &lt;- get_tree(forest, index = 1)\nplot(first_tree)\n\nTo predict in a new dataset requires a new X matrix,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict &lt;- model.matrix(~ x + z, data = to_predict)\n\nwhich can then be used to make predictions.\n\nforest_predicted &lt;- to_predict |&gt;\n  mutate(\n    yhat = predict(forest, newdata = X_to_predict) |&gt; \n      pull(predictions)\n  )\n\nWhen we visualize, we see that the forest from the package is also a good approximator of the conditional mean function. It is possible that the bias of this estimated forest arises from tuning parameters that did not grow sufficiently deep trees.\n\n\nCode\np_no_points +\n  geom_line(\n    data = forest_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are grf::regression_forest() predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\nOnce you have learned a forest yourself, you might try a regression forest using the baseball_population.csv data or another dataset of your choosing.\n\n\nForests as adaptive nearest neighbors\nA regression tree can be interpreted as an adaptive nearest-neighbor estimator: the prediction at predictor value \\(\\vec{x}\\) is the average outcome of all its neighbors, where neighbors are defined as all sampled data points that fall in the same leaf as \\(\\vec{x}\\). The estimator is adaptive because the definition of the neighborhood around \\(\\vec{x}\\) was learned from the data.\nRandom forests can likewise be interpreted as weighted adaptive nearest-neighbor estimators. For each unit \\(i\\), the predicted value is the average outcome of all other units where each unit \\(j\\) is weighted by the frequency with which it falls in the same leaf as unit \\(i\\). Seeing forest-based predictions as a weighted average of other units’ outcomes is a powerful perspective that has led to new advances in forests for uses that go beyond standard regression (Athey, Tibshirani, & Wager 2019).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#gradient-boosted-trees",
    "href": "algorithms_for_prediction.html#gradient-boosted-trees",
    "title": "Algorithms for prediction",
    "section": "Gradient boosted trees",
    "text": "Gradient boosted trees\n\nTo read more on this topic, see Ch 17.2 of Efron & Hastie (2016)\n\nGradient boosted trees are similar to a random forest in that the result is an average of trees. A key difference is how the trees are produced. In a random forest, the trees are all independent prediction functions learned in parallel. In boosting, the trees are learned sequentially, with each tree correcting the errors of preceding trees. More precisely, the algorithm begins by predicting the sample mean for all observations and defining residuals as the difference between the true outcome \\(Y\\) and the predicted value. Then the algorithm estimates a regression tree to model the residuals. It adds a regularized version of the predicted residuals to the predicted value, then calculates residuals and fits a new tree to predict the new residuals. Over a series of rounds, the predictions become sequentially closer to the truth, as visualized below.\n\n\nCode\nlibrary(xgboost)\nxgboost_illustration &lt;- foreach(round = 1:6, .combine = \"rbind\") %do% {\n  xgboost.out &lt;- xgboost(\n    data = model.matrix(~ x + z, data = simulated_data),\n    label = simulated_data |&gt; pull(y),\n    nrounds = round\n  )\n  to_predict |&gt;\n    mutate(yhat = predict(xgboost.out, newdata = X_to_predict),\n           round = paste(\"After\",round,ifelse(round == 1, \"round\", \"rounds\"),\"of boosting\"))\n}\np_no_points +\n  geom_line(data = xgboost_illustration,\n            aes(y = yhat)) +\n  facet_wrap(~round) +\n  ggtitle(\"Solid lines are xgboost::xgboost() predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\nA difficulty in boosting is knowing when to stop: the predictions improve over time but ultimately will begin to overfit. While more trees is always better in a random forest, the same is not true of boosting.\nTo learn more about boosting in math, we recommend Ch 17.2 of Efron & Hastie (2016). To try boosting, you can install the xgboost package in R and follow the code below.\n\nlibrary(xgboost)\n\nTo fit a boosting model, first define a predictor matrix and an outcome vector.\n\nX &lt;- model.matrix(~ x + z, data = simulated_data)\nY &lt;- simulated_data |&gt; pull(y)\n\nThe code below uses these data to carry out 10 rounds of boosting (recall that the number of rounds is a key tuning parameter, and 10 is only chosen for convenience).\n\nxgboost.out &lt;- xgboost(\n  data = model.matrix(~ x + z, data = simulated_data),\n  label = simulated_data |&gt; pull(y),\n  nrounds = 10\n)\n\nFinally, we can define new data at which to predict\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict &lt;- model.matrix(~ x + z, data = to_predict)\n\nand then produce predicted values.\n\nxgboost_predicted &lt;- to_predict |&gt;\n  mutate(yhat = predict(xgboost.out, newdata = X_to_predict))\n\nVisualizing the result, we can see that boosting performed well in our simulated example.\n\np_no_points +\n  geom_line(\n    data = xgboost_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are xgboost::xgboost() predictions.\\nDashed lines are the true conditional mean.\")",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#closing-thoughts",
    "href": "algorithms_for_prediction.html#closing-thoughts",
    "title": "Algorithms for prediction",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nAlgorithms for prediction are often powerful tools from data science. Because they are \\(\\hat{Y}\\) tools, their application in social science requires us to ask questions involving \\(\\hat{Y}\\). One such category of questions that have been the focus of this page are questions about conditional means. Questions about conditional means are \\(\\hat{Y}\\) questions because the conditional mean \\(E(Y\\mid\\vec{X} = \\vec{x})\\) would be the best possible prediction of \\(Y\\) given \\(\\vec{X} = \\vec{x}\\) when “best” is defined by squared error loss. We have therefore focused on algorithms for prediction that seek to minimize the sum of squared errors, since these algorithms may often be useful in social science as tools to estimate conditional means.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#footnotes",
    "href": "algorithms_for_prediction.html#footnotes",
    "title": "Algorithms for prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRaudenbush, S. W., and A.S. Bryk. (2002). Hierarchical linear models: Applications and data analysis methods. Advanced Quantitative Techniques in the Social Sciences Series/SAGE.↩︎\nWhile we write out \\(\\hat\\mu_0\\), algorithmic implementations of ridge regression often mean-center \\(Y\\) before applying the algorithm which is equivalent to having an unpenalized \\(\\hat\\mu_0\\).↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "resampling.html",
    "href": "resampling.html",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "",
    "text": "Here are slides and a PDF of this page. Notation and ideas on this page loosely draw on Efron & Hastie (2016) Ch 10–11.\nAs researchers adopt algorithmic estimation methods for which analytical standard errors do not exist, methods to produce standard errors by resampling become all the more important. We will discuss the bootstrap for simple random samples and extensions to allow resampling-based standard error estimates in complex survey samples.\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\nset.seed(90095)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#a-motivating-problem",
    "href": "resampling.html#a-motivating-problem",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "A motivating problem",
    "text": "A motivating problem\nOut of the population of baseball salaries on Opening Day 2023, imagine that we have a sample of 10 Dodger players.\n\npopulation &lt;- read_csv(\"data/baseball_population.csv\")\nsample &lt;- population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  sample_n(size = 10) |&gt;\n  select(player, team, salary)\n\nWe calculate the mean salary among the sampled Dodgers to be $3.8 million. How much should we trust this estimate?\nFor the sake of discussion, we provide the following information.\n\n\n# A tibble: 3 × 2\n  `Salary Among Sampled Dodgers`    Value\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 sample_mean                    3829119.\n2 sample_standard_deviation      6357851.\n3 sample_size                         10",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#classical-inference",
    "href": "resampling.html#classical-inference",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Classical inference",
    "text": "Classical inference\nTo know how confident to be in our sample-based estimate, we need to reason about why our sample-based estimate might differ from the true (but unknown) population parameter. Let \\(\\hat\\mu\\) denote our estimate for the sample mean of \\(Y\\).\n\\[\\hat\\mu = \\frac{1}{n}\\sum_{i}Y_i\\]\nAcross repeated samples from the population, the estimate \\(\\hat\\mu\\) equals the population mean on average but differs in any particular sample due to random sampling variance. The sample variance of the mean has a known formula.\n\\[\nV(\\hat\\mu) = V\\left(\\frac{1}{n}\\sum_i Y_i\\right) = \\frac{1}{n^2}\\sum_i V(Y_i) = \\frac{V(Y)}{n}\n\\]\nThe sample-to-sample variance of \\(\\hat\\mu\\) will be greater to the degree that \\(Y\\) varies substantially across individuals in the population (larger \\(V(Y)\\)), and will be smaller to the degree that many individuals are included in the sample (larger \\(n\\)).\nYou might be more familiar with this equation expressed as the standard deviation of the estimator, sometimes referred to as the standard error, which is the square root of the variance of the estimator,\n\\[\n\\text{SD}(\\hat\\mu) = \\sqrt{\\text{V}(\\hat\\mu)} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\n\\] where \\(\\text{SD}()\\) is the standard deviation of \\(Y\\) across individuals in the population.\nFrom the Central Limit Theorem, we know that even if \\(Y\\) is not Normally distributed the sample mean of \\(Y\\) converges to a Normal distribution as the sample size grows. Because we have formulas for these parameters, we can write down a formula for that sampling distribution.\n\\[\n\\hat\\mu \\rightarrow \\text{Normal}\\left(\\text{Mean} = \\text{E}(Y),\\quad \\text{SD} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\\right)\n\\] The graph below visualizes the sampling variability of the sample mean. Across repeated samples, the sample mean \\(\\hat\\mu\\) is normally distributed about its true population value. The middle 95% of sample estimates \\(\\hat\\mu\\) fall within a region that can be derived with known formulas,\n\n\n\n\n\n\n\n\n\nwhere \\(\\Phi^{-1}()\\) is the inverse CDF of the standard Normal distribution.\nYou might be concerned: can a Normal distribution be a good approximation when Dodger player salaries are highly right-skewed? After all this is the distribution of Dodger player salaries.\n\n\n\n\n\n\n\n\n\nBut the sample mean among 10 sampled Dodgers is actually quite close to a normal sampling distribution. This is because of the Central Limit Theorem.\n\n\n\n\n\n\n\n\n\n\nPlug-in estimators\nWe have a formula for the standard deviation of the sample mean, but it involves the term \\(\\text{SD}(Y)\\) which is the unknown population standard deviation of \\(Y\\). It is common to plug in the sample estimate of this value in order to arrive at a sample estimate of the standard deviation of the estimator.\n\\[\n\\widehat{\\text{SD}}(\\hat\\mu) = \\frac{\\widehat{\\text{SD}}(Y)}{\\sqrt{n}} = \\sqrt{\\frac{\\frac{1}{n-1}\\sum_i (Y_i - \\bar{Y})^2}{n}}\n\\]\nThe idea of a plug-in estimator may seem obvious, but soon we will see that the step at which plug-ins occur changes dramatically when we move to resampling methods for statistical inference.\n\n\nClassical confidence intervals\nA 95% confidence interval \\((\\hat\\mu_\\text{Lower},\\hat\\mu_\\text{Upper})\\) is an interval that has the property that across repeated samples the probability that \\(\\hat\\mu_\\text{Lower} &lt; \\mu &lt; \\hat\\mu_\\text{Upper}\\) is 0.95. One way to think about this is that two properties should hold: the probability that the lower limit is too high and the probability that the upper limit is too low are each 0.025.\n\\[\n\\begin{aligned}\n\\text{P}(\\hat\\mu_\\text{Lower} &gt; \\mu) &= .025 \\\\\n\\text{P}(\\hat\\mu_\\text{Upper} &lt; \\mu) &= .025\n\\end{aligned}\n\\]\nYou may know from statistics that a 95% confidence interval for the sample mean can be derived as follows.\n\\[\n\\hat\\mu \\pm \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\n\\] where \\(\\Phi^{-1}(.975)\\) is the value 1.96 that you might look up in the back of a statistics textbook. We can show that these confidence limits have the desired properties. For example, taking the lower limit:\n\\[\n\\begin{aligned}\n&\\text{P}\\left(\\hat\\mu_\\text{Lower} &gt; \\mu\\right)\\\\\n&=\\text{P}\\left(\\hat\\mu - \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu) &gt; \\mu\\right)\\\\\n&= \\text{P}\\left(\\hat\\mu - \\mu &gt; \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\\right)\\\\\n&= \\text{P}\\left(\\frac{\\hat\\mu - \\mu}{\\widehat{\\text{SD}}(\\hat\\mu)} &gt; \\Phi^{-1}(.975)\\right)\\\\\n&= .025\n\\end{aligned}\n\\]\nwhere the last line holds because \\(\\frac{\\hat\\mu - \\mu}{\\text{SD}(\\hat\\mu)}\\) follows a standard Normal distribution. The proof for the upper limit is similar.\nAcross repeated samples, a 95% confidence interval constructed in this way should contain the true mean 95% of the time. We can visualize this behavior by taking repeated samples of 10 Dodger players from our data.\n\n\n\n\n\n\n\n\n\nIn this particular simulation, we have slight undercoverage and the upper confidence limit is often the one that is incorrect. These problems may arise because our asymptotic normality of mean salaries is an imperfect approximation at a sample size of \\(n = 10\\).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#analytic-vs-computational-inference-procedures",
    "href": "resampling.html#analytic-vs-computational-inference-procedures",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Analytic vs computational inference procedures",
    "text": "Analytic vs computational inference procedures\nAnalytical confidence intervals (derived by math) are the default for many researchers. Yet the exercise above reveals some of their shortcomings. First, there is a lot of math! Second, despite the math we still relied on the plug-in principle: for unknown quantities such as \\(\\text{SD}(Y)\\) we plug in sample-based estimates \\(\\widehat{\\text{SD}}(Y)\\) and act as though these were known. Third, our results may still yield imperfect coverage because underlying assumptions may be only approximately met. For example, our confidence intervals may have undercovered because the asymptotics of the Central Limit Theorem are unreliable at \\(n = 10\\).\nNow suppose you had a complicated data science approach, such as a predicted value \\(\\hat{Y}_{\\vec{x}}=\\hat{\\text{E}}(Y\\mid \\vec{X} = \\vec{x})\\) from a LASSO regression. How would you place a confidence interval on that predicted value?\nComputational inference procedures take a different approach. These procedures focus on a generic estimator \\(s()\\) applied to data. Instead of deriving properties of the estimator by math, computational approaches seek to simulate what would happen when \\(s()\\) is applied to samples from the population, often by using a plug-in principle at an earlier step.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#the-estimator-function-s",
    "href": "resampling.html#the-estimator-function-s",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "The estimator function \\(s()\\)",
    "text": "The estimator function \\(s()\\)\nAt the core of a resampling-based inference procedure is a broad sense of how our estimate comes to be. First, the world has some cumulative distribution function \\(F\\) over data that could be generated. A particular sample \\(\\texttt{data}\\) is drawn from the probability distribution of the world. The researcher then applies an estimator function \\(s()\\) that takes in and returns an estimate \\(s(\\texttt{data})\\).\n\\[F\\rightarrow \\texttt{data} \\rightarrow s(\\texttt{data})\\]\nIn our baseball example, the estimator function is the sample mean of the salary variable.\n\nestimator &lt;- function(data) {\n  data |&gt;\n    summarize(estimate = mean(salary)) |&gt;\n    pull(estimate)\n}\n\nWe would like to repeatedly simulate \\(\\texttt{data}\\) from the world and see the performance of the estimator. But this is only possible in illustrations like the baseball example where the population data are known. When \\(F\\) is unknown and we only see one \\(\\texttt{data}\\), we need a new procedure.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#the-nonparametric-bootstrap",
    "href": "resampling.html#the-nonparametric-bootstrap",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "The nonparametric bootstrap",
    "text": "The nonparametric bootstrap\nThe nonparametric bootstrap simulates repeated-sample behavior by a plug-in principle.\n\nPlug in the empirical distribution \\(\\hat{F}\\) of our sample data as an estimate of the true distribution \\(F\\) for the unobserved full population of data\nGenerate a bootstrap sample \\(\\texttt{data}^*\\) by sampling from our empirical data with replacement.\nGenerate an estimate \\(s(\\texttt{data}^*)\\) using the bootstrap data.\nRepeat steps (2) and (3) many times to generate a large number \\(B\\) of bootstrap replicate estimates.\n\nVisually, this procedure is analogous to the above.\n\\[\\hat{F}\\rightarrow \\texttt{data}^* \\rightarrow s(\\texttt{data}^*)\\]",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#nonparametric-bootstrap-standard-errors",
    "href": "resampling.html#nonparametric-bootstrap-standard-errors",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Nonparametric bootstrap standard errors",
    "text": "Nonparametric bootstrap standard errors\nIn classical statistics, the standard error of an estimator is typically a mathematical expression derived for that particular estimator and then estimated by the plug-in principle. For example, the standard error of the mean is \\(\\text{SD}(\\hat\\mu) = \\text{SD}(Y) / \\sqrt{n}\\).\nWith the bootstrap, we avoid this altogether because we have \\(B\\) bootstrap replicate estimates. We can estimate the standard deviation of the estimator over repeated samples by the empirical standard deviation across bootstrap replicates.\n\\[\n\\widehat{\\text{SD}}(s) = \\frac{1}{B-1}\\sum_{r=1}^B \\bigg(s(\\texttt{data}^*_r) - s(\\texttt{data}^*_\\bullet)\\bigg)^2\n\\] where \\(s(\\texttt{data}^*_\\bullet)\\) is the mean of the estimate across the bootstrap samples. Note that just like the analytic standard errors, these have also relied on a plug-in principle: we plugged in the empirical distribution \\(\\hat{F}\\) for the population distribution \\(F\\) when generating bootstrap samples from our empirical data instead of actual samples from the population.\nIn our baseball example, here is our sample of 10 Dodger players.\n\n\n# A tibble: 10 × 3\n   player           team           salary\n   &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;\n 1 Barnes, Austin   L.A. Dodgers  3500000\n 2 Reyes, Alex*     L.A. Dodgers  1100000\n 3 Betts, Mookie    L.A. Dodgers 21158692\n 4 Vargas, Miguel   L.A. Dodgers   722500\n 5 May, Dustin      L.A. Dodgers  1675000\n 6 Bickford, Phil   L.A. Dodgers   740000\n 7 Jackson, Andre   L.A. Dodgers   722500\n 8 Thompson, Trayce L.A. Dodgers  1450000\n 9 Pepiot, Ryan*    L.A. Dodgers   722500\n10 Peralta, David   L.A. Dodgers  6500000\n\n\nThe code below generates a bootstrap sample from these 10 players by sampling 10 players with replacement. You will see that some players in the original sample do not appear, and others appear more than once.\n\nsample |&gt;\n  slice_sample(prop = 1, replace = TRUE)\n\n# A tibble: 10 × 3\n   player         team           salary\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 Betts, Mookie  L.A. Dodgers 21158692\n 2 Peralta, David L.A. Dodgers  6500000\n 3 Barnes, Austin L.A. Dodgers  3500000\n 4 Pepiot, Ryan*  L.A. Dodgers   722500\n 5 Jackson, Andre L.A. Dodgers   722500\n 6 May, Dustin    L.A. Dodgers  1675000\n 7 Reyes, Alex*   L.A. Dodgers  1100000\n 8 May, Dustin    L.A. Dodgers  1675000\n 9 Vargas, Miguel L.A. Dodgers   722500\n10 Peralta, David L.A. Dodgers  6500000\n\n\nThe code below carries out 500 bootstrap samples and estimates the sample mean in each one.\n\nbootstrap_estimates &lt;- foreach(r = 1:1000, .combine = \"c\") %do% {\n  sample |&gt;\n    # Draw a bootstrap sample\n    slice_sample(prop = 1, replace = TRUE) |&gt;\n    # Apply the estimator\n    estimator()\n}\n\nThe figure below shows how that the bootstrap distribution of the estimator compares to the actual sampling distribution of the estimator (known in this case since the population is known).\n\n\n\n\n\n\n\n\n\nThe bootstrap distribution is more heaped on 10 distinct salary values: the particular 10 Dodger player salaries included in our sample. When the variable being summarized takes continuous values, it will generally be more discretized in the bootstrap setting because there are only the sample size \\(n\\) distinct values instead of the population size \\(N\\) of distinct values. Otherwise, the two distributions are similar.\nThe bootstrap estimate of the standard error in this case is\n\nbootstrap_estimates |&gt; sd()\n\n[1] 1965073\n\n\nwhich is 86% of the size of the theoretical standard error of 2.2969632^{6}. Like all sample-based analogs to theoretical standard errors, the bootstrap estimate of the standard error can itself be sensitive to sampling variability.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#bootstrap-confidence-intervals",
    "href": "resampling.html#bootstrap-confidence-intervals",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Bootstrap confidence intervals",
    "text": "Bootstrap confidence intervals\nThe discussion above has focused on using the bootstrap to estimate standard errors. There are many methods to construct confidence intervals using bootstrap procedures. Two of the most common are the Normal approximation method and the percentile method.\n\nNormal approximation\nThe beginning of this page reviewed classical statistics in which we routinely rely on the Central Limit Theorem which ensures that sample mean estimators are asymptotically Normal. Likewise with the bootstrap, if we believe that \\(s(\\texttt{data})\\) has a Normal sampling distribution, then we can construct a confidence interval by the Normal approximation with the bootstrap estimate of the standard error.\n\\[\ns(\\texttt{data}) \\pm \\Phi^{-1}(.975)\\text{SD}\\big(s(\\text{data}^*)\\big)\n\\]\n\nestimator(sample) + c(-1,1) * qnorm(.975) * sd(bootstrap_estimates)\n\n[1]  -22353.11 7680591.51\n\n\n\n\nPercentile method\nThe bootstrap also offers another way to calculate the confidence interval: the middle 95% of the bootstrap estimates.\n\nquantile(bootstrap_estimates, probs = c(.025, .975))\n\n   2.5%   97.5% \n1103406 8216408 \n\n\n\nThe percentile method can work better than the Normal approximation method in cases where normality does not hold. For example, in the beginning of this page we used analytic intervals that seemed imperfect in part because the Central Limit Theorem had not adequately yielded normality at a sample size of 10.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#bootstrap-for-machine-learning-algorithms",
    "href": "resampling.html#bootstrap-for-machine-learning-algorithms",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Bootstrap for machine learning algorithms",
    "text": "Bootstrap for machine learning algorithms\nSuppose a researcher carries out the following procedure.\n\nSample \\(n\\) units from the population\nLearn an algorithm \\(\\hat{f}:\\vec{X}\\rightarrow Y\\) to minimize squared error\nReport a prediction \\(\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}) = \\hat{f}(\\vec{x})\\)\n\nHow would the researcher use the bootstrap to carry out this process?\n\nDraw a bootstrap sample \\(\\texttt{data}^*\\) of size \\(n\\)\nLearn the algorithm \\(\\hat{f}^*\\) in the bootstrap sample\nStore the bootstrap estimate \\(\\hat{f}^*(\\vec{x})\\)\n\nThen the researcher could create a confidence interval with either the Normal approximation or the percentile method. Note that the bootstrap confidence interval may have undercoverage if the estimator is biased; see the words of warning at the end of this page.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#discussion-what-belongs-in-s",
    "href": "resampling.html#discussion-what-belongs-in-s",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Discussion: What belongs in \\(s()\\)?",
    "text": "Discussion: What belongs in \\(s()\\)?\nIn each example, describe the steps the researcher might use to bootstrap this estimate while capturing all sources of uncertainty.\n\nA researcher first truncates the values of a skewed predictor variable \\(x\\) at the 1st and 99th percentile. Then the researcher learns a regression model and reports \\(\\hat\\beta\\).\nA researcher first uses cross-validation to select the tuning parameter \\(\\lambda\\) for ridge regression. Then, they estimate ridge regression with the chosen \\(\\lambda\\) value and make a prediction \\(\\hat{f}(\\vec{x})\\) at some predictor value \\(\\vec{x}\\) of interest.\nA researcher first learns a prediction function \\(\\hat{f}:\\vec{X}\\rightarrow Y\\) and then sees which subgroup \\(\\vec{x}\\) has the highest predicted value \\(\\hat{f}(\\vec{x})\\), which the researcher reports.\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\nMany steps of the analysis involve uncertainty. It can be ideal to include them all in your bootstrap! Write your estimator function to take in your raw data and return an estimate. The estimator function would include steps like truncating predictors at sample quantiles, choosing tuning parameters, and choosing subgroups of interest on which to focus.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#beyond-simple-random-samples",
    "href": "resampling.html#beyond-simple-random-samples",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Beyond simple random samples",
    "text": "Beyond simple random samples\nThe bootstrap in its simplest form is designed for simple random samples. Straightforward generalizations make it possible to move beyond simple random samples to more complex sampling designs.\n\nStratified bootstrap\nSuppose we draw a sample of players stratified by team: 10 players per team. No matter which random sample is drawn, there will always be 10 Dodgers, 10 Angels, 10 Yankees, and so on. Stratified sampling is often a more efficient estimator than simple random sampling, and our estimator should reflect that!\nAs an example, suppose we have a stratified sample of 10 players per team.\n\nstratified_sample &lt;- population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  ungroup()\n\nWe would generate a stratified bootstrap sample1 by stratifying by team, exactly as the data were generated.\n\nstratified_bootstrap_sample &lt;- stratified_sample |&gt;\n  group_by(team) |&gt;\n  slice_sample(prop = 1, replace = T)\n\nStratified bootstrapping can be important. Using our baseball example, suppose our estimator is the predicted mean salary of the Dodgers from a linear regression.\n\nestimator &lt;- function(data) {\n  ols &lt;- lm(salary ~ team_past_salary, data = data)\n  to_predict &lt;- population |&gt; \n    filter(team == \"L.A. Dodgers\") |&gt; \n    distinct(team_past_salary)\n  predicted &lt;- predict(ols, newdata = to_predict)\n  return(predicted)\n}\n\nWe get different estimates if we carry out simple vs stratified bootstrap sampling.\n\n\n\n\n\n\n\n\n\n\n\nCluster bootstrap\nSuppose we draw a sample of players clustered by team: all players on 10 sampled teams. Clustered sampling is often less expensive than simple random sampling because it can be easier for the person carrying out the survey. This often comes at a cost of statistical efficiency.\nAs an example, suppose we have a clustered sample of 10 teams.\n\nclustered_sample &lt;- population |&gt;\n  distinct(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  left_join(population, by = join_by(team))\n\nWe would generate a clustered bootstrap sample by resampling teams instead of players, exactly as the data were sampled.\n\nchosen_teams &lt;- clustered_sample |&gt;\n  distinct(team) |&gt;\n  slice_sample(prop = 1, replace = T)\nclustered_bootstrap_sample &lt;- foreach(i = 1:nrow(chosen_teams), .combine = \"rbind\") %do% {\n  chosen_teams[i,] |&gt;\n    left_join(clustered_sample, by = join_by(team))\n}\n\nAs before, we get different estimated standard errors if we carry out clustered bootstrap sampling vs standard bootstrap sampling.\n\n\n\n\n\n\n\n\n\n\n\nComplex survey samples\nMany surveys involve complex samples, such as samples stratified by state and then clustered in regions within states. Often the variables that define sampling strata or clusters are geographic, and therefore they are often redacted from the data made available to researchers due to privacy concerns.\nThankfully, many surveys make replicate weights available to researchers. The goal of replicate weights is to enable you to resample the data in a way that mimics the (hidden) ways in which the sample was originally drawn. The rest of this section walks through the use of replicate weights, first in a hypothetical example and then in real data.\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.\nWhen you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval2 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.\n\n\nComputational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.\n\n\nApplication in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#a-word-of-warning",
    "href": "resampling.html#a-word-of-warning",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "A word of warning",
    "text": "A word of warning\nThe bootstrap is a powerful tool, but there are notable cases in which it fails.\nFirst, all frequentist confidence intervals that are based solely on sampling variance may suffer undercoverage if applied to biased estimators. For example, many machine learning algorithms induce bias through regularization. This means that even if we correctly approximate sampling variance, the center of our confidence intervals may be systematically misaligned from the true population parameter, yielding undercoverage.\nSecond, the bootstrap can exhibit unexpected performance with statistics such as the maximum or minimum value, since these statistics can be sensitive to a particular data point. Taking the maximum as an example, the value \\(\\text{max}(\\vec{y}^*)\\) in a bootstrap sample will never be higher than \\(\\text{max}(\\vec{y})\\) in the sample from which that bootstrap was drawn. The entire bootstrap distribution of \\(\\text{max}(\\vec{y}^*)\\) will be at or below the original estimate of \\(\\text{max}(\\vec{y})\\). Like the max or min, quantiles of \\(\\vec{y}\\) can also lead to unexpected bootstrap behavior. Generally the bootstrap will have the best performance for statistics such as the mean for which no particular unit plays an especially determining role.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "resampling.html#footnotes",
    "href": "resampling.html#footnotes",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCalled the “multi-sample bootstrap” in Efron & Hastie.↩︎\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "prediction_for_inference.html",
    "href": "prediction_for_inference.html",
    "title": "Prediction for causal and population inference",
    "section": "",
    "text": "Here are slides and a pdf of this page.\nThis session will be about prediction to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators. If you have a Census with features \\(\\vec{X}\\), ignorable sampling conditional on \\(\\vec{X}\\), and a good sample estimator of \\(E(Y\\mid\\vec{X})\\) then you can predict \\(E(Y\\mid\\vec{X})\\) and aggregate over the Census-known population distribution of \\(\\vec{X}\\). For causal inference, being assigned to treatment is analogous to being sampled to observe the potential outcome under treatment.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#prediction-to-describe-with-non-probability-samples",
    "href": "prediction_for_inference.html#prediction-to-describe-with-non-probability-samples",
    "title": "Prediction for causal and population inference",
    "section": "Prediction to describe with non-probability samples",
    "text": "Prediction to describe with non-probability samples\nSuppose you collect a non-probability sample, such as people willing who respond on a survey on Amazon Mechanical Turk. Each person answers three questions:\n\nWhat is your sex?\nWhat is your age?\nHave you ever had a TikTok account?\n\nYou want to estimate the rate of using TikTok in the full U.S. population. But you worry about your sample: perhaps the age distribution of your sample is younger than the U.S. population, and younger people are more likely to have a TikTok account. Formally, whether a unit is sampled \\(S\\) is not independent of that unit’s outcome \\(Y\\).\n\\[\nS \\not\\unicode{x2AEB} Y\n\\]\nYou hope that it might help that your survey also measured sex and age. One thing you might do is to use logistic regression to estimate an outcome model to predict the probability of having a TikTok within these subgroups,\n\\[\\text{logit}\\left(\\hat{\\text{P}}(Y = 1\\mid S = 1, \\vec{X})\\right) = \\hat\\beta_0 + \\hat\\beta_1(\\text{Sex = Female}) + \\hat\\beta_2 (\\text{Age})\\]\nwhere \\(\\vec{X}\\) contains the variables sex and age. Now you can predict the probability of having a TikTok within any subgroup defined by sex and age.\nBut how to use these predictions? You might think it would help to have the population distribution of sex and age from the U.S. Census Bureau. From these data, you know the population distribution of \\(\\vec{X}\\).\n\\[\n{\\text{P}}(\\vec{X} = \\vec{x}) \\text{ is known for all }\\vec{x}\n\\]\nYou have an estimate of \\({\\text{P}}(Y = 1\\mid S, \\vec{X} = \\vec{x})\\) and a known value for \\({\\text{P}}(\\vec{X} = \\vec{x})\\). We would like to estimate the population mean by averaging the subgroup predictions over the distribution of subgroups.\n\\[\n{\\text{P}}(Y) = \\sum_{\\vec{x}}\\text{P}(Y\\mid\\vec{X} = \\vec{x}){\\text{P}}(\\vec{X} = \\vec{x})\n\\] But unfortunately, you still do not know the population mean TikTok use within groups defined by sex and age! While you have an estimate in your sample \\(\\hat{\\text{P}}(Y \\mid S = 1, \\vec{X} = \\vec{x})\\) this may not be the same as the population mean \\(\\text{P}(Y\\mid\\vec{X} = \\vec{x})\\).\nHere we will make a heroic assumption1 of conditionally ignorable sampling:\n\\[\nS \\unicode{x2AEB} Y \\mid \\vec{X} \\qquad (\\text{equivalently}) \\qquad {\\text{P}}(Y\\mid S = 1, \\vec{X} = \\vec{x}) = {\\text{P}}(Y \\mid \\vec{X} = \\vec{x})\n\\] This assumption says that whether one is sampled \\(S\\) tells me nothing about TikTok use \\(Y\\) once I am looking within a subgroup of people of a particular age and particular sex (\\(\\vec{X} = \\vec{x}\\)).\n\nGroup task. Draw a DAG to defend this assumption. Note possible edges that would undermine this assumption.\n\nIf the assumption holds, one can use the predicted values from the regression to estimate the population average value of \\(Y\\).\n\\[\n\\hat{\\text{P}}(Y) = \\sum_{\\vec{x}}\\hat{\\text{P}}(Y\\mid\\vec{X} = \\vec{x}){\\text{P}}(\\vec{X} = \\vec{x})\n\\]\nThis suggests a procedure to use regression to estimate population means:\n\nmeasure \\(\\vec{X}\\) and \\(Y\\) in a non-probability sample\nmeasure \\(\\vec{X}\\) in a probability sample or census\nassume exchangeable sampling given \\(\\vec{X}\\)\n\n(often a heroic assumption!)\n\nmodel \\(\\text{E}(Y\\mid\\vec{X})\\) or \\(\\text{P}(Y\\mid\\vec{X})\\) in the non-probability sample\nestimate \\(\\text{P}(\\vec{X} = \\vec{x})\\) in the probability sample or census\nre-aggregate \\(\\hat{\\text{E}}(Y\\mid\\vec{X})\\) using the weights \\(\\hat{\\text{P}}(\\vec{X} = \\vec{x})\\)\n\nYou could also use a weighting procedure, to be discussed next class.\n\nReal example: Xbox survey\nIn a survey carried out in 2012 on the Xbox gaming platform, Wang et al. (2015) asked respondents: “If the election were held today, who would you vote for?”\nWhy might this survey make for poor forecasts of the election outcome? The respondents looked very different from the U.S. electorate, notably much younger and much more likely to be men (see Fig 1 in the original paper). But the data were also very rich. There were over 700,000 responses. The researchers collected many demographic variables: sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.\nUsing these variables, the authors used a multilevel model to estimate the probability of supporting Obama in the 2012 presidential election. Under an assumption of exchangeable sampoling within subgroups, they were able to proportion supporting Obama within each subgroup \\(\\vec{X} = \\vec{x}\\).\n\\[\n\\hat{\\text{P}}(Y = 1\\mid S = 1,\\vec{X} = \\vec{x}) = \\text{logit}^{-1}(\\text{complicated function of }\\vec{x})\n\\] Then the authors used exit poll data from the 2008 election to estimate the population distribution of \\(\\vec{X}\\). They worked under the assumption that the 2012 electorate would be demographically similar to the 2008 electorate. Putting these together, they produced an overall estimate:\n\\[\n\\hat{\\text{P}}(Y = 1) = \\sum_{\\vec{x}}\\underbrace{\\hat{\\text{P}}(\\vec{X} = \\vec{x})}_{\\substack{\\text{Stratum size,}\\\\\\text{estimated from}\\\\\\text{2008 exit polls}}}\\underbrace{\\hat{\\text{P}}(Y = 1\\mid S = 1,\\vec{X} = \\vec{x})}_{\\substack{\\text{Prediction within the stratum,}\\\\\\text{estimated from Xbox survey}}}\n\\]\nThey also made predictions within particular states…and the predictions were remarkably accurate! Conditionally exchangeable sampling may be a heroic assumption, but it worked well in this particular case.\n\n\nTakeaways: Prediction to describe\nPredictive outcome models can greatly improve the usefulness of non-probability samples for population inferences. But there are a few key requirements to remember.\nFirst, you are studying \\(Y\\) but the real key is \\(\\vec{X}\\)!\n\n\\(\\vec{X}\\) must create conditional exchangeability: \\(S\\indep Y\\mid\\vec{X}\\)\n\\(\\vec{X}\\) must be measured in a probability sample or census\n\\(\\vec{X}\\) must be measured in the non-probability sample\n\nConsideration (1) may require a very extensive set of variables be included in \\(\\vec{X}\\). But even if you can measure them in your non-probability sample (2), you need to also be able to estimate their population distribution (3). In practice, the feasibility of (3) often leads to estimates that use only a small set of \\(\\vec{X}\\) variables, rendering (1) perhaps less credible.\nSecond, the task of moving from a non-probability sample to a population estimate does not necessarily require an outcome model; it can also be carried out by weighting (next class). The added benefit of a model is that it might produce better subgroup mean estimates \\(\\text{E}(Y\\mid\\vec{X} = \\vec{x})\\) by pooling information across subgroups (e.g., by a line).",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#causal-inference-example-on-paper",
    "href": "prediction_for_inference.html#causal-inference-example-on-paper",
    "title": "Prediction for causal and population inference",
    "section": "Causal inference: Example on paper",
    "text": "Causal inference: Example on paper\nPrediction for causal inference proceeds by the same general process. For simplicity, we will assume one probability sample.\n\nassume conditional exchangeability: \\(\\{Y^1,Y^0\\} \\unicode{x2AEB} A\\mid \\vec{X}\\)\nmodel \\(\\text{E}(Y\\mid A,\\vec{X})\\)\npredict \\(Y^1\\) and \\(Y^0\\) for all units\naverage over units\n\nWe will start with a simple example that can be carried out on paper. Suppose a researcher estimates the following regression of \\(Y\\) on treatment \\(A\\) and a confounder \\(X\\), which we assume is a sufficient adjustment set.\n\\[\n\\hat{\\text{E}}(Y\\mid \\vec{X}, A) = \\hat\\beta_\\text{Intercept} + \\hat\\beta_X X + \\hat\\beta_A A + \\hat\\beta_{XA} X A\n\\]\nA hypothetical set of estimates are provided below. \\[\n\\begin{aligned}\n\\hat\\beta_\\text{Intercept} &= 0 \\\\\n\\hat\\beta_X &= 1 \\\\\n\\hat\\beta_A &= 2 \\\\\n\\hat\\beta_{XA} &= 1 \\\\\n\\end{aligned}\n\\]\nYou want to estimate the average treatment effect over a population of four units. Using this model, can you fill in estimates for \\(\\hat{Y}^1\\) and \\(\\hat{Y}^0\\)? What is the average causal effect?\n\n\n\nID\n\\(X\\)\n\\(\\hat{Y}^1\\)\n\\(\\hat{Y}^0\\)\n\\(\\hat{Y}^1 - \\hat{Y}^0\\)\n\n\n\n\n1\n0\n?\n?\n?\n\n\n2\n1\n?\n?\n?\n\n\n3\n1\n?\n?\n?\n\n\n4\n1\n?\n?\n?",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#causal-inference-with-ols-simulated-example-in-code",
    "href": "prediction_for_inference.html#causal-inference-with-ols-simulated-example-in-code",
    "title": "Prediction for causal and population inference",
    "section": "Causal inference with OLS: Simulated example in code",
    "text": "Causal inference with OLS: Simulated example in code\nNext, we consider outcome modeling for causal inference using simulated data in code. The code below will generate a dataset of \\(n = 100\\) observations. Each observation contains several observed variables:\n\nL1 A numeric confounder\nL2 A numeric confounder\nA A binary treatment\nY A numeric outcome\n\nEach observation also contains outcomes that we know only because the data are simulated. These variables are useful as ground truth in simulations.\n\npropensity_score The true propensity score \\(P(A = 1 \\mid \\vec{L})\\)\nY0 The potential outcome under control\nY1 The potential outcome under treatment\n\nTo run this code, you will need the dplyr package. If you don’t have it, first run the line install.packages(\"dplyr\") in your R console. Then, add this line to your R script to load the package.\n\nlibrary(dplyr)\n\nIf you want your simulation to match our numbers exactly, add a line to set your seed.\n\nset.seed(90095)\n\n\nn &lt;- 500\ndata &lt;- tibble(\n  L1 = rnorm(n),\n  L2 = rnorm(n)\n) |&gt;\n  # Generate potential outcomes as functions of L\n  mutate(Y0 = rnorm(n(), mean = L1 + L2, sd = 1),\n         Y1 = rnorm(n(), mean = Y0 + 1, sd = 1)) |&gt;\n  # Generate treatment as a function of L\n  mutate(propensity_score = plogis(-2 + L1 + L2)) |&gt;\n  mutate(A = rbinom(n(), 1, propensity_score)) |&gt;\n  # Generate factual outcome\n  mutate(Y = case_when(A == 0 ~ Y0,\n                       A == 1 ~ Y1))\n\nA simulation is nice because the answer is known. In this simulation, the conditional average causal effect of A on Y equals 1 at any value of L1 and L_2.\nBecause the causal effect of A on Y is identified by adjusting for the confounders L1 and L2, we can estimate by outcome modeling.\n\nModel \\(E(Y\\mid A, L_1, L_2)\\), the conditional mean of \\(Y\\) given the treatment and confounders\nPredict potential outcomes\n\nset A = 1 for every unit. Predict \\(Y^1\\)\nset A = 0 for every unit. Predict \\(Y^0\\)\n\nAggregate to the average causal effect\n\n\n1) Model\nThe code below uses Ordinary Least Squares to estimate an outcome model.\n\nmodel &lt;- lm(Y ~ A*(L1 + L2), data = data)\n\n\n\n\nCall:\nlm(formula = Y ~ A * (L1 + L2), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1448 -0.7105  0.0097  0.6998  3.1743 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.01606    0.05699   0.282  0.77827    \nA            1.11555    0.18021   6.190 1.26e-09 ***\nL1           1.06333    0.05938  17.907  &lt; 2e-16 ***\nL2           1.11199    0.05951  18.685  &lt; 2e-16 ***\nA:L1        -0.39475    0.14279  -2.765  0.00591 ** \nA:L2        -0.28935    0.13940  -2.076  0.03844 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.111 on 494 degrees of freedom\nMultiple R-squared:  0.6732,    Adjusted R-squared:  0.6699 \nF-statistic: 203.6 on 5 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nWe chose a model where treatment A is interacted with an additive function of confounders L1 + L2. This is also known as a t-learner (Kunzel et al. 2019) because it is equivalent to estimating two separate regression models of outcome on confounders, one among those for whom A == 1 and among those for whom A == 0.\n\n\n2) Predict\nThe code below predicts the conditional average potential outcome under treatment and control at the confounder values of each observation.\nFirst, we create data with A set to the value 1.\n\ndata_1 &lt;- data |&gt;\n  mutate(A = 1)\n\n\n\n# A tibble: 500 × 7\n        L1     L2      Y0    Y1 propensity_score     A       Y\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  0.00304  1.03   0.677   1.59          0.276       1  0.677 \n2 -2.35    -1.66  -4.09   -3.53          0.00244     1 -4.09  \n3  0.104   -0.912  0.0659  1.31          0.0569      1  0.0659\n# ℹ 497 more rows\n\n\nThen, we create data with A set to the value 0.\n\ndata_0 &lt;- data |&gt;\n  mutate(A = 0)\n\n\n\n# A tibble: 500 × 7\n        L1     L2      Y0    Y1 propensity_score     A       Y\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  0.00304  1.03   0.677   1.59          0.276       0  0.677 \n2 -2.35    -1.66  -4.09   -3.53          0.00244     0 -4.09  \n3  0.104   -0.912  0.0659  1.31          0.0569      0  0.0659\n# ℹ 497 more rows\n\n\nWe use our outcome model to predict the conditional mean of the potential outcome under each scenario.\n\npredicted &lt;- data |&gt;\n  mutate(\n    Y1_predicted = predict(model, newdata = data_1),\n    Y0_predicted = predict(model, newdata = data_0),\n    effect_predicted = Y1_predicted - Y0_predicted\n  )\n\n\n\n# A tibble: 500 × 10\n        L1     L2      Y0    Y1 propensity_score     A       Y Y1_predicted\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1  0.00304  1.03   0.677   1.59          0.276       0  0.677         1.98 \n2 -2.35    -1.66  -4.09   -3.53          0.00244     0 -4.09         -1.81 \n3  0.104   -0.912  0.0659  1.31          0.0569      0  0.0659        0.451\n# ℹ 497 more rows\n# ℹ 2 more variables: Y0_predicted &lt;dbl&gt;, effect_predicted &lt;dbl&gt;\n\n\n\n\n3) Aggregate\nThe final step is to aggregate to an average causal effect estimate.\n\naggregated &lt;- predicted |&gt;\n  summarize(average_effect_estimate = mean(effect_predicted))\n\n\n\n# A tibble: 1 × 1\n  average_effect_estimate\n                    &lt;dbl&gt;\n1                    1.13",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#logistic-regression-a-realistic-simulated-example",
    "href": "prediction_for_inference.html#logistic-regression-a-realistic-simulated-example",
    "title": "Prediction for causal and population inference",
    "section": "Logistic regression: A realistic simulated example",
    "text": "Logistic regression: A realistic simulated example\nThis section illustrates outcome modeling with a more realistic simulated dataset involving many confounders. We also use logistic regression as our outcome model.\n\nData-based simulation\nTo what extent does completing a four-year college degree by age 25 increase the probability of having a spouse or residential partner with a four-year college degree at age 35, among the population of U.S. residents who were ages 12–16 at the end of 1996?\nThis causal question draws on questions in sociology and demography about assortative mating: the tendency of people with high education, income, or status to form households together2. One reason to care about assortative mating is that it can contribute to inequality across households: if people with high earnings potential form households together, then income inequality across households will be greater than it would be if people formed households randomly.\nOur question is causal: to what extent is the probability of marrying a four-year college graduate higher if one were hypothetically to finish a four-year degree, versus if that same person were hypothetically to not finish a college degree? But in data that exist in the world, we see only one of these two potential outcomes. The people for whom we see the outcome under a college degree are systematically different from those for whom we see the outcome under no degree: college graduates come from families with higher incomes, higher wealth, and higher parental education, for example. All of these factors may directly shape the probability of marrying a college graduate even in the absence of college. Thus, it will be important to adjust for a set of measured confounders, represented by \\(\\vec{X}\\) in our DAG.\n\n\n\n\n\n\n\n\n\nBy adjusting for the variables \\(\\vec{X}\\), we block all non-causal paths between the treatment \\(A\\) and the outcome \\(Y\\) in the DAG. If this DAG is correct, then conditional exchangeability holds with this adjustment set: \\(\\{Y^1,Y^0\\}\\unicode{x2AEB} A \\mid\\vec{X}\\).\nTo estimate, we use data from the National Longitudinal Survey of Youth 1997, a probability sample of U.S. resident children who were ages 12–16 on Dec 31, 1996. The study followed these children and interviewed them every year through 2011 and then every other year after that.\nWe will analyze a simulated version of these data (nlsy97_simulated.csv), which you can access with this line of code.\n\nall_cases &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/nlsy97_simulated.csv\")\n\n\n\n\n\n\n\nExpand to learn how to get the actual data\n\n\n\n\n\nTo access the actual data, you would need to register for an account, log in, upload the nlsy97.NLSY97 tagset that identifies our variables, and then download. Unzip the folder and put the contents in a directory on your computer. Then run our code file prepare_nlsy97.R in that folder. This will produce a new file d.RDS, contains the data. You could analyze that file. In the interest of transparency, we wrote the code nlsy97_simulated.R to convert these real data to simulated data that we can share.\n\n\n\nThe data contain several variables\n\nid is an individual identifier for each person\na is the treatment, containing the respondent’s education coded treated if the respondent completed a four-year college degree and untreated if not.\ny is the outcome: TRUE if has a spouse or residential partner at age 35 who holds a college degree, and FALSE if no spouse or partner or if the spouse or partner at age 35 does not have a degree.\nThere are several pre-treatment variables\n\nsex is coded Female and Male\nrace is race/ethnicity and is coded Hispanic, Non-Hispanic Black, and Non-Hispanic Non-Black.\nmom_educ is the respondent’s mother’s education as reported in 1997. It takes the value No mom if the child had no residential mother in 1997, and otherwise is coded with her education: &lt; HS, High school, Some college, or College.\ndad_educ is the respondent’s father’s education as reported in 1997. It takes the value No dad if the child had no residential father in 1997, and otherwise is coded with his education: &lt; HS, High school, Some college, or College.\nlog_parent_income is the log of gross household income in 1997\nlog_parent_wealth is the log of household net worth in 1997\ntest_percentile is the respondent’s percentile score on a test of math and verbal skills administered in 1999 (the Armed Services Vocational Aptitude Battery).\n\n\nWhen values are missing, we have replcaed them with predicted values. In the simulated data, no row represents a real person because values have been drawn randomly from a probability distribution designed to mimic what exists in the real data. As discussed above, we did this in order to share the file with you by a download on this website.\n\n\n1) Model\nOne can estimate outcome models using all the data (as in the example above) or separately by treatment status. In this example, we use the latter option.\n\nuntreated_cases &lt;- all_cases |&gt; filter(a == \"untreated\")\ntreated_cases &lt;- all_cases |&gt; filter(a == \"treated\")\n\nWe use the untreated cases to estimate a model for \\(Y^0\\) as a function of \\(X\\). If our data include sampling weights, then we weight this model by the sampling weights.\n\nlogistic_model_for_y0 &lt;- glm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  family = binomial,\n  data = untreated_cases,\n  weights = sampling_weight\n)\n\nLikewise, we estimate a model for \\(Y^1\\) among treated units.\n\nlogistic_model_for_y1 &lt;- glm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  family = binomial,\n  data = treated_cases,\n  weights = sampling_weight\n)\n\nThese models return a warning that there is a non-integer number of successes. This is normal and not a concern when estimating logistic regression models with weights.\n\n\n2) Predict\nWe then predict the probability of \\(Y\\) under each treatment condition, using type = \"response\" to predict the probability of \\(Y\\) instead of the log odds of \\(Y\\).\n\nlogistic_predicted_potential_outcomes &lt;- all_cases |&gt;\n  mutate(\n    yhat1 = predict(\n      logistic_model_for_y1, \n      newdata = all_cases, \n      type = \"response\"\n    ),\n    yhat0 = predict(\n      logistic_model_for_y0, \n      newdata = all_cases, \n      type = \"response\"\n    ),\n    effect = yhat1 - yhat0\n  )\n\n\n\n# A tibble: 7,688 × 6\n     id sampling_weight a         yhat1  yhat0 effect\n  &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1           0.989 untreated 0.254 0.0861  0.168\n2     2           0.999 treated   0.726 0.562   0.164\n3     3           0.967 untreated 0.177 0.0261  0.151\n# ℹ 7,685 more rows\n\n\n\n\n3) Aggregate\nWe can then aggregate the predicted potential outcomes to estimate the average treatment effect over all cases (ATT),\n\nlogistic_ate_estimate &lt;- logistic_predicted_potential_outcomes |&gt;\n  summarize(ate = weighted.mean(effect, w = sampling_weight)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n    ate\n  &lt;dbl&gt;\n1 0.204\n\n\nor among those who were factually treated or untreated,\n\nlogistic_predicted_potential_outcomes |&gt;\n  group_by(a) |&gt;\n  summarize(conditional_average_effect = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 2 × 2\n  a         conditional_average_effect\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 treated                        0.240\n2 untreated                      0.195\n\n\nor among any subpopulation by grouping by any confounding variables.\nWe estimate that completing college increases the probability of having a college-educated by 0.204. This causal conclusion relies both on our causal assumptions (the DAG) and our statistical assumptions (the chosen model).",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#machine-learning-outcome-models-for-causal-inference",
    "href": "prediction_for_inference.html#machine-learning-outcome-models-for-causal-inference",
    "title": "Prediction for causal and population inference",
    "section": "Machine learning outcome models for causal inference",
    "text": "Machine learning outcome models for causal inference\nOutcome models for causal inference just need to be input-output machines:\n\ninput \\(\\vec{X}\\) and a treatment value \\(a\\)\noutput \\(\\hat{Y}^a = \\text{E}(Y\\mid A = a, \\vec{X})\\)\n\nMachine learning estimators can be used as the algorithm to make the predictions.\n\nA promising story\nMany researchers increasingly turn to machine learning estimators in the service of causal inference. Some of the early advances involved direct plug-ins, where a machine learning function\n\\[\\hat{f}:\\{A,\\vec{X}\\} \\rightarrow \\hat{Y}\\]\nis learned to map values of treatment \\(A\\) and confounders \\(\\vec{X}\\) to a predicted outcome \\(\\hat{Y}\\). Viewed this way, one can use any machine learning approach as an estimator of an average causal effect.\n\\[\n\\hat{\\text{E}}(Y^1 - Y^0) = \\frac{1}{n}\\sum_i \\left(\\hat{f}(1,\\vec{x}_i) - \\hat{f}(0,\\vec{x}_i)\\right)\n\\]\nA widely-cited early application was Hill (2011), which used Bayesian Additive Regression Trees (BART) to model the response surface and then predict to estimate average causal effects and many conditional average effects. By outsourcing the functional form to an algorithm, approaches like this free the researcher to focus on the causal question and the DAG rather than the assumed functional form of statistical relationships. These algorithmic approaches often performed well in competitions where statisticians applied a series of estimators to simulated data to see who would come closest to the true causal effects (known in simulation, see Dorie et al. 2019). Recently, new developments have expanded tree and forest estimators to explicitly address causal questions (e.g., Athey & Imbens 2016).\n\n\nA warning example\nThere are many reasons to be optimistic, but one also must be cautious: it is also possible for a machine learning model that predict \\(Y\\) well to be a poorly performing estimator of a causal effect.\n\nthere may be unmeasured confounding\nthe regularization in machine learning models can induce a large bias\nto predict \\(Y^1\\), the model is trained on treated units. But untreated units may have a very different distribution of \\(\\vec{X}\\)\n\nAs an example, using our simulated data from above, suppose a researcher models \\(Y\\mid A, \\vec{X}\\) using a regression tree, which they plan to use to predict \\(Y^1\\) and \\(Y^0\\) for all cases.\n\nlibrary(rpart)\nset.seed(90095)\nfit &lt;- rpart(\n  y ~ a + sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile,\n  data = all_cases\n)\nrpart.plot::rpart.plot(fit)\n\n\n\n\n\n\n\n\nThere are some concerns with this tree:\n\nMost variables were never used in splitting\nIt split on test_percentile, but only coarsely: each leaf contains a wide range of test_percentile values\n\nAs a result, each leaf contains a wide range of values on the confounding variables \\(\\vec{X}\\). If you imagine predicting \\(Y^0\\) and \\(Y^1\\) for a new unit with \\(\\vec{X} = \\vec{x}\\), the untreated cases and the treated cases that are averaged into these predictions will have \\(\\vec{X}\\) values that are widely ranging, not equal to \\(\\vec{X}\\) and not equally distributed between the treated and the untreated.\n\n\nCode\nall_cases |&gt;\n  filter(test_percentile &lt; 69) |&gt;\n  ggplot(aes(x = test_percentile, fill = a)) +\n  geom_density(alpha = .4) +\n  theme_minimal() +\n  ylab(\"Density\") +\n  xlab(\"Test Percentile\") +\n  scale_fill_discrete(\n    name = \"Treatment\",\n    labels = c(\"Treated:\\nCompleted college\",\"Untreated:\\nDid not complete college\")\n  ) +\n  theme(legend.key.height = unit(.4,\"in\")) +\n  ggtitle(\n    \"Coarse leaves do not fully adjust for confounders\",\n    subtitle = \"Treated and untreated units used to predict for a new unit with\\ntest percentile below 69 have different distributions of that confounder.\"\n  )\n\n\n\n\n\n\n\n\n\nThere exist work-around solutions for this problem, such as methods to carry out further confounder adjustment within leaves to better estimate causal effects (Brand et al. 2021). These problems and solutions from causal inference point to a general fact: off-the-shelf algorithms to predict \\(Y\\) typically involve data-driven choices (e.g., prune the tree) that lead to better predictions of \\(Y\\), but in some cases those same choices can lead to worse estimates of causal effects (\\(Y^1 - Y^0\\)).\nIn the coming weeks, we will learn how to make better use of machine learning algorithms for causal inference by combining treatment models and outcome models.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "prediction_for_inference.html#footnotes",
    "href": "prediction_for_inference.html#footnotes",
    "title": "Prediction for causal and population inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHeroic assumptions are assumptions that do a lot of work to make our quantities of interest identified. When I use “heroic”, I often mean an assumption that is not very credible.↩︎\nFor reviews, see Mare 1991 and Schwartz 2013.↩︎",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "problem_sets.html",
    "href": "problem_sets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "This page will contain a problem set corresponding to each class meeting. The problem sets are very open-ended and are designed to connect the material from class to your ongoing project.\nFor every problem set, submit a PDF. If your code is not embedded in your PDF, then also submit a code file.\nWe chose together the following weekly pattern for problem sets:\nWe will be using identified peer reviews. The reason these are not anonymous is that we are a small class, and we will get to know one another’s projects. Your peer reviewer will not be grading you or assigning point values, but they will be commenting on your work. A good peer review is a short paragraph that comments on promising aspects of your peer’s work as well as offering suggestions for improvement or future directions."
  },
  {
    "objectID": "problem_sets.html#problem-set-1",
    "href": "problem_sets.html#problem-set-1",
    "title": "Problem Sets",
    "section": "Problem Set 1",
    "text": "Problem Set 1\n\n1. Your ongoing paper.\n1.1. (15 points) Write an abstract of your research paper.\n1.2. (5 points) What is one unit-specific quantity in your paper?\n1.3. (5 points) What is one target population in your paper?\n\n\n2. Regression for a conditional mean\n(25 points)\nIn class, we used OLS regression to predict the mean outcome in a subgroup. Do the same thing in a dataset of your choosing, which might be the dataset from your project. Define the outcome variable and the population subgroup you are describing."
  },
  {
    "objectID": "problem_sets.html#problem-set-2",
    "href": "problem_sets.html#problem-set-2",
    "title": "Problem Sets",
    "section": "Problem Set 2",
    "text": "Problem Set 2\n\n(20 points) In your project, do you have something analogous to many groups with a few observations per group? This could more generally be any case where the number of parameters to be estimated is somewhat large for the sample size available. If not in your project, think of an example in your general research area. Write a few sentences about why one might want penalized regression in that setting.\n\nThe remainder of this problem set is based in the idea of multilevel models, though you can alternatively estimate by LASSO or ridge regression if you prefer. You can complete parts (2) and (3) in your own dataset if you can define at least 10 groups with at least 20 units per group. Wherever we say “team”, substitute whatever group is in your dataset. You may also use the baseball_population.csv data, in which the group variable is team.\n\n(15 points) Draw a sample of 5 units per group. Estimate an unpenalized OLS regression and a penalized regression (multilevel, ridge, or LASSO). Produce a graph that compares the predicted values.\n(15 points) Repeat the exercise above but with a sample of 20 units per group. Visualize the estimates. How do your results change?\n\nNote that on (3) it is likely that the estimates will look different from (2), but it is also possible that they will be similar depending on your setting. For example, if your algorithm sets a very large penalty in your setting then all the group-specific deviations could be shrunk to zero in both (2) and (3). There are many right answers."
  },
  {
    "objectID": "problem_sets.html#problem-set-3",
    "href": "problem_sets.html#problem-set-3",
    "title": "Problem Sets",
    "section": "Problem Set 3",
    "text": "Problem Set 3\nUsing the simulation data from class or data from your own project:\n\n(15 points) Estimate a regression tree and visualize it. If you are using R, there are packages to visualize the tree. If you are using Stata, there may be packages or you are welcome to use the summary text output.\n(10 points) Write 3-4 sentences for someone who has never heard of this method. Pick some predictor vector value and explain how the tree produces a prediction for that predictor vector.\n(15 points) Estimate a random forest. Visualize the predicted values as a function of the predictors. If your data have only two predictors, this could be as in class with one on the \\(x\\)-axis and one for colors. If you have many predictors, you may need to find a creative way to visualize your predictions. You may choose to make predictions while holding one or more variables at a single value.\n(10 points) In a few sentences, give an example of a problem in your area of research (possibly your paper) where the functional form linking \\(\\vec{X}\\) to \\(E(Y\\mid\\vec{X})\\) is unknown, such that a random forest might be a way to learn the functional form from the data."
  },
  {
    "objectID": "problem_sets.html#problem-set-4",
    "href": "problem_sets.html#problem-set-4",
    "title": "Problem Sets",
    "section": "Problem Set 4",
    "text": "Problem Set 4\nWrite the abstract of your research paper. If you do not have results yet, pretend that your results are really amazing! You can make up any estimates that you want. As an added challenge, try writing the abstract twice with two sets of possible results. It would be ideal if all possible results are interesting.\nAs you write, minimize jargon so that you are writing for a New York Times reader. Emphasize big claims and don’t bury them in statistical terminology. The goal of this exercise is to help us ask and frame a high-impact question that will speak to a broad audience. Possibly, the exercise will lead you to change your question.\nYour peer reviewer should comment on things that may include:\n\nwhat about this abstract is especially compelling?\nwhat parts (if any) about this abstract are directed to a small subset of academia and perhaps should be broadened? This may be parts with jargon.\nwhat claims could be emphasized more?\n\nThe hope is that by writing an abstract (or abstracts) with findings that you could produce, we might come up with better ways of asking and presenting our questions."
  },
  {
    "objectID": "problem_sets.html#problem-set-5",
    "href": "problem_sets.html#problem-set-5",
    "title": "Problem Sets",
    "section": "Problem Set 5",
    "text": "Problem Set 5\nThis problem set is unlike the preceding ones in two ways. It is entirely conceptual, with no data analysis, and it is entirely about hypothetical examples rather than your paper. It is a chance to reinforce a few basic ideas from our class on panel data. Questions 1–5 are on difference in difference. Questions 6–7 are on regression discontinuity.\nIn the figures below, the treated group becomes treated between time 1 and time 2. The control group never becomes treated. Figures are hypothetical scenarios that depict true potential outcomes even if those outcomes would not be observed in an actual study.\n\n\n\n\n\n\n\n\n\n1 (10 points). In which setting does the parallel trends assumption hold: A, B, neither, or both?\n2 (10 points). In actual data analysis, can we ever know for certain whether we are in Setting A or Setting B? If the answer is no, then tell us which outcome cannot be observed.\n3 (10 points) A researcher comes to you with the data below, which depict only observed outcomes. That researcher wants to run a difference in difference analysis. Here, we have not depicted the counterfactual outcome because the researcher would not know it. Why is the parallel trends assumption doubtful in this setting?\n\n\n\n\n\n\n\n\n\n4 (10 points). A researcher is interested in the causal effect of a minimum wage increase on employment. They plan to analyze data on the U.S. only, and they are interested in a time when the minimum wage rose simultaneously at every place in the U.S. Why won’t a difference in difference design work for the researcher’s question?\n5 (10 points). Propose another design that the researcher could use to answer the question in (1.4), which may involve data outside the U.S. or may involve a different analysis of data within the U.S. Answer this question in no more than 3 sentences. Many answers are possible."
  },
  {
    "objectID": "problem_sets.html#problem-set-6",
    "href": "problem_sets.html#problem-set-6",
    "title": "Problem Sets",
    "section": "Problem Set 6",
    "text": "Problem Set 6\nConstruct a bootstrap confidence interval using data from your project. Some things to tell us in writing include:\n\nwhat is your population parameter?\nwhat is your estimator?\nhow many bootstrap samples did you draw?\ndid you use the Normal approximation or the percentile method?\n\nIf your data are not suitable for a bootstrap confidence interval for some reason (e.g., you have the entire population so that confidence intervals don’t make sense), go ahead and carry out the steps for practice and then tell us why the bootstrap might not be appropriate in your setting."
  },
  {
    "objectID": "problem_sets.html#problem-set-7",
    "href": "problem_sets.html#problem-set-7",
    "title": "Problem Sets",
    "section": "Problem Set 7",
    "text": "Problem Set 7\nFor (1) to (5),\n\nList all paths connecting \\(A\\) and \\(Y\\). Remember that a path can be written in one line as a sequence of nodes connected by edges.\nFor each path, answer\n\nis this path open before we condition on anything?\nis this path open conditional on \\(X\\)?\n\nAre any non-causal paths between \\(A\\) and \\(Y\\) open once we condition on \\(X\\)? In other words, other than causal paths from \\(A\\) to \\(Y\\) is there any other reason \\(A\\) and \\(Y\\) would be associated conditional on \\(X\\)?"
  },
  {
    "objectID": "problem_sets.html#problem-set-8",
    "href": "problem_sets.html#problem-set-8",
    "title": "Problem Sets",
    "section": "Problem Set 8",
    "text": "Problem Set 8\n1 (25 points). A DAG in your research area.\nIn your research project or field of research, define a causal question. Define the treatment variable and potential outcomes. Draw a DAG and identify a sufficient adjustment set given your DAG.\nCoding help: How to draw a DAG.\nFor this class, you can draw your DAG by hand and take a picture of it, and include the picture in your writeup. Alternatively, you can learn to type a DAG as a tikzpicture in LaTeX, as I do. Both are equally good!\nTo produce a DAG with code, I include the following in my LaTeX document header:\n\n\\usepackage{tikz}\n\\usetikzlibrary{arrows,shapes.arrows,positioning,shapes,patterns,calc}\n\nThen I use code like the below to draw a DAG.\n\n\\begin{tikzpicture}[x = .3in, y = .3in]\n    \\node (x) at (0,-1) {$X$};\n    \\node (a) at (0,0) {$A$};\n    \\node (y) at (1,0) {$Y$};\n    \\draw[-&gt;, thick] (x) -- (a);\n    \\draw[-&gt;, thick] (a) -- (y);\n    \\draw[-&gt;, thick] (x) -- (y);\n\\end{tikzpicture}\n\n2 (25 points). Outcome model for causal inference.\nDefine a causal estimand and estimate it using an outcome model. You are welcome to use any causal estimand you are interested in, in any dataset you like (potentially from your project). The outcome model can be statistical (e.g., OLS) or a more data-science model (e.g., random forest).\nIf you’d rather use data we provide, feel free to use the data from my analogous undergrad problem set. This is a simulated version of data from an ongoing project with Soonhong."
  },
  {
    "objectID": "yhat_regression.html",
    "href": "yhat_regression.html",
    "title": "Regression for Y-hat",
    "section": "",
    "text": "slides\nThis class is about regression as a tool to approximate a conditional expectation function. From this perspective, the \\(\\hat\\beta\\) estimates are only a step toward the broader purpose of regression to produce \\(\\hat{Y}\\) values that achieve this approximation well. This perspective will ultimately allow us to consider machine learning estimators beyond regression.\nSome of this class relies on an ongoing project on description: ilundberg.github.io/description",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Regression for Y-hat"
    ]
  },
  {
    "objectID": "weighting_for_inference.html",
    "href": "weighting_for_inference.html",
    "title": "Weighting for causal and population inference",
    "section": "",
    "text": "This session will be about weighting to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators. Slides are available here and may be updated more before class.\n\nNote. This week does not yet have website materials. I haven’t yet finished updating materials for the website, but you are welcome to see my undergrad course notes which share many similarities to the grad course notes.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by weighting"
    ]
  },
  {
    "objectID": "panel_data.html",
    "href": "panel_data.html",
    "title": "Panel data",
    "section": "",
    "text": "Here are slides for today.\n\nThis session will cover a series of estimators that apply when\n\nmany units are observed over many time periods each\none or more units become treated at some point in the observation window\n\nThese estimators include difference in difference, interrupted time series, regression discontinuity, and synthetic control. In order to cover many estimators, today’s class will focus more on conceptual ideas than on coding. Thus, material is currently available in slide form rather than in written note form.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Panel data"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "Mediation",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Mediation"
    ]
  },
  {
    "objectID": "asking_a_research_question.html",
    "href": "asking_a_research_question.html",
    "title": "Asking a research question",
    "section": "",
    "text": "slides\nThis class is about how to ask a quantitative research question. The focus will be on summarizing outcomes over well-defined populations, for which a regression coefficient \\(\\hat\\beta\\) may or may not be a meaningful summary. We will focus on summarizing subgroups by the average value of \\(Y\\), or in small samples by summarizing subgroup means using predicted values (\\(\\hat{Y}\\)) from regression models.",
    "crumbs": [
      "Problem Sets",
      "Asking a Research Question"
    ]
  },
  {
    "objectID": "doubly_robust.html",
    "href": "doubly_robust.html",
    "title": "Doubly-robust estimation",
    "section": "",
    "text": "This session will combine prediction and weighting methods for an approach with properties superior to either on its own.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Doubly-robust estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to UCLA SOCIOL 212B (Winter 2025). See the syllabus.\nW 9–11:50am. Powell 320B.\nThis course is about answering social science questions using quantitative data. We will especially focus on how computational power is transforming the ways we can carry out quantitative research, covering both statistical and machine learning tools from the perspective of social science applications. The course especially emphasizes how to translate social science theories into quantities that can be estimated by algorithms designed for prediction. We will consider prediction in the service of both description and causal inference, building on ideas from SOCIOL 212A. The end product of the course is an extended abstract containing data analysis using the ideas from the course. For students continuing to 212C, the abstract can serve as the basis for the research project in that course. Students will leave the course prepared to connect social science theories to empirical evidence that can be produced by algorithms designed for prediction."
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Welcome!",
    "section": "Learning goals",
    "text": "Learning goals\nStudents will learn to\n\ndefine a precise quantitative research question\nconnect that question to predictions that can be made by statistical or machine learning algorithms\nmake a principled argument for the choice of a particular learning approach"
  },
  {
    "objectID": "index.html#schedule-of-topics-tentative",
    "href": "index.html#schedule-of-topics-tentative",
    "title": "Welcome!",
    "section": "Schedule of topics (tentative)",
    "text": "Schedule of topics (tentative)\nPart 1. Descriptive data science with probability samples.\n\nJan 8. Asking research questions without \\(\\hat\\beta\\) and regression for \\(\\hat{Y}\\)\nJan 15 and 22. Algorithms for prediction\nJan 29. Data-driven selection of an estimator\nFeb 5. Panel data (actually a Part 2 topic, presented out of order to align with afternoon CCPR workshop)\nFeb 12. Statistical uncertainty by resampling\n\nPart 2. Non-probability samples and observational causal inference\n\nFeb 19. Nonparametric identification\nFeb 26. Estimation by prediction\nMar 5. Estimation by weighting\nMar 12. Doubly-robust estimation"
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "Welcome!",
    "section": "Who should take this course?",
    "text": "Who should take this course?\nThe course is designed to support the development of quantitative social science research projects. The course is a good fit for PhD students in sociology, statistics, political science, economics, and other social sciences. PhD students from disciplines other than sociology should request a code from the instructor to enroll."
  },
  {
    "objectID": "index.html#prerequisite",
    "href": "index.html#prerequisite",
    "title": "Welcome!",
    "section": "Prerequisite",
    "text": "Prerequisite\nFamiliarity with basic probability and statistics (e.g., random variables, expectation, confidence intervals). Soc 212A is formally a prerequisite, but students who did not take Soc 212A are welcome to talk with me about whether Soc 212B would be a good fit for them."
  },
  {
    "objectID": "index.html#instructional-format",
    "href": "index.html#instructional-format",
    "title": "Welcome!",
    "section": "Instructional format",
    "text": "Instructional format\nLecture with in-class exercises. Bring computers to class."
  },
  {
    "objectID": "index.html#course-readings",
    "href": "index.html#course-readings",
    "title": "Welcome!",
    "section": "Course readings",
    "text": "Course readings\nReadings will be available online for free. See the course website for an updated schedule of readings and topics.\nMany readings from books with free PDFs available online:\n\nEfron, B., & T Hastie. 2016. Computer Age Statistical Inference: Algorithms, Evidence and Data Science. Cambridge: Cambridge University Press.\nHastie, T., R. Tibshirani, & J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer.\nHernán, M.A., & J.M. Robins. 2024. Causal Inference: What If? Boca Raton: Chapman & Hall / CRC."
  },
  {
    "objectID": "index.html#statistical-software",
    "href": "index.html#statistical-software",
    "title": "Welcome!",
    "section": "Statistical software",
    "text": "Statistical software\nYou can use any statistical software you prefer. I use R and will best be able to support you in R. In addition to R, we will attempt to provide Stata support where possible. Not all algorithms are available in Stata. If you are fluent in another software, you are welcome to use that. The focus of this course is on conceptual ideas, not a programming language."
  },
  {
    "objectID": "missing_data.html",
    "href": "missing_data.html",
    "title": "Missing data",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Missing data"
    ]
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "Scale construction",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Scale construction"
    ]
  },
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team!\n\n\n\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy! I am excited about causal inference and finding new research questions and ways to answer them. Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nSoonhong Cho\ntnsehdtm@gmail.com\n(he / him)\nSoonhong is a PhD candidate in political science at UCLA."
  },
  {
    "objectID": "nonparametric_identification.html",
    "href": "nonparametric_identification.html",
    "title": "Nonparametric Identification",
    "section": "",
    "text": "Here are slides and a PDF of this page.\nThe course so far has focused on descriptive claims. For example, we have estimated the average value of an outcome in the population, or among population subgroups. This lecture pivots to causal claims: what would happen if a population were exposed to a hypothetical intervention.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#fundamental-problem-of-causal-inference",
    "href": "nonparametric_identification.html#fundamental-problem-of-causal-inference",
    "title": "Nonparametric Identification",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nHealth professionals often advise people to eat a Mediterranean diet high in healthy fats such as olive oil, whole grains, and fruits. There is descriptive evidence that lifespans are longer among people who eat a Mediterranean diet compared with among people who eat a standard diet. But does eating a Mediterranean diet cause longer lifespan? The figure below visualizes this question in the potential outcomes framework.\n\nIn this hypothetical example, each row corresponds to a person. Person 1 follows a Mediterranean diet and is observed to have a lifespan indicated in blue. Person 2 does not follow a Mediterranean diet and is observed to have a lifespan indicated in green. The descriptive evidence is that lifespans are longer among those eating a Mediterranean diet (blue outcomes on the left) compared with those eating standard diets (green outcomes on the left).\nThe right side of the figure corresponds to the causal claim, which is different. Person 1 has two potential outcomes: a lifespan that would be realized under a Mediterranean diet and a lifespan that would be realized under a standard diet. The causal effect for Person 1 is the difference between the lifespans that would be realized for that person under each of the two diets. But there is a fundamental problem: person 1 ate a Mediterranean diet, and we did not get to observe their outcome under a standard diet. The fundamental problem of causal inference (Holland 1986) is that causal claims involve a contrast between potential outcomes, but for each unit only one of these potential outcomes is realized. The other is counterfactual and cannot be directly observed.\nWe will need additional argument and assumptions to use the factual data (left side of the figure) in order to produce answers about causal effects (right side of the figure). Causal inference is a missing data problem insofar as many of the potential outcomes we need are missing.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#mathematical-notation",
    "href": "nonparametric_identification.html#mathematical-notation",
    "title": "Nonparametric Identification",
    "section": "Mathematical notation",
    "text": "Mathematical notation\nBecause each person has more than one potential outcome, we need new mathematical notation to formalize causal claims. We will use subscripts to indicate units (rows of our data). Let \\(Y_i\\) be the outcome for person \\(i\\), such as whether person \\(i\\) survived. Let \\(A_i\\) be the treatment of person \\(i\\), for example taking the value or the value . To refer more abstractly to a value the treatment could take, we use the lower case notation \\(a\\) for a treatment value. Define potential outcomes \\(Y_i^\\text{MediterraneanDiet}\\) and \\(Y_i^\\text{StandardDiet}\\) as the lifespan outcomes that person \\(i\\) would realize under each of the treatment conditions. More generally, let \\(Y_i^a\\) denote the potential outcome for unit \\(i\\) that would be realized if assigned to treatment value \\(a\\).\nThe causal effect is a contrast across potential outcomes. For example, the causal effect on Ian’s lifespan of eating a Mediterranean diet versus a standard diet is \\[Y_\\text{Ian}^\\text{MediterraneanDiet} - Y_\\text{Ian}^\\text{StandardDiet}\\]\nTo connect causal claims to ideas we have already covered from sampling, we will adopt a framework in which potential outcomes are fixed quantities with randomness arising from sampling and/or from random treatment assignment. Each person has a fixed outcome \\(Y_i^\\text{MediterraneanDiet}\\) that would be observed if they were sampled and assigned a Mediterranean diet. This is just like how every baseball player from last week had a salary that would be observed if they were sampled. We will sometimes omit the \\(i\\) subscript to refer to the random variable for the potential outcome of a randomly-sampled person from the population, \\(Y^\\text{MediterraneanDiet}\\).",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#the-consistency-assumption",
    "href": "nonparametric_identification.html#the-consistency-assumption",
    "title": "Nonparametric Identification",
    "section": "The consistency assumption",
    "text": "The consistency assumption\nWe want to make causal claims about potential outcomes \\(Y_i^a\\), but what we observe are factually realized outcome \\(Y_i\\). To draw the connection, we need to assume that the factual outcomes are consistent with what would be observed if the person in question were assigned to the treatment condition that factually observed. Using \\(A_i\\) to denote the factual treatment for person \\(i\\), we assume\n\\[\nY_i^{A_i} = Y_i \\qquad\\text{(consistency assumption)}\n\\] This assumption is often obviously true, but we will see later examples where it is violated. One example is when person \\(i\\)’s outcome depends not only on their own treatment but also on the treatment of some neighboring person \\(j\\). In many of our initial examples, we will simply assume the consistency assumption holds.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#potential-outcomes-in-math-and-in-words",
    "href": "nonparametric_identification.html#potential-outcomes-in-math-and-in-words",
    "title": "Nonparametric Identification",
    "section": "Potential outcomes in math and in words",
    "text": "Potential outcomes in math and in words\nWe will often use potential outcomes within mathematical statements. For example, we might write about the expected outcome if assigned to a Mediterranean diet, \\(E(Y^\\text{MediterraneanDiet})\\). Recall that the expectation operator \\(E()\\) says to take the population mean of the random variable within the parentheses. We will also use conditional expectations, such as \\(E(Y\\mid A = \\text{MediterraneanDiet})\\) which reads “the expected value of \\(Y\\) given that \\(A\\) took the value .” The vertical bar says that we are taking the expected value of the variable on the left of the bar within the subgroup defined on the right side of the bar.\nIn class, we practiced writing statements in math and in English. For example, the mathematical statement \\[\nE(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{TRUE}) &gt; E(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{FALSE})\n\\] is a descriptive statement that says the expected value of earnings is higher among the subgroup with college degrees than among those without college degrees. We made these kinds of descriptive claims already in code by using group_by() and summarize().\nThe mathematical statement \\[\nE\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{TRUE}}\\right) &gt; E\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{FALSE}}\\right)\n\\] is a causal claim that the expected value of earnings that a random person would realize if assigned to a college degree is higher than the expected value if a random person were assigned to no college degree. Because there is no vertical bar (\\(\\mid\\)), the causal claim is an average over the entire population on both sides of the inequality. The descriptive claim, by contrast, is an average over two different sets of units. The figure below visualizes the difference between these descriptive and causal claims using a visual analogous to the one used to introduce a Mediterranean diet.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#exchangeability-in-simple-random-samples",
    "href": "nonparametric_identification.html#exchangeability-in-simple-random-samples",
    "title": "Nonparametric Identification",
    "section": "Exchangeability in simple random samples",
    "text": "Exchangeability in simple random samples\nCausal effects involve both factual and counterfactual outcomes, yet data that we can observe involve only factual outcomes. To learn about causal effects from data that can be observed requires assumptions about the data that are not observed. One way to learn about that is by making an assumption known as exchangeability.\nThe figure below illustrates a population of 6 people. Each person has an outcome \\(Y_i\\), which for example might be that person’s employment at age 40. A researcher draws a random sample without replacement with equal sampling probabilities and records the sampled outcomes. The researcher uses the average of the sampled outcomes as an estimator for the population mean.\n\nWhy do probability samples like this work? They work because selection into the sample (\\(S = 1\\)) is completely randomized and thus independent of the outcome \\(Y\\). In other words, the people who are sampled (\\(S = 1\\)) and the people who are unsampled (\\(S = 0\\)) have the same distribution of outcomes (at least in expectation over samples). We might say that the sampled and the unsampled units are exchangeable in the sense that they follow the same distribution in terms of \\(Y\\). In math, exchangeable sampling can be written as follows.\n\\[\n\\underbrace{Y}_\\text{Outcome}\\quad \\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Is}\\\\\\text{Independent}\\\\\\text{of}}} \\quad \\underbrace{S}_{\\substack{\\text{Sample}\\\\\\text{Inclusion}}}\n\\]\nExchangeability holds in simple random samples because sampling is completely independent of all outcomes by design. In other types of sampling, such as convenience samples that enroll anyone who is interested, exchangeability may hold but is far from guaranteed. Perhaps people who are employed are more likely to answer a survey about employment, so that the employment rate in a convenience sample might far exceed the population mean employment rate. Exchangeability is one condition under which reliable population estimates can be made from samples, and probability samples are good because they make exchangeability hold by design.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#exchangeability-in-randomized-experiments",
    "href": "nonparametric_identification.html#exchangeability-in-randomized-experiments",
    "title": "Nonparametric Identification",
    "section": "Exchangeability in randomized experiments",
    "text": "Exchangeability in randomized experiments\nThe figure below illustrates our population if they all enrolled in a hypothetical randomized experiment. In this experiment, we imagine that each unit is either randomized to attain a four-year college degree (\\(A = 1)\\) or to finish education with a high school diploma (\\(A = 0\\)).\n\nIn this randomization, Maria, Sarah, and Jes'us were randomized to attain a four-year college degree. We observe their outcomes under this treatment condition (\\(Y^1\\)). Because treatment was randomized with equal probabilities, these three units form a simple random sample from the full population of 6 people. We could use the sample mean of \\(Y^1\\) among the treated units (Maria, Sarah, Jes'us) as an estimator of the population mean of \\(Y^1\\) among all 6 units.\nWilliam, Rich, and Alondra were randomized to finish their education with a high school diploma. We see their outcomes under this control condition \\(Y^0\\). Their treatment assignment (\\(A = 0\\)) is analogous to being sampled from the population of \\(Y^0\\) values. We can use their sample mean outcome as an estimator of the population mean of \\(Y^0\\).\nFormally, we can write the exchangeability assumption for treatment assignments as requiring that the set of potential outcomes are independent of treatment assignment.\n\\[\n\\underbrace{\\{Y^1,Y^0\\}}_{\\substack{\\text{Potential}\\\\\\text{Outcomes}}}\\quad\\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Are}\\\\\\text{Independent}\\\\\\text{of}}}\\quad  \\underbrace{A}_\\text{Treatment}\n\\] Exchangeability holds in randomized experiments because treatment is completely independent of all potential outcomes by design. In observational studies, where treatment values are observed but are not assigned randomly by the researcher, exchangeability may hold but is far from guaranteed. In the coming classes, we will talk about generalizations of the exchangeability assumption that one can argue might hold in some observational settings.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#causal-identification",
    "href": "nonparametric_identification.html#causal-identification",
    "title": "Nonparametric Identification",
    "section": "Causal identification",
    "text": "Causal identification\nA population-average causal effect could take many possible values. Using data alone, it is impossible to identify which of these many possible values is the correct one. By pairing data together with causal assumptions, however, one can identify the average causal effect by equating it with a statistical quantity that only involves observable random variables.\n\nCausal identification. A mathematical proof linking a causal estimand (involving potential outcomes) to a statistical quantity involving only factual random variables.\n\nIn a randomized experiment, the average causal effect is identified by the assumptions of consistency and exchangeability. A short proof can yield insight about the goals and how these assumptions are used.\n\\[\n\\begin{aligned}\n&\\overbrace{\\text{E}\\left(Y^1\\right) - \\text{E}\\left(Y^0\\right)}^{\\substack{\\text{Average}\\\\\\text{causal effect}\\\\\\text{(among everyone)}}} \\\\\n&= \\text{E}\\left(Y^1\\mid A = 1\\right) - \\text{E}\\left(Y^0\\mid A = 0\\right) &\\text{by exchangeability}\\\\\n&= \\underbrace{\\text{E}\\left(Y\\mid A = 1\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the treated}}} - \\underbrace{\\text{E}\\left(Y\\mid A = 0\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the untreated}}} &\\text{by consistency}\n\\end{aligned}\n\\]\nThe proof begins with the average causal effect and equates it to a statistical estimand: the mean outcome among the treated minus the mean outcome among the untreated. The first quantity involves potential outcomes (with superscripts), whereas the last quantity involves only factual random variables.\nThe exchangeability assumption allows us to move from the first line to the second line. Under exchangeability, the mean outcome that would be realized under treatment (\\(\\text{E}(Y^1)\\)) equals the mean outcome under treatment among those who were actually treated (\\(\\text{E}(Y^0)\\)). Likewise for outcomes under no treatment. This line is true because the treated (\\(A = 1\\)) and the untreated (\\(A = 0\\)) are both simple random samples from the full population.\nThe consistency assumption allows us to move from the second line to the third. Among the treated, (\\(A = 1\\)), the outcome that is realized is \\(Y = Y^1\\). Among the untreated (\\(A = 0\\)), the outcome that is realized is \\(Y = Y^0\\). Under the assumption that factual outcomes are consistent with the potential outcomes under the assigned treatment, the second line equal the third.\nSomething nice about a causal identification proof is that there is no room for error: it is mathematically true that the premise and the assumptions together yield the result. As long as the assumptions hold, the statistical estimand equals the causal estimand. Causal inference thus boils down to research designs and arguments that can lend credibility to the assumptions that let us draw causal claims from data that are observed.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#conditional-exchangeability",
    "href": "nonparametric_identification.html#conditional-exchangeability",
    "title": "Nonparametric Identification",
    "section": "Conditional exchangeability",
    "text": "Conditional exchangeability\nConditional exchangeability is an assumption that exchangeability holds within population subgroups. This assumption holds by design in a conditionally randomized experiment, and may hold under certain causal beliefs in observational settings where the treatment is not randomized.\nSuppose we were to carry out an experiment on a simple random sample of U.S. high school students. Among those performing in the top 25% of their high school class, we randomize 80% to attain a four-year college degree. Among those performing in the bottom 75% of their high school class, we randomize 20% to attain a four-year college degree. We are interested in effects on employment at age 40 (\\(Y\\)).\n\n\n\n\n\n\n\n\n\nThis experiment is conditionally randomized because the probability of treatment (four-year degree) is different among the higher- and lower-performing high school students.\n\nConditionally randomized experiment. An experiment in which the probability of treatment assignment depends on the values of pre-treatment covariates. \\(\\text{P}(A = 1\\mid\\vec{X} = \\vec{x})\\) depends on the value \\(\\vec{x}\\).\n\nIn a conditionally randomized experiment, exchangeability is not likely to hold. People who are treated (assigned to a four-year degree) are more likely to have come from the top 25% of their high school class. They might be especially hard-working people. The treated and untreated might have had different employment at age 40 even if none of them had been treated.\nEven though exchangeability does not hold marginally (across everyone), in a conditionally randomized experiment exchangeability does hold within subgroups. If we focus on those in the top 25% of the class, the 90% who are assigned to finish college are a simple random sample of the entire higher-performing subgroup. If we focus on those in the bottom 75% of the class, the 10% who are assigned to finish college are a simple random sample of the entire lower-performing subgroup.\nFormally, conditional exchangeability takes the exchangeability assumption (\\(\\{Y^0,Y^1\\}\\unicode{x2AEB} A\\)) and adds a conditioning bar \\(\\mid\\vec{X}\\), meaning that this assumption holds within subgroups defined by one or more pre-treatment variables \\(\\vec{X}\\).\n\nConditional exchangeability. The assumption that potential outcomes \\(\\{Y^0,Y^1\\}\\) are independent of treatment \\(A\\) among subpopulations that are identical along a set of pre-treatment covariates \\(\\vec{X}\\). Formally, \\(\\{Y^0,Y^1\\} \\unicode{x2AEB} A \\mid \\vec{X}\\).\n\nConditional exchangeability holds by design in conditionally randomized experiments: the probability of treatment assignment differs across subgroups, but within each subgroup we have a simple randomized experiment where each unit has an equal probability of being treated.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#conditional-average-treatment-effects",
    "href": "nonparametric_identification.html#conditional-average-treatment-effects",
    "title": "Nonparametric Identification",
    "section": "Conditional average treatment effects",
    "text": "Conditional average treatment effects\nIn our conditionally randomized experiment, we could identify conditional average treatment effects: the average effects of college on employment at age 40 (1) among those in the top 25% of their high school class, and the and (2) among those in the bottom 75% of their high school class.\n\nConditional average treatment effect (CATE). The average causal effect within a population subgroup, \\(\\tau(x) = \\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) - \\text{E}\\left(Y^0\\mid \\vec{X} = \\vec{x}\\right)\\).\n\nOnce we assume conditional exchangeability and consistency, CATEs are causally identified by working within a subgroup defined by \\(\\vec{X} = \\vec{x}\\) and taking the difference in means across subgroups of units assigned to treatment and control.\n\\[\n\\begin{aligned}\n&\\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) -  \\text{E}\\left(Y^0\\mid\\vec{X} = \\vec{x}\\right)\\\\\n&= \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 1\\right) - \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 0\\right)\n\\end{aligned}\n\\]\nIn our concrete example, this means that we could first focus on the subgroup for whom \\(\\vec{X} = (\\text{Top 25\\% of high school class})\\). Within this subgroup, we can compare employment at age 40 among those randomized to a 4-year college degree to employment at age 40 among those randomized to finish education after high school. This mean difference identifies the CATE: the average causal effect of college among those in the top 25% of their high school class.\nLikewise, our experiment would also identify the CATE among those in the bottom 75% of their high school class.\nThere are often good reasons to expect the Conditional Average Treatment Effect (CATE) to differ across subpopulations. In our example, suppose that those from the top 25% of the high school class are very creative and hard-working, and would find ways to be employed at age 40 regardless of whether they finished college. The average causal effect of college on employment in this subgroup might be small. Meanwhile, the average causal effect of college on employment might be quite large among those from the bottom 75% of their high school class. This would be an example of effect heterogeneity,\n\nEffect heterogeneity. Differences in Conditional Average Treatment Effects (CATEs) across subpopulations. \\(\\tau(\\vec{x})\\neq\\tau(\\vec{x}')\\).\n\nAn advantage of analyzing randomized experiments conditionally (within subgroups) is that one can search for effect heterogeneity.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#observational-studies-call-for-dags",
    "href": "nonparametric_identification.html#observational-studies-call-for-dags",
    "title": "Nonparametric Identification",
    "section": "Observational studies call for DAGs",
    "text": "Observational studies call for DAGs\nThe material above focused on randomized experiments in which the researcher randomizes the treatment variable. In a randomized experiment, the researcher knows the probability with which treatment was assigned to each unit. Either exchangeability or conditional exchangeability holds by design. Many social science studies, however, focus on questions in which randomization is infeasible due to high costs or even the impossibility of randomizing a treatment. Many social science studies are therefore observational studies.\n\nObservational study. A study seeking to answer a causal question in which the researcher has not randomized the treatment variable.\n\nWhere randomized experiments call for precise research design in which the researcher assigns the treatment, observational studies call for precise theory about how the world has assigned the treatments and how the world has generated outcomes. Directed Acyclic Graphs (DAGs) are a tool to formalize causal assumptions mathematically in graphs.\n\nDirected Acyclic Graph (DAG). A mathematical graph involving nodes and edges, where all edges are directed and there are no cycles. See below for definitions of nodes and edges.\n\nDAGs are especially helpful in observational studies because they provide rules for selecting a sufficient adjustment set (\\(\\vec{X}\\)) such that conditional exchangeability holds.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#nodes-edges-and-paths",
    "href": "nonparametric_identification.html#nodes-edges-and-paths",
    "title": "Nonparametric Identification",
    "section": "Nodes, edges, and paths",
    "text": "Nodes, edges, and paths\nConsider again our conditionally randomized experiment in which the researcher assigned participants to the treatment condition of (four-year college degree) vs (high school degree) with probabilities that depended on high school class rank. In this experiment, being in the top 25% of one’s high school class caused a higher chance of receiving the treatment. We will also assume that both high school performance and college completion may causally shape employment at age 40.\nWe can formalize these ideas in a graph where each node (a letter) is a variable and each edge (\\(\\rightarrow\\)) is a causal relationship.\n\n\n\n\n\n\n\n\n\nBetween each pair of nodes, you can enumerate every path or sequence of edges connecting the nodes.\n\nPath. A path between nodes \\(A\\) and \\(B\\) is any set of edges that starts at \\(A\\) and ends at \\(B\\). Paths can involve arrows in either direction.\n\nIn our DAG above, there are two paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\leftarrow X \\rightarrow Y\\)\n\nWe use paths to determine the reasons why \\(A\\) and \\(Y\\) might be statistically dependent or independent.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#causal-paths",
    "href": "nonparametric_identification.html#causal-paths",
    "title": "Nonparametric Identification",
    "section": "Causal paths",
    "text": "Causal paths\nThe first type of path that we consider is a causal path.\n\nCausal path. A path in which all arrows point in the same direction. For example, \\(A\\) and \\(D\\) could be connected by a causal path \\(A\\rightarrow B \\rightarrow C \\rightarrow D\\).\n\nA causal path creates statistical depends between the nodes because the first node causes the last node, possibly through a sequence of other nodes.\nIn our example, there is a causal path \\(A\\rightarrow Y\\): being assigned to a four-year college degree affects employment at age 40. Because of this causal path, people who are assigned to a four-year degree have different rates of employment at age 40 than those who are not.\nA causal path can go through several variables. For example, if we listed the paths between \\(X\\) and \\(Y\\) we would include the path \\(X\\rightarrow A \\rightarrow Y\\). This is a causal path because being in the top 25% of one’s high school class increases the probability of assignment to a four-year degree in our experiment, which in turn increases the probability of employment at age 40.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#fork-structures",
    "href": "nonparametric_identification.html#fork-structures",
    "title": "Nonparametric Identification",
    "section": "Fork structures",
    "text": "Fork structures\nTo reason about non-causal paths, we have to think about several structures that can exist along these paths. The first such structure is a fork structure.\n\nFork structure. A fork structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) are both caused a third variable (\\(C\\)): \\(A\\leftarrow C \\rightarrow B\\).\n\nIn our example, the path \\(A\\leftarrow X \\rightarrow Y\\) involves a fork structure. being in the top 25% of one’s high school class causally affects both the treatment (college degree) and the outcome (employment at age 40).\nA fork structure creates statistical dependence between \\(A\\) and \\(Y\\) that does not correspond to a causal effect of \\(A\\) on \\(Y\\). In our example, people who are assigned to the treatment value (college degree) are more likely to have been in the top 25% of their high school class, since this high class rank affected treatment assignment in our experiment. A high class rank also affected employment at age 40. Thus, in our experiment the treatment would be associated with the outcome even if finishing college had no causal effect on employment.\nFork structures can be blocked by conditioning on the common cause. In our example, suppose we filter our data to only include those in the top 25% of their high school class. We sometimes use a box to denote conditioning on a variable, (\\(A\\leftarrow\\boxed{X}\\rightarrow Y\\)). Conditioning on \\(X\\) blocks the path because within this subgroup \\(X\\) does not vary, so it cannot cause the values of \\(A\\) and \\(Y\\) within the subgroup. In our example, if we looked among those in the top 25% of their high school classes the only reason college enrollment would be related to employment at age 40 would be the causal effect \\(A\\rightarrow Y\\).\nTo emphasize ideas, it is also helpful to consider a fork structure in an example where the variables have no causal relationship.\nSuppose a beach records for each day the number of ice cream cones sold and the number of rescues by lifeguards. There is no causal effect between these two variables; eating ice cream does not cause more lifeguard rescues and vice versa. But the two are correlated because they share a common cause: warm temperatures cause high ice cream sales and also high lifeguard rescues. A fork structure formalizes this notion: \\((\\text{ice cream sales}) \\leftarrow (\\text{warm temperature}) \\rightarrow (\\text{lifeguard rescues})\\).\nIn the population of days as a whole, this fork structure means that ice cream sales are related to lifeguard rescues. But if we condition on having a warm temperature by filtering to days when the temperature took a particular value, ice cream sales would be unrelated to lifeguard rescues across those days. This is the sense in which conditioning on the common cause variable blocks the statistical associations that would otherwise arise from a fork structure.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#collider-structures",
    "href": "nonparametric_identification.html#collider-structures",
    "title": "Nonparametric Identification",
    "section": "Collider structures",
    "text": "Collider structures\nIn contrast to a fork structure where one variable affects two others (\\(\\bullet\\leftarrow\\bullet\\rightarrow\\bullet\\)), a collider structure is a structure where one variable is affected by two others.\n\nCollider structure. A collider structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) both cause a third variable (\\(C\\)). We say that \\(C\\) is a collider on the path \\(A\\rightarrow C \\leftarrow B\\).\n\nFork and collider structures have very different properties, as we will illustrate through an example.\nSuppose that every day I observe whether the grass on my lawn is wet. I have sprinklers that turn on with a timer at the same time every day, regardless of the weather. It also sometimes rains. When the grass is wet, it is wet because either the sprinklers have been on or it has been raining.\n\\[\n(\\text{sprinklers on}) \\rightarrow (\\text{grass wet}) \\leftarrow (\\text{raining})\n\\]\nIf I look across all days, the variable (sprinklers on) is unrelated to the variable (raining). After all, the sprinklers are just on a timer! Formally, we say that even though (sprinklers on) and (raining) are connected by the path above, this path is blocked by the collider structure. A path does not create dependence between two variables when it contains a collider structure.\nIf I look only at the days when the grass is wet, a different pattern emerges. If the grass is wet and the sprinklers have not been on, then it must have been raining: the grass had to get wet somehow. If the grass is wet and it has not been raining, then the sprinklers must have been on. Once I look at days when the grass is wet (or condition on the grass being wet), the two input variables become statistically associated.\nA collider blocks a path when that collider is left unadjusted, but conditioning on the collider variable opens the path containing the collider.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#open-and-blocked-paths",
    "href": "nonparametric_identification.html#open-and-blocked-paths",
    "title": "Nonparametric Identification",
    "section": "Open and blocked paths",
    "text": "Open and blocked paths\nA central purpose of a DAG is to connect causal assumptions to implications about associations that should be present (or absent) in data under those causal assumptions. To make this connection, we need a final concept of open and blocked paths.\n\nA path is blocked if it contains an unconditioned collider or a conditioned non-collider. Otherwise, the path is open. An open path creates statistical dependence between its terminal nodes whereas a blocked path does not.\n\nAs examples for a path with no colliders,\n\n\\(A\\leftarrow B \\rightarrow C \\rightarrow D\\) is an open path because no variables are conditioned and it contains no colliders.\n\\(A\\leftarrow \\boxed{B}\\rightarrow C \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(B\\).\n\\(A\\leftarrow  B\\rightarrow \\boxed{C} \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(C\\).",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#determining-statistical-dependence",
    "href": "nonparametric_identification.html#determining-statistical-dependence",
    "title": "Nonparametric Identification",
    "section": "Determining statistical dependence",
    "text": "Determining statistical dependence\nWe are now ready to use DAGs to determine if \\(A\\) and \\(B\\) are statistically dependent. The process involves three steps.\n\nList all paths between \\(A\\) and \\(B\\).\nCross out any paths that are blocked.\nThe causal assumptions imply that \\(A\\) and \\(B\\) may be statistically dependent only if any open paths remain.\n\nAs an example, below we consider a hypothetical DAG.\n\n\n\n\n\n\n\n\n\n1. Marginal dependence\nMarginally without any adjustment, are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow X\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(A\\leftarrow X\\rightarrow D\\)\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.\n2. Dependence conditional on \\(X\\)\nIf we condition on \\(X\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause no open path remains, \\(A\\) and \\(D\\) are statistically independent.\n3. Dependence conditional on \\(\\{X,B\\}\\)\nIf we condition on \\(X\\) and \\(B\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(A\\rightarrow \\boxed{B} \\leftarrow C\\rightarrow D\\) (open since collider \\(B\\) is conditioned)\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#causal-identification-with-dags",
    "href": "nonparametric_identification.html#causal-identification-with-dags",
    "title": "Nonparametric Identification",
    "section": "Causal identification with DAGs",
    "text": "Causal identification with DAGs\nWhen our aim is to identify the average causal effect of \\(A\\) on \\(Y\\), we want to choose a set of variables for adjustment so that all remaining paths are causal paths. We call this a sufficient adjustment set.\n\nA sufficient adjustment set for the causal effect of \\(A\\) on \\(Y\\) is a set of nodes that, when conditioned, block all non-causal paths between \\(A\\) and \\(Y\\).\n\nIn our example from the top of this page, there were two paths between \\(A\\) and \\(Y\\):\n\n\\((A\\text{: college degree})\\rightarrow (Y\\text{: employed at age 40})\\)\n\\((A\\text{: college degree})\\leftarrow (X\\text{: top 25\\% of high school class})\\rightarrow (Y\\text{: employed at age 40})\\)\n\nIn this example, \\(X\\) is a sufficient adjustment set. Once we condition on \\(X\\) by e.g. filtering to those in the top 25% of their high school class, the only remaining path between \\(A\\) and \\(Y\\) is the causal path \\(A\\rightarrow Y\\). Thus, the difference in means in \\(Y\\) across \\(A\\) within subgroups defined by \\(X\\) identifies the conditional average causal effect of \\(A\\) on \\(Y\\).\nA more difficult example.\nSufficient adjustment sets can be much more complicated. As an example, consider the DAG below.\n\n\n\n\n\n\n\n\n\nWe first list all paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\leftarrow X_2\\rightarrow Y\\)\n\\(A\\leftarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_3\\leftarrow X_2 \\rightarrow Y\\)\n\nThe first two paths are causal, and the others are non-causal. We want to find a sufficient adjustment set to block all the non-causal paths.\nIn order to block paths (3), (5), and (6) we might condiiton on \\(X_3\\). But doing so opens path (2), which was otherwise blocked by the collider \\(X_3\\). In order to also block path (2), we might additionally condition on \\(X_1\\). In this case, our sufficient adjustment set is \\(\\{X_1,X_3\\}\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\leftarrow X_2\\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3}\\leftarrow X_2 \\rightarrow Y}\\)\n\nThen the only open paths are paths (1) and (2), both of which are causal paths from \\(A\\) to \\(Y\\).\nSometimes there are several sufficient adjustment sets. In this example, sufficient adjustment sets include:\n\n\\(\\{X_1,X_3\\}\\)\n\\(\\{X_2,X_3\\}\\)\n\\(\\{X_1,X_2,X_3\\}\\)\n\nWe sometimes call the first two minimal sufficient adjustment sets because they are the smallest.\n\nA minimal sufficient adjustment set is an adjustment set that achieves causal identification by conditioning on the fewest number of variables possible.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#how-to-draw-a-dag",
    "href": "nonparametric_identification.html#how-to-draw-a-dag",
    "title": "Nonparametric Identification",
    "section": "How to draw a DAG",
    "text": "How to draw a DAG\nSo far, we have focused on causal identification with a DAG that has been given. But how do you draw one for yourself?\nWhen drawing a DAG, there is an important rule: any node that would have edges pointing into any two nodes already represented in the DAG must be included. This is because what you omit from the DAG is far more important than what you include in the DAG. Many of your primary assumptions are about the nodes and edges that you leave out.\nFor example, suppose we are estimating the causal effect of a college degree on employment at age 40. After beginning our DAG with only these variables, we have to think about any other variables that might affect these two. High school performance is one example. Then we have to include any nodes that affect any two of {high school performance, college degree, employment at age 40}. Perhaps a person’s parents’ education affects that person’s high school performance and college degree attainment. Then parents’ education should be included as an additional node. The cycle continues, so that in observational causal inference settings you are likely to have a DAG with many nodes.\nIn practice, you may not have data on all the nodes that comprise the sufficient adjustment set in your graph. In this case, we recommend that you first draw a graph under which you can form a sufficient adjustment set with the measured variables. This allows you to state one set of causal beliefs under which your analysis can answer your causal question. Then, also draw a second DAG that includes the other variables you think are relevant. This will enable you to reason about the sense in which your results could be misleading because of omitting important variables.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "nonparametric_identification.html#what-to-read",
    "href": "nonparametric_identification.html#what-to-read",
    "title": "Nonparametric Identification",
    "section": "What to read",
    "text": "What to read\nIf you’d like to learn more about defining and identifying causal effects, look here:\n\nHern'an, Miguel A. and James M. Robins. 2025. Causal Inference: What If?. Boca Raton: Chapman & Hall / CRC.\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation. Here is a link to read online through the UCLA Library.\n\nIf you’d like to learn from some canonical references on DAGs, look here:\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. New York: Wiley.\nPearl, Judea. 2009. Causality: Models, Reasoning, and Inference. Edition 2. Cambridge: Cambridge University Press.\nGreenland, Sander, Judea Pearl, and James M. Robins. 1999. “Causal diagrams for epidemiologic research.” Epidemiology 10(1):37–48.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "data_driven_selection.html",
    "href": "data_driven_selection.html",
    "title": "Data-driven selection of an estimator",
    "section": "",
    "text": "If you are taking notes, here is a PDF of the slides and a PDF of this page. If you are a Stata user, most of this page is R code. The one most important exercise is sample splitting, for which you can use sample_split.do.\nQuantitative social scientists have long faced the question of how to choose a model. Even within the scope of linear regression, one might wonder whether a model that interacts two predictors is better than one that includes them only additively. As computational advances have yielded new algorithms for prediction, the number of choices has exploded. Many models are possible. How should we choose?\nAn algorithm for prediction takes as its input a feature vector \\(\\vec{x}\\) and returns as its output a predicted value, \\(\\hat{y}\\). One way to choose among several algorithms is to find the one that produces predictions \\(\\hat{y}\\) that are as close as possible to the true outcomes \\(y\\). While this predictive metric might seem grounded in data science, this page will show how metrics of predictive performance can also help with a classical social science task: estimating subgroup means. By the end of the page, we will have motivated why one should care about metrics of predictive performance and learned tools to estimate predictive performance by sample splitting.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#predicting-for-individuals",
    "href": "data_driven_selection.html#predicting-for-individuals",
    "title": "Data-driven selection of an estimator",
    "section": "Predicting for individuals",
    "text": "Predicting for individuals\nContinuing with the example of baseball player salaries, we consider a model to predict salary this year as a linear function of team average salary from last year. We first prepare the environment and load data.\n\nlibrary(tidyverse)\n\n\nbaseball &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nThen we draw a sample of 5 players per team.\n\nset.seed(90095)\nlearning &lt;- baseball |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nWe estimate a linear regression model on this sample.\n\nlinear_regression &lt;- lm(\n  salary ~ team_past_salary, \n  data = learning\n)\n\nThe figure below visualizes the predictions from this linear regression, calculated for all players who were not part of the random learning sample.\n\n\n\n\n\n\n\n\n\nWhile one might have hoped to tell a story about high-quality prediction, the dominant story in the individual-level prediction plot is one of poor prediction: players’ salaries vary widely around the estimated regression line. To put that fact to a number, one might consider \\(R^2\\) which involves a ratio of two expected squared prediction errors, one from the prediction function \\(\\hat{f}\\) and one from a comparison model that predicts the mean for all cases.1\n\\[\nR^2\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                Y - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(Y - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{Variance of $Y$}\n}\n\\] In other words, subtracting the predicted values from the individual players’ salaries only reduces the expected squared error by 5.8%. If the goal is to predict for individuals, the model does not seem very good!",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#estimating-subgroup-means",
    "href": "data_driven_selection.html#estimating-subgroup-means",
    "title": "Data-driven selection of an estimator",
    "section": "Estimating subgroup means",
    "text": "Estimating subgroup means\nA social scientist might respond that the goal was never to accurately predict the salary of any individual baseball player. Rather, the data on individual players was in service of a more aggregate goal: estimating the mean salary on each team. Noting that the prediction is the same for every player on a team, the social scientist might propose the graph below, in which the unit of analysis is a team instead of a player.\n\n\n\n\n\n\n\n\n\nThe social scientist might argue that the model is quite good for team salaries. If we take the goal to be to estimate the team average salary, then we might create an analogous version of \\(R^2\\) focused on estimation of team-average salaries.2\n\\[\nR^2_\\text{Group}\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{If Predicted $\\text{E}(Y)$ for Everyone}\n}\n\\] By that metric, the model seems quite good, predicting away 57.1% of the expected squared error at the team level. Surprisingly, a model that was not very good at predicting for individuals might be quite good at predicting the team-average outcomes!\nOne might respond that prediction and estimation are simply different goals, with little to do with one another. But in fact the two are mathematically linked. Given two models to choose from, the one that predicts better (in squared error loss) will also be a better estimator of the subgroup means.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#prediction-and-estimation-are-connected",
    "href": "data_driven_selection.html#prediction-and-estimation-are-connected",
    "title": "Data-driven selection of an estimator",
    "section": "Prediction and estimation are connected",
    "text": "Prediction and estimation are connected\nTo formalize the problem of choosing an estimator, suppose we have two prediction functions \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\). Each function takes in a vector of features \\(\\vec{x}\\) and returns a predicted value, \\(\\hat{f}_1(\\vec{x})\\) or \\(\\hat{f}_2(\\vec{x})\\). We will assume for simplicity that each function has already been learned on a simple random sample from our population, and that the remaining units available to us are those that were not used in the learning process.\nSuppose we draw a random unit with features \\(\\vec{X}\\) and outcome \\(Y\\). For this unit, algorithm one would have a squared prediction error \\((Y - \\hat{f}_1(\\vec{X}))^2\\). We might score each algorithm’s performance by the average squared prediction error, with the average taken across units.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}} = \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] In our baseball example, the algorithm \\(\\hat{f}_1\\) makes an error when Mookie Betts has a salary of 21.2m but the algorithm only predicts 7.0m. The expected squared prediction error is the squared difference between these two values, taken on average over all players.\nOur social scientist has already replied that we rarely care about predicting the salary of an individual player. Instead, our questions are really about estimating subgroup means, such as the mean pay on each team. The social scientist might instead want to know about estimation error, \\[\n\\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} = \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] where \\(\\hat{f}(\\vec{X})\\) is the predicted salary of a player on team \\(\\vec{X}\\) and \\(\\text{E}(Y\\mid\\vec{X})\\) is the true population average salary on that team. This social scientist does not care about predicting for individual salaries \\(Y\\), but rather about accurately estimating the mean salary \\(\\text{E}(Y\\mid\\vec{X})\\) in each team.\nA little math (proof at the end of this section) can show that these two goals are actually closely linked. Expected squared prediction error equals expected squared estimation error plus expected within-group variance.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}  = \\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} + \\underbrace{\\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]}_{\\substack{\\text{Expected Within-}\\\\\\text{Group Variance}}}\n\\] Taking our baseball example, there are two sources of prediction error for Mookie Betts.\nFirst, salaries vary among the Dodger players (\\(\\text{V}(Y\\mid\\vec{X} = \\text{Dodgers})\\)). Because Mookie Betts and Freddie Freeman are both players on the Dodgers, they are identical from the perspective of the model (they have identical \\(\\vec{X}\\) values) and it has to make the same prediction for both of them. Just as there is variance within the Dodgers, there is variance within all MLB teams. The within-team variance averaged over teams (weighted by size) is the term at the right of the decomposition.\nSecond, the expected squared estimation error is the average squared difference between each player’s predicted salary and the true mean pay on that player’s team, \\(\\text{E}(Y\\mid\\vec{X})\\). In the case of Mookie Betts, this is the difference between the prediction for Mookie Betts and the true mean salary on his team, the Dodgers. Estimation error corresponds to our error if our goal is to estimate the mean salary on each team, instead of predicting the salary for each individual.\nNow suppose two prediction algorithms \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\) have different performance. For example, maybe the first algorithm is a better predictor: \\(\\text{ESPE}(\\hat{f}_1) &lt; \\text{ESPE}(\\hat{f}_2)\\). Regardless of which algorithm is used, the within-group variance component of the decomposition is unchanged. Therefore, if algorithm 1 is the better predictor, then it must also be the better estimator: \\(\\text{ESEE}(\\hat{f}_1) &lt; \\text{ESEE}(\\hat{f}_2)\\).\nIn fact, suppose an algorithm was omniscient and managed to predict the true conditional mean function for every observation, \\(\\hat{f}_\\text{Omniscient}(\\vec{X}) = \\text{E}(Y\\mid\\vec{X})\\). Then estimation error would be zero for this function. Prediction error would equal the expected within-group variance. The best possible prediction function (with squared error loss) is the conditional mean. This is one intuitive reason why an algorithm with good predictive performance is also a good estimator.\nThese facts motivate an idea for choosing an estimation function: to estimate conditional means well, choose the algorithm that minimizes squared prediction error.\nAppendix to section. A proof of the decomposition is provided below, but the ideas above are more important than the details of the proof.\n\\[\n\\begin{aligned}\n\\text{ESPE}(\\hat{f})\n&= \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&\\text{Add zero} \\\\\n&= \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X}) + \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&= \\underbrace{\n  \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right) ^ 2\\right]\n}_{=\\text{E}[\\text{V}(Y\\mid\\vec{X})]}\n  + \\underbrace{\n  \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right) ^ 2\\right]\n  }_{=\\text{ESEE}(\\hat{f})}\n  \\\\\n  &\\qquad + \\underbrace{\n  2\\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right)\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)\\right]\n  }_{\\substack{=\\text{Cov}[Y - \\text{E}(Y\\mid\\vec{X}), E(Y\\mid\\vec{X} - \\hat{f}(\\vec{X}))]=0\\\\\\text{covariance of within-group error and estimation error,}\\\\\\text{equals zero if the test case }Y\\text{ is not used to learn }\\hat{f}}} \\\\\n  &= \\text{ESEE}(\\hat{f}) + \\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]\n\\end{aligned}\n\\]",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#why-out-of-sample-prediction-matters",
    "href": "data_driven_selection.html#why-out-of-sample-prediction-matters",
    "title": "Data-driven selection of an estimator",
    "section": "Why out-of-sample prediction matters",
    "text": "Why out-of-sample prediction matters\nThe connection between prediction and estimation opens a powerful bridge: we can find good estimators by exploring which algorithms predict well. But it is important to remember that this bridge exists only for out-of-sample prediction error: error when predictions are made on a new sample that did not include the initial predictions.\n\nk-nearest neighbors estimator\nTo illustrate in-sample and out-of-sample prediction error, we consider a nearest neighbors estimator. When making a prediction for player \\(i\\), we might worry that we have too few sampled units on the team of player \\(i\\). We might solve this issue by averaging the sampled salaries within the team of player \\(i\\) and also the \\(k\\) nearest teams whose salaries in the past season were most similar.\nFor example, the Dodgers’ past-year average salary was $8.39m. The most similar team to them was the N.Y. Mets, who had an average salary of $8.34m. Because the past salaries are so similar, we might pool information: predict the Dodgers’ mean salary by the average of sampled players on both the Dodgers and the N.Y. Mets. If we wanted to pool more information, we might include the next-most similar team, the N.Y. Yankees with past salary $7.60m. We could pool more by also including the 3rd-nearest neighbor (Philadelphia, $6.50m), the 4th-nearest neighbor (San Diego, $6.39m), and so on. The more neighbors we include, the more pooled our estimate becomes.\n\n\nIn-sample performance\nHow many neighbors should we include? We first consider evaluating by in-sample performance: learn the estimator on a sample and evaluate predictive performance in that same sample. We repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error in that same sample\n\nThe blue line in the figure below shows results. In-sample mean squared prediction error is lowest when we pool over 0 neighbors. With in-sample evaluation, the predictions become gradually worse (higher mean squared error) as we pool information over more teams. If our goal were in-sample prediction, we should choose an estimator that does not pool information at all: the Dodgers’ population mean salary would be estimated by the mean among the sampled Dodgers only.\n\n\n\n\n\n\n\n\n\n\n\nOut-of-sample performance\nThe red line in the figure above shows a different performance metric: out-of-sample performance. This line shows what happens when we repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error on all units not included in that sample\n\nThe red line of out-of-sample performance looks very different than the blue line of in-sample performance, in two ways.\nFirst, the red line is always higher than the blue line. It is always harder to predict out-of-sample cases than to predict in-sample cases. This is unsurprising—the blue line was cheating by getting to see the outcomes of the very cases it was trying to predict!\nSecond, the red line exhibits a U-shaped relationship. Predictive performance improves (lower mean squared error) as we pool information over a few nearby teams. This is because the variance of the estimator is declining. After reaching an optimal value at around 10 neighbors, predictive performance begins to become worse (higher mean squared error).\nOne way to think about the red and blue lines is in terms of the signal and the noise. In any particular sample of 10 Dodger players, there is some amount of signal (true information about the Dodger population mean) and some amount of noise (randomness in the sample average arising from which 10 players we happened to sample). The distinction is irrelevant for in-sample prediction error, for which a close fit to both the signal and the noise yields low prediction error. But for out-of-sample prediction error, fitting to the signal improves performance while fitting to the noise harms performance. As one moves to the right in the graph, one is getting less of the signal and less of the noise. Thus, the blue line of in-sample performance gets consistently worse. The red line improves at first as the reduction in noise outweighs the reduction in signal, but then gets worse as the reduction in signal begins to outweigh the reduction in noise. In-sample prediction error is a poor metric because fitting closely to the noise can make this metric look misleadingly good. Out-of-sample error avoids this problem. The best value for nearest neighbors is the one that optimizes the tradeoff between signal and noise, where the red curve is minimized.\nAnother way to think about the lines is in terms of a bias-variance tradeoff. As we pool the Dodgers together with the N.Y. Mets and other teams, the variance of the estimator declines because the Dodger predicted salary is averaged over more teams. But the bias of the estimator increases: the N.Y. Mets are not the Dodgers, and including them in the average induces a bias. The minimum of the red curve is the amount of information pooling that optimizes the bias-variance tradeoff.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#sample-splitting",
    "href": "data_driven_selection.html#sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Sample splitting",
    "text": "Sample splitting\nThe illustration above showed that we ideally evaluate an estimator learned in a sample by performance when making predictions for the rest of the population (excluding that sample). But in practice, we often have access only to the sample and not to the rest of the population. To learn out-of-sample predictive performance, we need sample splitting.\nIn its simplest version, sample splitting proceeds in three steps:\n\nRandomly partition sampled cases into training and test sets\nLearn the prediction function among training cases\nEvaluate its performance among test cases\n\nVisually, we begin by randomly assigning cases into training or test sets.\n\n\n\n\n\n\n\n\n\nThen we separate these into two datasets. We use the train set to learn the model and the test set to evaluate the performance of the learned model.\n\n\n\n\n\n\n\n\n\nIn code, we can carry out a train-test split by first loading the baseball population,\n\nbaseball_population &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nand drawing a sample of 10 players per team.\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  ungroup()\n\nWe can then create the split. The code below first stratifies by team and then randomly samples 50% of cases to be used for the train set.\n\ntrain &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  slice_sample(prop = .5)\n\nThe code below takes all remaining cases not used for training to be used as the test set.\n\ntest &lt;- baseball_sample |&gt;\n  anti_join(train, by = join_by(player))\n\nWe can then learn a prediction function in the train set and evaluate performance in the test set. For example, the code below uses OLS.\n\nols &lt;- lm(salary ~ team, data = train)\ntest_mse &lt;- test |&gt;\n  # Make predictions\n  mutate(yhat = predict(ols, newdata = test)) |&gt;\n  # Calculate squared errors\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  # Summarize\n  summarize(mse = mean(squared_error))",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "href": "data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Tuning parameters by sample splitting",
    "text": "Tuning parameters by sample splitting\nOne way we might use sample splitting is for parameter tuning: to choose the value of some unknown tuning parameter such as the penalty \\(\\lambda\\) in ridge regression.\n\nlibrary(glmnet)\n\nLoaded glmnet 4.1-8\n\nridge_regression &lt;- glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nThe glmnet package makes estimates at many values of the penalty parameter \\(\\lambda\\). We can make predictions in the test set from models using these various values of \\(\\lambda\\).\n\npredicted &lt;- predict(\n  ridge_regression,\n  newx = model.matrix(~team, data = test)\n)\n\nWe can extract the penalty parameter values with ridge_regression$lambda, organizing them in a tibble for ease of access.\n\nlambda_tibble &lt;- tibble(\n  lambda = ridge_regression$lambda,\n  lambda_index = colnames(predicted)\n)\n\nFor each \\(\\lambda\\) value, the predictions are a column of predicted. The code below wrangles these predictions into a tidy format.\n\npredicted_tibble &lt;- as_tibble(predicted) |&gt;\n  # Append the actual value of the test outcome\n  mutate(y = test |&gt; pull(salary)) |&gt;\n  pivot_longer(cols = -y, names_to = \"lambda_index\", values_to = \"yhat\") |&gt;\n  left_join(lambda_tibble, by = join_by(lambda_index)) |&gt;\n  mutate(squared_error = (y - yhat) ^ 2)\n\nAt each \\(\\lambda\\) value, we can calculate mean squared error in the test set.\n\nmse &lt;- predicted_tibble |&gt;\n  group_by(lambda) |&gt;\n  summarize(mse = mean(squared_error))\n\nThe figure below visualizes these estimates. As the penalty parameter grows larger, predictive performance improves (lower error) to a point and then begins to get worse. The selected best value of the tuning parameter \\(\\lambda\\) is highlighted in blue.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#cross-validation",
    "href": "data_driven_selection.html#cross-validation",
    "title": "Data-driven selection of an estimator",
    "section": "Cross-validation",
    "text": "Cross-validation\nA downside of 50-50 sample splitting is that the train set is only half as large as the full available sample. We might prefer if it were larger, for instance 80% of the data used for training and only 20% used for testing. But then our estimates of performance in the test set might be noisy.\nOne solution to this problem is cross-validation, which proceeds in a series of steps:\n\nRandomize the sampled cases into a set of folds (e.g., 5 folds).\nTake fold 1 as the test set and estimate predictive performance.\nTake fold 2 as the test set and estimate predictive performance.\nIterate until all folds have served as the test set\nAverage predictive performance over the folds\n\n\n\n\n\n\n\n\n\n\nOptionally, repeat for many repetitions of randomly assigning cases to folds to reduce stochastic error.\nCross-validation is so common that it is already packaged into some of the learning algorithms we have considered in class. For example, the code below carries out cross-validation to automatically select the penalty parameter for ridge regression.\n\nridge_regression_cv &lt;- cv.glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nWe can use ridge_regression_cv$lambda.min to extract the chosen value of \\(\\lambda\\) that minimizes cross-validated mean squared error (2.7074931^{7}). We can also visualize the performance with a plot() function.\n\nplot(ridge_regression_cv)\n\n\n\n\n\n\n\n\nWe can make predictions at the chosen value of \\(\\lambda\\) by specifying the s argument in the predict() function.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat = predict(\n      ridge_regression_cv,\n      s = \"lambda.min\",\n      newx = model.matrix(~team, data = test)\n    )[,1]\n  )",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "href": "data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Choosing an algorithm by sample splitting",
    "text": "Choosing an algorithm by sample splitting\nIn the example above, we used sample splitting to choose the optimal value of a penalty parameter (\\(\\lambda\\)) within a particular algorithm for prediction (ridge regression). One can also use a train-test split to choose among many very different estimators. For example, we might estimate OLS, a tree, and a forest.\n\nlibrary(rpart)\nlibrary(grf)\n\n\nols &lt;- lm(salary ~ team_past_salary, data = train)\ntree &lt;- rpart(salary ~ team_past_salary, data = train)\nforest &lt;- regression_forest(\n  X = model.matrix(~team_past_salary, data = train),\n  Y = train |&gt; pull(salary)\n)\n\nWe can make predictions for all of these estimators in the test set.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat_ols = predict(ols, newdata = test),\n    yhat_tree = predict(tree, newdata = test),\n    yhat_forest = predict(forest, newdata = model.matrix(~team_past_salary, data = test))$predictions\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"yhat\"), names_to = \"estimator\", values_to = \"yhat\"\n  )\n\nThe figure below visualizes the predicted values.\n\n\n\n\n\n\n\n\n\nWe can calculate mean squared error in the test set for each algorithm to determine which one is producing the best predictions.\n\nmse &lt;- predicted |&gt;\n  group_by(estimator) |&gt;\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  summarize(mse = mean(squared_error))",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#footnotes",
    "href": "data_driven_selection.html#footnotes",
    "title": "Data-driven selection of an estimator",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen estimating \\(R^2\\), we often use the training sample mean as an estimator of \\(\\text{E}(Y)\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎\nWhen estimating \\(R^2_\\text{Group}\\), we often use the subgroup training sample mean as an estimator of \\(\\text{E}(Y\\mid\\vec{X})\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\n\n\n\nTerm\nMeaning\n\n\n\n\nUnit of analysis\nA row of your data\n\n\nUnit-specific quantity\nOutcome or potential outcome(s)\n\n\nTarget population\nSet of units over which to aggregate\n\n\n\n\nWhy? To ask \\(\\hat{Y}\\) questions, not \\(\\hat\\beta\\) questions"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-1",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-1",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nSome language notes:\n\nCausal language\n\nX verb Y\n\nNon-causal language\n\ndifferences in Y across X"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-2",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-2",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nMany submitted models like this:\n\\[E(Y \\mid X) = \\alpha + \\beta X\\]\n\nWhen \\(X\\) is binary, this model is like taking subgroup means.\n\n\n\\[\n\\begin{aligned}\nE(Y\\mid X = 0) &= \\alpha \\\\\nE(Y\\mid X = 1) &= \\alpha + \\beta\n\\end{aligned}\n\\]\n\n\nIt is not really a “model” at all"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-3",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-3",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nOther things to discuss?"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#goals-for-today",
    "href": "slides/lec2/penalized_regression_slides.html#goals-for-today",
    "title": "Algorithms for prediction",
    "section": "Goals for today",
    "text": "Goals for today\n\nreview OLS to predict\nunderstand regularization to estimate\nmany parameters while avoiding high variance\nwalk through three regularized linear model algorithms\n\nmultilevel models\nridge regression\nLASSO regression"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\nAll 944 Major League Baseball Players, Opening Day 2023.\nCompiled by USA Today\nI appended each team’s win-loss record from 2022\nAvailable in baseball_population.csv"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-1",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-1",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\nLoad packages:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n\n\nLoad data:\n\nbaseball_population &lt;- read_csv(\n  \"https://ilundberg.github.io/soc212b/data/baseball_population.csv\"\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-2",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-2",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\nbaseball_population |&gt; print(n = 3)\n\n# A tibble: 944 × 5\n  player               salary position team    team_past_record\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457\n2 Marte, Ketel       11600000 2B       Arizona            0.457\n3 Ahmed, Nick        10375000 SS       Arizona            0.457\n# ℹ 941 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-3",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-3",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\n\nCode\nbaseball_population |&gt;\n  group_by(position) |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  mutate(position = fct_reorder(position, -salary)) |&gt;\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across positions\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-4",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-4",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\n\nCode\nbaseball_population |&gt;\n  group_by(team) |&gt;\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |&gt;\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across teams\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\n\nWe have the full population:\n\ntrue_dodger_mean &lt;- baseball_population |&gt;\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |&gt;\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |&gt;\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-1",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-1",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nImagine we have\n\npredictors for everyone\nsalary for a sample of 5 players per team"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-2",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-2",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-3",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-3",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-4",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-4",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-5",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-5",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\n\nlearn model on everyone\ncreate data to predict: the Dodger population\npredict the outcome in this target population\naverage over units"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-6",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-6",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nDraw a sample of 5 players per team.\n\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup() |&gt;\n  print()\n\n# A tibble: 150 × 5\n   player                 salary position team    team_past_record\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Carroll, Corbin       1625000 OF       Arizona            0.457\n 2 Herrera, Jose          724300 C        Arizona            0.457\n 3 Melancon, Mark*       6000000 RHP      Arizona            0.457\n 4 Ahmed, Nick          10375000 SS       Arizona            0.457\n 5 Robinson, Kristian**   720000 OF       Arizona            0.457\n 6 Arcia, Orlando        2300000 SS       Atlanta            0.623\n 7 Lee, Dylan             730000 LHP      Atlanta            0.623\n 8 Pillar, Kevin         3000000 OF       Atlanta            0.623\n 9 Matzek, Tyler*        1200000 LHP      Atlanta            0.623\n10 Hilliard, Sam          750000 OF       Atlanta            0.623\n# ℹ 140 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-7",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-7",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nOur 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n# A tibble: 5 × 5\n  player           salary position team         team_past_record\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;\n1 Phillips, Evan  1300000 RHP      L.A. Dodgers            0.685\n2 Miller, Shelby  1500000 RHP      L.A. Dodgers            0.685\n3 Taylor, Chris  15000000 OF       L.A. Dodgers            0.685\n4 Betts, Mookie  21158692 OF       L.A. Dodgers            0.685\n5 Outman, James    722500 OF       L.A. Dodgers            0.685"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-create-data-to-predict",
    "href": "slides/lec2/penalized_regression_slides.html#task-create-data-to-predict",
    "title": "Algorithms for prediction",
    "section": "Task: Create data to predict",
    "text": "Task: Create data to predict\nWe want to predict salary for all Dodger players:\n\ndodgers_to_predict &lt;- baseball_population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  select(-salary)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-model",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-model",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Model",
    "text": "Ordinary Least Squares: Model\nModel salary as a function of team_past_record\n\nols &lt;- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-predict",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-predict",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Predict",
    "text": "Ordinary Least Squares: Predict\n\nols_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |&gt;\n  print(n = 3)\n\n# A tibble: 35 × 5\n  player           position team         team_past_record predicted_salary\n  &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;            &lt;dbl&gt;\n1 Freeman, Freddie 1B       L.A. Dodgers            0.685         5552999.\n2 Heyward, Jason   OF       L.A. Dodgers            0.685         5552999.\n3 Betts, Mookie    OF       L.A. Dodgers            0.685         5552999.\n# ℹ 32 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-average",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-average",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Average",
    "text": "Ordinary Least Squares: Average\nAverage predictions over the target population\n\nols_estimate &lt;- ols_predicted |&gt;\n  summarize(ols_estimate = mean(predicted_salary)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n  ols_estimate\n         &lt;dbl&gt;\n1     5552999."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS",
    "text": "Performance of OLS\n\nWe usually have only one sample\nIn this case, we have the population\n\n\n\nPlan\n\ndraw repeated samples\nevaluate performance over repeated samples"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\nA function that\n\ntakes in a sample\nreturns an estimate"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-1",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-1",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\n\nols_estimator &lt;- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-2",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-2",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\nApply the estimator in repeated samples.\n\n\nCode\nmany_sample_estimates &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample\n  estimate &lt;- ols_estimator(baseball_sample)\n  return(estimate)\n}"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-3",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-3",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\n\n\nCode\ntibble(y = many_sample_estimates) |&gt;\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |&gt;\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#model-approximation-error",
    "href": "slides/lec2/penalized_regression_slides.html#model-approximation-error",
    "title": "Algorithms for prediction",
    "section": "Model approximation error",
    "text": "Model approximation error\n\n\nCode\npopulation_ols &lt;- lm(salary ~ team_past_record, data = baseball_population)\nforplot &lt;- baseball_population |&gt;\n  mutate(fitted = predict(population_ols)) |&gt;\n  group_by(team) |&gt;\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers &lt;- forplot |&gt; filter(team == \"L.A. Dodgers\")\nforplot |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\", text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#model-approximation-error-1",
    "href": "slides/lec2/penalized_regression_slides.html#model-approximation-error-1",
    "title": "Algorithms for prediction",
    "section": "Model approximation error",
    "text": "Model approximation error\nHow to solve model approximation error? \nA model with more parameters \n\nols_team_categories &lt;- lm(\n  salary ~ team,\n  data = baseball_sample\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ols-with-many-parameters",
    "href": "slides/lec2/penalized_regression_slides.html#ols-with-many-parameters",
    "title": "Algorithms for prediction",
    "section": "OLS with many parameters",
    "text": "OLS with many parameters\n\n\nCode\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-1",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-1",
    "title": "Algorithms for prediction",
    "section": "Regularization",
    "text": "Regularization\nTwo estimators:\n\nnonparametric mean of 5 sampled players on each team\n(high variance)\n\nlinear prediction: linear fit on team past record\n(biased by model approximation error)\n\n\nWe might regularize the nonparametric toward the linear prediction"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-2",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-2",
    "title": "Algorithms for prediction",
    "section": "Regularization",
    "text": "Regularization\n\n\nCode\nestimates_by_w &lt;- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |&gt;\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Sample Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-how-much",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-how-much",
    "title": "Algorithms for prediction",
    "section": "Regularization: How much?",
    "text": "Regularization: How much?\n\nWant to minimize expected squared error.\n\\[\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-how-much-1",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-how-much-1",
    "title": "Algorithms for prediction",
    "section": "Regularization: How much?",
    "text": "Regularization: How much?\n\n\nCode\nrepeated_simulations &lt;- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  \n  ols_fit &lt;- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate &lt;- predict(\n    ols_fit, \n    newdata = baseball_population |&gt; \n      filter(team == \"L.A. Dodgers\") |&gt;\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate &lt;- a_sample |&gt;\n    filter(team == \"L.A. Dodgers\") |&gt;\n    summarize(salary = mean(salary)) |&gt;\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated &lt;- repeated_simulations |&gt;\n  group_by(w) |&gt;\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |&gt;\n  mutate(best = mse == min(mse))\n\naggregated |&gt;\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", text = element_text(size = 18)) +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-idea",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-idea",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Idea",
    "text": "Multilevel models: Idea\n\nmany groups\na few observations per group\npartially pool between\n\ngroup-specific mean estimates (high variance)\na model pooled across groups (potentially biased)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-code",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-code",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Code",
    "text": "Multilevel models: Code\n\nlibrary(lme4)\n\n\nmultilevel &lt;- lmer(\n  salary ~ team_past_record + (1 | team), \n  data = baseball_sample\n)\n\nMake predictions\n\nmultilevel_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Intuition",
    "text": "Multilevel models: Intuition\n\n\nCode\np &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition-1",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition-1",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Intuition",
    "text": "Multilevel models: Intuition\n\n\nCode\nbigger_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 20) |&gt;\n  ungroup()\nmultilevel_big &lt;- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel_big)[1], \n    slope = fixef(multilevel_big)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-performance",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-performance",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Performance",
    "text": "Multilevel models: Performance\n\n\nCode\nmany_sample_estimates_multilevel &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel &lt;- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate &lt;- mutilevel_predicted |&gt;\n    summarize(mutilevel_estimate = mean(predicted_salary)) |&gt;\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-in-math",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-in-math",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: In math",
    "text": "Multilevel models: In math\n\nPerson-level model. For player \\(i\\) on team \\(t\\),\n\\[\\begin{equation}\nY_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\n\\end{equation}\\]\n\n\nGroup-level model. There is a model for \\(\\mu_t\\), mean salary on team \\(t\\).\n\\[\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)\\] where \\(X_t\\) is team past record"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-1",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-1",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\nOften taught in data science classes\n\nmany coefficients\npenalize large coefficients\nminimize a loss function"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-2",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-2",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\]\nMinimize a loss function:\n\\[\n\\underbrace{\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t\\gamma_t^2}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-code",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-code",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Code",
    "text": "Ridge regression: Code\n\nlibrary(glmnet)\n\n\nridge &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-estimates",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-estimates",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Estimates",
    "text": "Ridge regression: Estimates\n\n\nCode\np_ridge &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_ridge"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage",
    "text": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  bind_cols(fitted) |&gt;\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |&gt;\n  pivot_longer(cols = contains('Lambda')) |&gt;\n  distinct() |&gt;\n  mutate(name = fct_rev(name)) |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage-1",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage-1",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage",
    "text": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))\n\n\n\n\n\n\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\nPerformance over repeated samples\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter \\(\\lambda\\) rises. The biase of the estimates also increases as the value of \\(\\lambda\\) increases. The optimal value of \\(\\lambda\\) is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\nCode\nmany_sample_estimates_ridge &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-1",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-1",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\nmultilevel &lt;- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n\n\n\nCode\nyhat_multilevel &lt;- baseball_sample |&gt;\n  mutate(yhat_multilevel = predict(multilevel)) |&gt;\n  distinct(team, yhat_multilevel)\nlambda_equivalent &lt;- as_tibble(VarCorr(multilevel)) |&gt;\n  select(grp, vcov) |&gt;\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |&gt;\n  mutate(lambda_equivalent = Residual / team) |&gt;\n  pull(lambda_equivalent)\n\n\nI will also fit ridge regression with \\(\\lambda\\) set to 9.\n\n\nCode\nX &lt;- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered &lt;- baseball_sample$salary - mean(baseball_sample$salary)\nbeta_ridge &lt;- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\nyhat_ridge &lt;- beta_ridge + mean(baseball_sample$salary)\n\n\nFinally, we create a tibble with both the ridge regression and multilevel model estimates.\n\n\nCode\nboth_estimators &lt;- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |&gt;\n  rename(team = rowname) |&gt;\n  mutate(team = str_remove(team,\"team\")) |&gt;\n  left_join(yhat_multilevel, by = join_by(team))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-2",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-2",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\n\nCode\np &lt;- both_estimators |&gt;\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  geom_point(alpha = 0) +\n  theme(text = element_text(size = 18))\np"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-3",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-3",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\n\nCode\np + \n  geom_point()"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-4",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-4",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\nMultilevel model:\n\\[\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n\\] Estimates of \\(\\mu_t\\) given \\(\\hat\\mu_0\\), \\(\\hat\\tau^2\\), and \\(\\hat\\sigma^2\\):\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-5",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-5",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n\\]\nRearranged to:\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-6",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-6",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\]\n\nResult:\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-7",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-7",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\nMultilevel:\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\]\nRidge:\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-regression",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-regression",
    "title": "Algorithms for prediction",
    "section": "LASSO regression",
    "text": "LASSO regression\nOutcome model similar to ridge:\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\]\nLoss function penalty has absolute value:\n\\[\n\\underbrace{\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t\\lvert\\gamma_t\\rvert}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-in-code",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-in-code",
    "title": "Algorithms for prediction",
    "section": "LASSO: In code",
    "text": "LASSO: In code\n\nlibrary(glmnet)\n\n\nlasso &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates\n\n\nCode\np_lasso &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_lasso"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates-1",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates-1",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates-2",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates-2",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates\n\n\nCode\nfitted &lt;- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-performance",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-performance",
    "title": "Algorithms for prediction",
    "section": "LASSO: Performance",
    "text": "LASSO: Performance\n\n\nCode\nmany_sample_estimates_lasso &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap",
    "href": "slides/lec2/penalized_regression_slides.html#recap",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\nA recipe for prediction:\n\nmodel in learning data\npredict in target population\naverage"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap-1",
    "href": "slides/lec2/penalized_regression_slides.html#recap-1",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\nPerformance (expected squared error) can suffer from two sources:\n\nbias (model approximation error)\nvariance (too many parameters)\n\n\nRegularization balances these two concerns"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap-2",
    "href": "slides/lec2/penalized_regression_slides.html#recap-2",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\n\nOLS: No regularization\nRidge regression: Penalizes sum of squared coefficients\nLASSO regression: Penalizes sum of absolute coefficients\n\n\nMultilevel models regularize group-specific estimates depending on the within-group precision, a special case of ridge regression."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nRidge regression: Applies well to many settings with a large number of parameters, regardless of whether they are groups.\n\\[\n\\hat{\\vec\\beta}^\\text{Ridge} = \\underset{\\vec\\beta}{\\text{arg min}} \\underbrace{\\sum_i \\left(Y_i - \\left(\\alpha + \\vec{X}'\\vec\\beta\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_p \\beta_p^2}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-1",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-1",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nLASSO regression: Applies well to many settings with a large number of parameters, regardless of whether they are groups.\n\\[\n\\hat{\\vec\\beta}^\\text{Ridge} = \\underset{\\vec\\beta}{\\text{arg min}} \\underbrace{\\sum_i \\left(Y_i - \\left(\\alpha + \\vec{X}'\\vec\\beta\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_p \\lvert\\beta_p\\rvert}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-2",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-2",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nMultilevel models can have group-specific slopes\n\\[\n\\begin{aligned}\nY_{gi} &\\sim \\text{Normal}\\left(\\alpha_g + \\beta_g X_{gi}, \\sigma^2\\right) \\\\\n\\alpha_g &\\sim \\text{Normal}\\left(\\eta_0, \\tau^2\\right) \\\\\n\\beta_g &\\sim \\text{Normal}\\left(\\lambda_0, \\delta^2\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-3",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-3",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nMultilevel models can have more than two levels\n\\[\n\\begin{aligned}\nY_{ijk} &\\sim \\text{Normal}\\left(\\alpha_{ij}, \\sigma^2\\right) \\\\\n\\alpha_{ij} &\\sim \\text{Normal}\\left(\\beta_{i}, \\tau^2\\right) \\\\\n\\beta_{i} &\\sim \\text{Normal}\\left(\\lambda, \\delta^2\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#reading-optional",
    "href": "slides/lec2/penalized_regression_slides.html#reading-optional",
    "title": "Algorithms for prediction",
    "section": "Reading (optional)",
    "text": "Reading (optional)\nMultilevel models\n\nGelman & Hill 2006 for worked examples\nMurphy 2012 ch 5.6.2 for a succinct mathematical presentation"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#reading-optional-1",
    "href": "slides/lec2/penalized_regression_slides.html#reading-optional-1",
    "title": "Algorithms for prediction",
    "section": "Reading (optional)",
    "text": "Reading (optional)\nRidge & LASSO: Hastie, Tibshirani, & Friedman 2017 Ch 3.4\nRidge regression\n\nEfron & Hastie 2016 Ch 7.3 (all of ch 7 is useful)\nMurphy 2012 ch 7.5 for a Bayes view with lots of math\n\nLASSO regression\n\nEfron & Hastie 2016 Ch 16"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connection-to-project",
    "href": "slides/lec2/penalized_regression_slides.html#connection-to-project",
    "title": "Algorithms for prediction",
    "section": "Connection to project",
    "text": "Connection to project\nIn your project, do you have a variable that creates\n\nmany groups with\nfew observations per group?\n\nWhere else do you have many parameters to estimate?\nHow could penalized regression apply in your project?"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-2",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-2",
    "title": "Algorithms for prediction",
    "section": "Problem Set 2",
    "text": "Problem Set 2\nDue Monday 9pm: link"
  }
]