[
  {
    "objectID": "algorithms_for_prediction.html",
    "href": "algorithms_for_prediction.html",
    "title": "Algorithms for prediction",
    "section": "",
    "text": "For two sessions, we will cover a few algorithms for prediction. We aim to\nThis page contains embedded R code. If you are a Stata user, you can download do files that illustrate Ordinary Least Squares, multilevel models, ridge regression, LASSO regression, and trees and forests.\nA version of this page in slide format is available here:\nHere is a PDF of this page for those taking notes by hand.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#sec-baseball",
    "href": "algorithms_for_prediction.html#sec-baseball",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\nWe will explore algorithms for prediction through a simple example dataset. The data contain the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today. After scraping the data and selecting a few variables, I appended each team’s win-loss record from 2022. The data are available in baseball_population.csv.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n\n\nbaseball_population &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nThe first rows of the data are depicted below. Each row is a player. The player Madison Bumgarner had a salary of $21,882,892. His position was LHP for left-handed pitcher. His team was Arizona, and this team’s record in the previous season was 0.457, meaning that they won 45.7% of their games.\n\n\n# A tibble: 944 × 6\n  player               salary position team    team_past_record team_past_salary\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457         2794887.\n2 Marte, Ketel       11600000 2B       Arizona            0.457         2794887.\n3 Ahmed, Nick        10375000 SS       Arizona            0.457         2794887.\n# ℹ 941 more rows\n\n\nWe will summarize mean salaries. Some useful facts about mean salaries are that they vary substantially across positions and also across teams.\n\n\nCode\nbaseball_population |&gt;\n  group_by(position) |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  mutate(position = fct_reorder(position, -salary)) |&gt;\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", size = 3, fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme(axis.text.x = element_text(size = 7)) +\n  ggtitle(\"Baseball salaries vary across positions\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbaseball_population |&gt;\n  group_by(team) |&gt;\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |&gt;\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Baseball salaries vary across teams\")",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "href": "algorithms_for_prediction.html#task-predict-the-dodgers-mean-salary",
    "title": "Algorithms for prediction",
    "section": "Task: Predict the Dodgers’ mean salary",
    "text": "Task: Predict the Dodgers’ mean salary\nAs a task, we will often focus on estimating the mean salary of the L.A. Dodgers. Because we have the full population of data, we can calculate the answer directly:\n\ntrue_dodger_mean &lt;- baseball_population |&gt;\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |&gt;\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |&gt;\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196. We will imagine that we don’t know this number. Instead of having the full population, we will imagine we have\n\ninformation on predictors for all players: position, team, team past record\ninformation on salary for a random sample of 5 players per team\n\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nIn our sample, we observe the salaries of 5 players per team. Our 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n# A tibble: 5 × 6\n  player           salary position team        team_past_record team_past_salary\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Phillips, Evan  1300000 RHP      L.A. Dodge…            0.685         8388736.\n2 Miller, Shelby  1500000 RHP      L.A. Dodge…            0.685         8388736.\n3 Taylor, Chris  15000000 OF       L.A. Dodge…            0.685         8388736.\n4 Betts, Mookie  21158692 OF       L.A. Dodge…            0.685         8388736.\n5 Outman, James    722500 OF       L.A. Dodge…            0.685         8388736.\n\n\nOur task is to predict the salary for all 35 Dodger players and average those predictions to yield a predicted vaue of the Dodgers’ mean salary on opening day 2023. The data we will use to predict include all variables except the outcome for the Dodger players.\n\ndodgers_to_predict &lt;- baseball_population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  select(-salary)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ordinary-least-squares",
    "href": "algorithms_for_prediction.html#ordinary-least-squares",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nTo walk through the steps of our prediction task, we first consider Ordinary Least Squares. After walking through these steps, we will consider a series of more advanced algorithms for prediction that involve similar steps.\nFor OLS, we might model salary as a function of team_past_record. The code below learns this model in our sample.\n\nols &lt;- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)\n\nWe can then make a prediction for every player on the Dodgers. Because our only predictor is a team-level predictor (team_past_record), the prediction will be the same for every player. But this may not always be the case, as further down the page when we consider position as an additional predictor.\n\nols_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |&gt;\n  print(n = 3)\n\n# A tibble: 35 × 6\n  player       position team  team_past_record team_past_salary predicted_salary\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Freeman, Fr… 1B       L.A.…            0.685         8388736.         5552999.\n2 Heyward, Ja… OF       L.A.…            0.685         8388736.         5552999.\n3 Betts, Mook… OF       L.A.…            0.685         8388736.         5552999.\n# ℹ 32 more rows\n\n\nFinally, we can average over these predictions to estimate the mean salary on the Dodgers.\n\nols_estimate &lt;- ols_predicted |&gt;\n  summarize(ols_estimate = mean(predicted_salary))\n\nBy OLS prediction, we estimate that the mean Dodger salary was $5.6 million. Because we estimated in a sample and under some modeling assumptions, this is a bit lower than the true population mean of $6.2 million.\n\nPerformance over repeated samples\nBecause this is a hypothetical setting, we can consider the performance of our estimator across repeated samples. The chunk below pulls our code into a single function that we call estimator(). The estimator takes a sample and returns an estimate.\n\nols_estimator &lt;- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}\n\nWe can run the estimator repeatedly, getting one estimate for each repeated sample from the population. This exercise is possible because in this simplified setting we have data on the full population.\n\nmany_sample_estimates &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample\n  estimate &lt;- ols_estimator(baseball_sample)\n  return(estimate)\n}\n\nThen we can visualize the performance across repeated samples.\n\n\nCode\ntibble(y = many_sample_estimates) |&gt;\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |&gt;\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nAcross repeated samples, the estimates have a standard deviation of $1.3 million and are on average $1.2 million too high.\n\n\nModel approximation error\nAn OLS model for salary that is linear in the team past record clearly suffers from model approximation error. If you fit a regression line to the entire population of baseball players you would see that th Dodger’s mean salary is below this line.\n\n\nCode\npopulation_ols &lt;- lm(salary ~ team_past_record, data = baseball_population)\nforplot &lt;- baseball_population |&gt;\n  mutate(fitted = predict(population_ols)) |&gt;\n  group_by(team) |&gt;\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers &lt;- forplot |&gt; filter(team == \"L.A. Dodgers\")\nforplot |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHow can we solve model approximation error? One might replace the linear term team_past_record with a series of categories for team in the OLS model.\n\nols_team_categories &lt;- lm(\n  salary ~ team,\n  data = baseball_sample\n)\n\nBut because the sample contains only 5 players per team, these estimates are quite noisy.\n\n\nCode\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\nWe would like to make the model more flexible to reduce model approximation error, while also avoiding high variance. To balance these competing objectives, we need a new strategy from machine learning.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#a-big-idea-regularization",
    "href": "algorithms_for_prediction.html#a-big-idea-regularization",
    "title": "Algorithms for prediction",
    "section": "A big idea: Regularization",
    "text": "A big idea: Regularization\nRegularization encompasses a broad class of approaches designed for models that have many parameters, such as a unique intercept for every team in Major League Baseball. Regularization allows us to estimate many parameters while pulling them toward some common value in order to reduce the high variance that tends to results.\nAs one concrete example of regularization, we might want a middle ground between two extremes:\n\nour OLS linear prediction: \\(\\hat{Y}^\\text{Linear}_\\text{Dodgers} = \\hat\\alpha + \\hat\\beta\\text{(Past Record)}_\\text{Dodgers}={}\\) $3.9 million\nthe mean of the 5 sampled Dodger salaries: \\(\\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} = {}\\) $7.9 million\n\nA regularized estimator could be a weighted average of these two, with weight \\(w\\) placed on the nonparametric mean and weight \\(1 - w\\) placed on the linear prediction.\n\\[\n\\hat\\tau = w \\hat{Y}^\\text{Nonparametric}_\\text{Dodgers} + (1 - w)\\hat{Y}^\\text{Linear}_\\text{Dodgers}\n\\] The graph below visualizes the resulting estimate at various values of \\(w\\). The regularized estimates are partially pooled toward the linear model prediction, with the amount of pooling controlled by \\(w\\).\n\n\nCode\nestimates_by_w &lt;- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |&gt;\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Sample Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPartial pooling is one way to navigate the bias-variance tradeoff: it allows us to have a flexible model while reducing the high amount of variance that such a model can create. In our case, the best estimator lies somewhere between the fully-pooled estimator (linear regression) and the fully-separate estimator (unique intercepts for each team).\nOne way to formally investigate the properties of our estimator is by defining a concept known as expected squared error. Let \\(\\hat\\mu_\\text{Dodgers}\\) be an estimator: a function that when applied to a sample \\(S\\) returns an estimate \\(\\hat\\mu_\\text{Dodgers}(S)\\). The squared error of this estimator in a particular sample \\(S\\) is \\((\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers})^2\\). Expected squared error is the expected value of this performance taken across repeated samples \\(S\\) from the population.\n\\[\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n\\]\nOrdinarily, one has only one sample and cannot directly calculate expected squared error. But our setting is useful for pedagogical purposes because we have the full population of baseball players and can repeatedly draw samples to evaluate performance. Below, we simulate \\(r = 1,\\dots,100\\) repeated samples and estimated expected squared error by the mean squared error of the estimates \\(\\hat\\mu_\\text{Dodgers}(S_r)\\) that we get from each simulated sample \\(S_r\\).\n\\[\n\\widehat{\\text{Expected Squared Error}}(\\hat\\mu_\\text{Dodgers}) = \\frac{1}{100}\\sum_{r=1}^{100}\\left(\\hat\\mu_\\text{Dodgers}(S_r) - \\mu_\\text{Dodgers}\\right)^2\n\\]\n\n\nCode\nrepeated_simulations &lt;- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  \n  ols_fit &lt;- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate &lt;- predict(\n    ols_fit, \n    newdata = baseball_population |&gt; \n      filter(team == \"L.A. Dodgers\") |&gt;\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate &lt;- a_sample |&gt;\n    filter(team == \"L.A. Dodgers\") |&gt;\n    summarize(salary = mean(salary)) |&gt;\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated &lt;- repeated_simulations |&gt;\n  group_by(w) |&gt;\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |&gt;\n  mutate(best = mse == min(mse))\n\naggregated |&gt;\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )\n\n\n\n\n\n\n\n\n\nIn this illustration, the best performance is an estimator that puts 75% of the weight on the linear fit and 25% of the weight on the mean among the 5 sampled Dodgers.\nThe example above illustrates an idea known as regularization, shrinkage, or partial pooling: we may often want to combine an estimate on a subgroup (the Dodger mean) with an estimate made on the full population (the linear fit). We will consider various methods to accomplish regularization, and we will return at the end to consider connections among them.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#multilevel-models",
    "href": "algorithms_for_prediction.html#multilevel-models",
    "title": "Algorithms for prediction",
    "section": "Multilevel models",
    "text": "Multilevel models\nMultilevel models1 are an algorithm for prediction that is fully grounded in classical statistics. They are especially powerful for the problem depicted above: making predictions when there are many groups (teams) with a small sample size in each group.\nWe will first illustrate a multilevel model’s performance and then consider the statistics behind this model. We will estimate using the lme4 package. If you don’t have this package, install it with install.packages(\"lme4\").\n\nlibrary(lme4)\n\nIn the syntax, the code (1 | team) says that our model should have a unique intercept for every team, and that these intercepts should be regularized (more on this soon).\n\nmultilevel &lt;- lmer(salary ~ team_past_record + (1 | team), data = baseball_sample)\n\nWe can make predictions from a multilevel model just like we can from OLS. For example, the code below makes predictions for the Dodgers.\n\nmultilevel_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )\n\n\nIntuition\nThe multilevel model is a partial-pooling estimator. The figure below displays this visually. For each team, the solid dot is the mean salary among the 5 sampled players. The ends of the arrows are the multilevel model estimates. The multilevel model pools the team-specific estimates toward the model-based prediction.\n\n\nCode\np &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np\n\n\n\n\n\n\n\n\n\nThe multilevel model only regularizes the team-specific estimates to the degree that they are imprecise. If we repeat the entire process on a sample of 20 players per team, each team-specific estimate becomes more precise and the overall amount of shrinkage is less.\n\n\nCode\nbigger_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 20) |&gt;\n  ungroup()\nmultilevel_big &lt;- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\n\n\n\n\n\n\n\n\n\n\n\nPerformance over repeated samples\nWe previously discussed how an OLS prediction that was linear in the past team record was a biased estimator with low variance. The sample mean within each team was an unbiased estimator with high variance. The multilevel model falls in between these two extremes.\n\n\nCode\nmany_sample_estimates_multilevel &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel &lt;- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate &lt;- mutilevel_predicted |&gt;\n    summarize(mutilevel_estimate = mean(predicted_salary)) |&gt;\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nIn math\nMathematically, a multilevel model is a maximum likelihood estimator. For our case, the model assumes that the salary of player \\(i\\) on team \\(t\\) is assumed to be normally distributed around the team mean salary \\(\\mu_t\\), with variance \\(\\sigma^2\\) which in our case is assumed to be the same across teams.\n\\[Y_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\\] The team-specific mean \\(\\mu_t\\) involves two components. First, this mean is assumed to be centered at a linear prediction \\(\\alpha + X_t\\beta\\) where \\(X_t\\) is the win-loss record of team \\(t\\) in the previous year. This is the value toward which team-specific estimates are regularized. Second, the mean for the particular team \\(i\\) is drawn from a normal distribution with standard deviation \\(\\tau^2\\), which is the standard deviation of the team-specific mean salary residuals across teams.\n\\[\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)\\] By maximizing the log likelihood of the observed data under this model, one comes to maximum likelihood estimates of all of the unknown parameters. The \\(\\mu_t\\) estimates will partially pool between two estimators,\n\nthe sample mean \\(\\bar{Y}_t\\) within team \\(t\\)\nthe linear model prediction \\(\\hat\\alpha + X_t\\hat\\beta\\)\n\nwhere the weight on (1) will depend on the relative precision of this within-team estimate and the weight (2) will depend on the relative precision of the between-team estimate. After explaining ridge regression, we will return to a simplified case where the formula for the multilevel model estimates allows further intuition building.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#ridge-regression",
    "href": "algorithms_for_prediction.html#ridge-regression",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\nWhile multilevel models are often approached from the standpoint of classical statistics, they are very similar to another approach commonly approached from the standpoint of data science: ridge regression.\n\nIntuition\nConsider our sample of 30 baseball teams with 5 players per team. We might want to fit a linear regression model as follows,\n\\[\nY_{ij} = \\alpha + \\beta X_i + \\gamma_{i} + \\epsilon_{ij}\n\\] where \\(Y_{ij}\\) is the salary of player \\(i\\) on team \\(j\\) and \\(X_i\\) is the past win-loss record of that team. In this model, \\(\\gamma_i\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_ij\\) is the deviation for player \\(j\\) from their own team’s average salary.\nThe problem with this model is its high variance: with 30 teams, there are 30 different values of \\(\\gamma_i\\) to be estimated. And there are only 5 players per team! We might believe that \\(\\gamma_i\\) values will generally be small, so we might want to estimate by penalizing large values of \\(\\gamma_i\\).\nWe can penalize large values of \\(\\gamma_i\\) by an estimator written as it might be written in a data science course:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_i\\sum_j\\left(Y_{ij} - \\left(\\alpha + \\beta X_i + \\gamma_{i}\\right)\\right)^2 + \\lambda\\sum_i\\gamma_i^2\\right]\n\\]\nwhere the estimated values \\(\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\}\\) are chosen to minimize a loss function which is the sum over the data of the squared prediction error plus a penalty on large values of \\(\\gamma_i\\).\n\n\nIn code\n\nlibrary(glmnet)\n\n\nridge &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)\n\nWe can visualize the ridge regression estimates just like the multilevel model estimates. Because the penalty applies to squared values of team deviations from the line, the points furthest from the line are most strongly regularized toward the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nThe amount of regularization will depend on the chosen value of the penalty parameter \\(\\lambda\\). To the degree that \\(\\lambda\\) is large, estimates will be more strongly pulled toward the regression line. Below is a visualization at three values of \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  bind_cols(fitted) |&gt;\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |&gt;\n  pivot_longer(cols = contains('Lambda')) |&gt;\n  distinct() |&gt;\n  mutate(name = fct_rev(name)) |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\).\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter \\(\\lambda\\) rises. The biase of the estimates also increases as the value of \\(\\lambda\\) increases. The optimal value of \\(\\lambda\\) is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\nCode\nmany_sample_estimates_ridge &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)\n\n\n\n\n\n\n\n\n\n\n\nConnections: Multilevel model and ridge regression\nMultilevel models and ridge regression are closely connected, and they can yield mathematically equivalent estimates in special cases. Consider again the multilevel model for the salary \\(Y_{ti}\\) of player \\(i\\) on team \\(t\\).\n\\[\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n\\] For simplicity, suppose we already have estimates \\(\\hat\\mu_0\\), \\(\\hat\\tau^2\\), and \\(\\hat\\sigma^2\\). One can show that the multilevel model estimates of \\(\\mu_t\\) are:\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\] This formula can be interpreted as analogous to a sample mean, but with some added observations. The first part of the numerator and denominator corresponds to a sample mean: the sum \\(\\sum_i Y_{ti}\\) of salaries of all sampled players in team \\(t\\) in the numerator and the number of such players \\(n_t\\) in the denominator. The right side of the numerator and denominator correspond to pseudo-observations. It is as though in addition to the sampled players, we also saw \\(\\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\) additional players with exactly the overall baseball mean salary estimate \\(\\hat\\mu_0\\). This part of the formula partially pools the team-specific mean toward this overall mean. Partial pooling is greater to the degree that there is small within-team variance (small \\(\\sigma^2\\)) or large across-team variance (large \\(\\hat\\tau^2\\)).\nNext, we consider a ridge regression estimator that minimizes an objective function where \\(\\hat\\mu_0\\) is an unpenalized estimate for the baseball-wide mean salary.2\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n\\] We can gain some additional intuition for this estimate by rearranging to pull the penalty into the main term.\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\] The first part of this term is the sum over observed squared errors within team \\(t\\), \\(\\sum_i\\left(Y_{it}-\\mu_t\\right)^2\\). The second part is as though we had observed an additional \\(\\lambda\\) cases within team \\(t\\) with an outcome value \\(\\mu_0\\) equal to the baseball-wide mean. With this intuition, the ridge regression estimator becomes\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\] which is the same as the multilevel model estimator in the special case when \\(\\lambda = \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\). Multilevel models and ridge regression are actually accomplishing the same thing!\nBelow, we use code to check that these two give the same thing. We first fit a multilevel model,\n\nmultilevel &lt;- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n\nThen we make predictions,\n\nyhat_multilevel &lt;- baseball_sample |&gt;\n  mutate(yhat_multilevel = predict(multilevel)) |&gt;\n  distinct(team, yhat_multilevel)\n\nand extract the implied value of \\(\\lambda\\) for an equivalent ridge regression (from the math above).\n\nlambda_equivalent &lt;- as_tibble(VarCorr(multilevel)) |&gt;\n  select(grp, vcov) |&gt;\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |&gt;\n  mutate(lambda_equivalent = Residual / team) |&gt;\n  pull(lambda_equivalent)\n\nNow we estimate ridge regression with that \\(\\lambda\\) value. Because glmnet internally rescales variables, it is difficult to carry out this check with glmnet. Instead, we will write our own ridge regression estimator. We first define our predictor matrix X and a mean-centered outcome vector y_centered. The reason to mean-center the outcome is to allow an unpenalized grand mean (\\(\\mu_0\\) in the math above). We will add this value back to predictions later.\n\nX &lt;- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered &lt;- baseball_sample$salary - mean(baseball_sample$salary)\n\nThe ridge regression estimator can be written in matrix form as follows:\n\\[\nY - \\hat\\mu_0 = \\mathbf{X}\\vec\\beta + \\epsilon\n\\] with \\(\\hat{\\vec\\beta}_\\text{Ridge} = (\\mathbf{X}'\\mathbf{X} + \\text{diag}(\\lambda))^{-1}\\mathbf{X}'\\vec{Y}\\). The code below estimates \\(\\vec\\beta\\)\n\nbeta_ridge &lt;- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\n\nBecause there is one \\(\\beta\\) value for each team, we convert to predicted values for each team by adding the grand mean to the estimated coefficients.\n\nyhat_ridge &lt;- beta_ridge + mean(baseball_sample$salary)\n\nFinally, we create a tibble with both the ridge regression and multilevel model estimates.\n\nboth_estimators &lt;- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |&gt;\n  rename(team = rowname) |&gt;\n  mutate(team = str_remove(team,\"team\")) |&gt;\n  left_join(yhat_multilevel, by = join_by(team))\n\nWe can confirm in code that the two predictions are numerically equal.\n\nboth_estimators |&gt;\n  # remove names; it is ok if names are unequal\n  mutate_all(unname) |&gt;\n  # summarize whether the two columns are equal\n  summarize(numerically_equal = all.equal(yhat_ridge, yhat_multilevel))\n\n# A tibble: 1 × 1\n  numerically_equal\n  &lt;lgl&gt;            \n1 TRUE             \n\n\nWe produce a plot with one point for each team, with ridge regression predictions on the \\(x\\)-axis and multilevel model predictions on the \\(y\\)-axis. We can see that these two estimators are equivalent.\n\n\nCode\nboth_estimators |&gt;\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_point() +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  ggtitle(\"Ridge regression and multilevel models can yield\\nequal estimates for the mean salary on each team\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe equivalency of ridge regression and multilevel models may be surprising. Ridge regression is often motivated from a loss function (squared error + penalty), using terms that are common in data science and machine learning. Multilevel models are often motivated from sociological examples where units are clustered in groups, with terminology more common in statistics. Yet the two are mathematically related. An important difference is that multilevel models learn the amount of regularization from the data, whereas ridge regression needs an additional step to learn the penalty parameter (to be discussed in a future class session on data-driven estimator selection).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#lasso-regression",
    "href": "algorithms_for_prediction.html#lasso-regression",
    "title": "Algorithms for prediction",
    "section": "LASSO regression",
    "text": "LASSO regression\nLASSO regression is just like ridge regression except for one key difference: instead of penalizing the sum of squared coefficients, LASSO penalizes the sum of the absolute value of coefficients. As we will see, this change means that some parameters become regularized all the way to zero so that some terms drop out of the model completely.\nAs with ridge regression, our outcome model is\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\] where \\(Y_{ti}\\) is the salary of player \\(i\\) on team \\(t\\) and \\(X_t\\) is the past win-loss record of that team. In this model, \\(\\gamma_t\\) is a team-specific deviation from the linear fit, which corrects for model approximation error that will arise if particular teams have average salaries not well-captured by the linear fit. The error term \\(\\epsilon_{ti}\\) is the deviation for player \\(i\\) from their own team’s average salary.\nTo solve the high-variance estimates of \\(\\gamma_t\\), LASSO regression uses a different penalty term in its loss function:\n\\[\n\\{\\hat\\alpha, \\hat\\beta, \\hat{\\vec\\gamma}\\} = \\underset{\\alpha,\\beta,\\vec\\gamma}{\\text{arg min}} \\left[\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2 + \\lambda\\sum_t\\lvert\\gamma_t\\rvert\\right]\n\\]\nwhere \\(\\gamma_t^2\\) from the ridge regression penalty has been replaced by the absolute value \\(\\lvert\\gamma_t\\rvert\\). We will first apply this in code and then see how it changes the performance of the estimator.\n\nIn code\n\nlibrary(glmnet)\n\n\nlasso &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)\n\nVisualizing the estimates, we see behavior similar to what we have previously seen from multilevel models and ridge regression. All estimates are pulled toward a linear regression fit. There are two key differences, however.\nFirst, the previous estimators most strongly pulled the large deviations toward the linear fit. The LASSO estimates pull all parameter estimates toward the linear fit to a similar degree, regardless of their size. This is because the LASSO estimates penalize the absolute value of \\(\\gamma_i\\) instead of the squared value of \\(\\gamma_i\\).\nSecond, the multilevel and ridge estimates never pulled any of the estimates all the way to the line; instead, the estimates asymptoted toward the line as the penalty parameter grew. In LASSO, some estimates are pulled all the way to the line.\n\n\nCode\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\n\n\n\n\n\n\n\n\n\nFocusing on the prediction for the Dodgers, we can see how the estimate changes as a function of the penalty parameter \\(\\lambda\\). At a very small penalty, the estimate is approximately the same as the mean among the 5 sampled Dodger players. As the penalty parameter \\(\\lambda\\) gets larger, the estimates move around. They generally move toward zero, but not always: as some other team-specific deviations are pulled to zero, the unregularized intercept and slope on team past record move around in response. Ultimately, the penalty becomes so large that the Dodger estimate is regularized all the way to the estimate we would get in a model with no team-specific deviations.\n\n\nCode\nfitted &lt;- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs with ridge regression, we will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\n\n\nPerformance over repeated samples\nSimilar to the ridge regression estimator, we can visualize the performance of the LASSO regression estimator across repeated samples from the population.\n\n\nCode\nmany_sample_estimates_lasso &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#trees",
    "href": "algorithms_for_prediction.html#trees",
    "title": "Algorithms for prediction",
    "section": "Trees",
    "text": "Trees\n\nTo read more on this topic, see Ch 8.4 of Efron & Hastie (2016)\n\nPenalized regression performs well when the the response surface \\(E(Y\\mid\\vec{X})\\) is well-approximated by the functional form of a particular assumed model. In some settings, however, the response surface may be more complex, with nonlinearities and interaction terms that the researcher may not know about in advance. In these settings, one might desire an estimator that adaptively learns the functional form from the data.\n\nA simulation to illustrate trees\nAs an example, the figure below presents some hypothetical data with a binary predictor \\(Z\\), a numeric predictor \\(X\\), and a numeric outcome \\(Y\\).\n\n\nCode\ntrue_conditional_mean &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\np_no_points &lt;- true_conditional_mean |&gt;\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\np &lt;- p_no_points +\n  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)\np\n\n\n\n\n\n\n\n\n\nIf we tried to approximate these conditional means with an additive linear model, \\[\\hat{E}_\\text{Linear}(Y\\mid X,Z) = \\hat\\alpha + \\hat\\beta X + \\hat\\gamma Z\\] then the model approximation error would be very large.\n\n\nCode\nbest_linear_fit &lt;- lm(mu ~ x + z, data = true_conditional_mean)\np +\n  geom_line(\n    data = true_conditional_mean |&gt;\n      mutate(mu = predict(best_linear_fit))\n  ) +\n  theme_bw() +\n  ggtitle(\"An additive linear model (solid lines) poorly approximates\\nthe true conditional mean function (dashed lines)\")\n\n\n\n\n\n\n\n\n\n\n\nTrees repeatedly split the data\nRegression trees begin from a radically different place. With no model at all, suppose we were to split the sample into two subgroups. For example, we might choose to split on \\(Z\\) and say that all units with z = TRUE are one subgroup while all units with z = FALSE are another subgroup. Or we might split on \\(X\\) and say that all units with x &lt;= .23 are one subgroup and all units with x &gt; .23 are another subgroup. After choosing a way to split the dataset into two subgroups, we would then make a prediction rule: for each unit, predict the mean value of all sampled units who fall in their subgroup. This rule would produce only two predicted values: one prediction per resulting subgroup.\nIf you were designing an algorithm to predict this way, how would you choose to define the split?\nIn regression trees to estimate conditional means, the split is often chosen to minimize the resulting sum of squared prediction errors. Suppose we choose this rule. Suppose we consider splitting on \\(X\\) being above or below each decile of its empirical distribution. Suppose we consider splitting on \\(Z\\) being FALSE or TRUE. The graph below shows the sum of squared prediction error resulting from each rule.\n\n\nCode\nx_split_candidates &lt;- quantile(simulated_data$x, seq(.1,.9,.1))\nz_split_candidates &lt;- .5\nby_z &lt;- simulated_data |&gt;\n  group_by(z) |&gt;\n  mutate(yhat = mean(y)) |&gt;\n  ungroup() |&gt;\n  summarize(sum_squared_error = sum((yhat - y) ^ 2))\nby_x &lt;- foreach(x_split = x_split_candidates, .combine = \"rbind\") %do% {\n  simulated_data |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n}\n\nby_x |&gt;\n  mutate(split = \"If Splitting on X\") |&gt;\n  rename(split_value = x_split) |&gt;\n  bind_rows(\n    by_z |&gt;\n      mutate(split = \"If Splitting on Z\") |&gt;\n      mutate(split_value = .5)\n  ) |&gt;\n  ggplot(aes(x = split_value, y = sum_squared_error)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~split) +\n  labs(\n    x = \"Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWith the results above, we would choose to split on \\(Z\\), creating a subpopulation with \\(Z \\leq .5\\) and a subgroup with \\(Z\\geq .5\\). Our prediction function would look like this. Our split very well approximates the true conditional mean function when Z = FALSE, but is still a poor approximator when Z = TRUE.\n\n\nCode\np +\n  geom_line(\n    data = simulated_data |&gt;\n      group_by(z) |&gt;\n      mutate(mu = mean(y))\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter one split on Z\")\n\n\n\n\n\n\n\n\n\nWhat if we make a second split? A regression tree repeats the process and considers making a further split within each subpopulation. The graph below shows the sum of squared error in the each subpopulation of Z when further split at various candidate values of X.\n\n\nCode\n# Split 2: After splitting by Z, only X remains on which to split\nleft_side &lt;- simulated_data |&gt; filter(!z)\nright_side &lt;- simulated_data |&gt; filter(z)\n\nleft_split_candidates &lt;- quantile(left_side$x, seq(.1,.9,.1))\nright_split_candidates &lt;- quantile(right_side$x, seq(.1,.9,.1))\n\nleft_split_results &lt;- foreach(x_split = left_split_candidates, .combine = \"rbind\") %do% {\n  left_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nright_split_results &lt;- foreach(x_split = right_split_candidates, .combine = \"rbind\") %do% {\n  right_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nsplit2_results &lt;- left_split_results |&gt; mutate(split1 = \"Among Z = FALSE\") |&gt;\n  bind_rows(right_split_results |&gt; mutate(split1 = \"Among Z = TRUE\"))\n\nsplit2_results |&gt;\n  ggplot(aes(x = x_split, y = sum_squared_error)) +\n  geom_line(color = 'gray') +\n  geom_point(aes(color = chosen)) +\n  scale_color_manual(values = c(\"gray\",\"blue\")) +\n  facet_wrap(~split1) +\n  theme_bw() +\n  labs(\n    x = \"X Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  )\n\n\n\n\n\n\n\n\n\nThe resulting prediction function is a step function that begins to more closely approximate the truth.\n\n\nCode\nsplit2_for_graph &lt;- split2_results |&gt;\n  filter(chosen) |&gt;\n  mutate(z = as.logical(str_remove(split1,\"Among Z = \"))) |&gt;\n  select(z, x_split) |&gt;\n  right_join(simulated_data, by = join_by(z)) |&gt;\n  mutate(x_left = x &lt;= x_split) |&gt;\n  group_by(z, x_left) |&gt;\n  mutate(yhat = mean(y))\n\np +\n  geom_line(\n    data = split2_for_graph,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter two splits on (Z,X)\")\n\n\n\n\n\n\n\n\n\nHaving made one and then two splits, the figure below shows what happens when each subgroup is the created by 4 sequential splits of the data.\n\n\nCode\nlibrary(rpart)\nrpart.out &lt;- rpart(\n  y ~ x + z, data = simulated_data, \n  control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n)\np +\n  geom_step(\n    data = true_conditional_mean |&gt;\n      mutate(mu_hat = predict(rpart.out, newdata = true_conditional_mean)),\n    aes(y = mu_hat)\n  ) +\n  ggtitle(\"Prediction from regression tree grown to depth 4\")\n\n\n\n\n\n\n\n\n\nThis prediction function is called a regression tree because of how it looks when visualized a different way. One begins with a full sample which then “branches” into a left and right part, which further “branch” off in subsequent splits. The terminal nodes of the tree—subgroups defined by all prior splits—are referred to as “leaves.” Below is the prediction function from above, visualized as a tree. This visualization is made possible with the rpart.plot package which we practice further down the page.\n\n\nCode\nlibrary(rpart.plot)\nrpart.plot::rpart.plot(rpart.out)\n\n\n\n\n\n\n\n\n\n\n\nYour turn: Fit a regression tree\nUsing the rpart package, fit a regression tree like the one above. First, load the package.\n\nlibrary(rpart)\n\nThen use this code to simulate data. If you are a Stata user, download this simulated data file from the website.\n\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\n\nUse the rpart function to grow a tree.\n\nrpart.out &lt;- rpart(y ~ x + z, data = simulated_data)\n\nFinally, we can define a series of predictor values at which to make predictions,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nand then make predictions\n\npredicted &lt;- predict(rpart.out, newdata = to_predict)\n\nand visualize in a plot.\n\nto_predict |&gt;\n  mutate(yhat = predicted) |&gt;\n  ggplot(aes(x = x, y = yhat, color = z)) +\n  geom_step()\n\n\n\n\n\n\n\n\nWhen you succeed, there are a few things you can try:\n\nVisualize the tree using the rpart.plot() function applied to your rpart.out object\nAttempt a regression tree using the baseball_population.csv data\nTry different specifications of the tuning parameters. See the control argument of rpart, explained at ?rpart.control. To produce a model with depth 4, we previously used the argument control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4).\n\n\n\nChoosing the depth of the tree\nHow deep should one make a tree? Recall that the depth of the tree is the number of sequential splits that define a leaf. The figure below shows relatively shallow trees (depth = 2) and relatively deep trees (depth = 4) learned over repeated samples. What do you notice about performance with each choice?\n\n\nCode\nestimator &lt;- function(maxdepth) {\n  foreach(rep = 1:3, .combine = \"rbind\") %do% {\n    this_sample &lt;- simulate(100)\n    rpart.out &lt;- rpart(y ~ x + z, data = this_sample, control = rpart.control(minsplit = 2, cp = 0, maxdepth = maxdepth))\n    true_conditional_mean |&gt;\n      mutate(yhat = predict(rpart.out, newdata = true_conditional_mean),\n             maxdepth = maxdepth,\n             rep = rep)\n  }\n}\nresults &lt;- foreach(maxdepth_value = c(2,5), .combine = \"rbind\") %do% estimator(maxdepth = maxdepth_value)\np_no_points +\n  geom_line(\n    data = results |&gt; mutate(maxdepth = case_when(maxdepth == 2 ~ \"Shallow Trees\\nDepth = 2\", maxdepth == 5 ~ \"Deep Trees\\nDepth = 5\")),\n    aes(group = interaction(z,rep), y = yhat)\n  ) +\n  facet_wrap(\n    ~maxdepth\n  )\n\n\n\n\n\n\n\n\n\nShallow trees yield predictions that tend to be more biased because the terminal nodes are large. At the far right when z = TRUE and x is large, the predictions from the shallow trees are systematically lower than the true conditional mean.\nDeep trees yield predictions that tend to be high variance because the terminal nodes are small. While the flexibility of deep trees yields predictions that are less biased, the high variance can make deep trees poor predictors.\nThe balance between shallow and deep trees can be chosen by various rules of thumb or out-of-sample performance metrics, many of which are built into functions like rpart. Another way out is to move beyond trees to forests, which involve a simple extension that yields substantial improvements in performance.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#forests",
    "href": "algorithms_for_prediction.html#forests",
    "title": "Algorithms for prediction",
    "section": "Forests",
    "text": "Forests\n\nTo read more on this topic, see Ch 17.1 of Efron & Hastie (2016)\n\nWe saw previously that a deep tree is a highly flexible learner, but one that may have poor predictive performance due to its high sampling variance. Random forests (Breiman 2001) resolve this problem in a simple but powerful way: reduce the variance by averaging the predictions from many trees. The forest is the average of the trees.\nIf one simply estimated a regression tree many times on the same data, every tree would be the same. Instead, each time a random forest grows a tree it proceeds by:\n\nbootstrap a sample \\(n\\) of the \\(n\\) observations chosen with replacement\nrandomly sample some number \\(m\\) of the variables to consider for splitting\n\nThere is an art to selection of the tuning parameter \\(m\\), as well as the parameters of the tree-growing algorithm. But most packages can select these tuning parameters automatically. The more trees you grow, the less the forest-based predictions will be sensitive to the stochastic variability that comes from the random sampling of data for each tree.\n\nIllustration with bagged forest\nFor illustration, we will first consider a simple version of random forest that is a bagging estimator: all predictors are included in every tree and variance is created through bagging, or bootstrap aggregating. The code below builds intuition, and the code later using the regression_forest function from the grf package is one way we would actually recommend learning a forest in practice.\n\ntree_estimates &lt;- foreach(tree_index = 1:100, .combine = \"rbind\") %do% {\n  # Draw a bootstrap sample of the data\n  simulated_data_star &lt;- simulated_data |&gt;\n    slice_sample(prop = 1, replace = T)\n  # Learn the tree\n  rpart.out &lt;- rpart(\n    y ~ x + z, data = simulated_data_star, \n    # Set tuning parameters to grow a deep tree\n    control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n  )\n  # Define data to predict\n  to_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n  # Make predictions\n  predicted &lt;- to_predict |&gt;\n    mutate(\n      yhat = predict(rpart.out, newdata = to_predict),\n      tree_index = tree_index\n    )\n  return(predicted)\n}\n\nWe can then aggregate the tree estimates into a forest prediction by averaging over trees.\n\nforest_estimate &lt;- tree_estimates |&gt;\n  group_by(z,x) |&gt;\n  summarize(yhat = mean(yhat), .groups = \"drop\")\n\nThe forest is very good at approximating the true conditional mean.\n\n\nCode\np_no_points +\n  geom_line(\n    data = forest_estimate,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are forest predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\n\n\nYour turn: A random forest with grf\nIn practice, it is helpful to work with a function that can choose the tuning parameters of the forest for you. One such function is the regression_forest() function in the grf package.\n\nlibrary(grf)\n\nTo illustrate its use, we first produce a matrix X of predictors and a vector Y of outcome values.\n\nX &lt;- model.matrix(~ x + z, data = simulated_data)\nY &lt;- simulated_data |&gt; pull(y)\n\nWe then estimate the forest with the regression_forest() function, here using the tune.parameters = \"all\" argument to allow automated tuning of all parameters.\n\nforest &lt;- regression_forest(\n  X = X, Y = Y, tune.parameters = \"all\"\n)\n\nWe can extract one tree from the forest with the get_tree() function and then visualize with the plot() function.\n\nfirst_tree &lt;- get_tree(forest, index = 1)\nplot(first_tree)\n\nTo predict in a new dataset requires a new X matrix,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict &lt;- model.matrix(~ x + z, data = to_predict)\n\nwhich can then be used to make predictions.\n\nforest_predicted &lt;- to_predict |&gt;\n  mutate(\n    yhat = predict(forest, newdata = X_to_predict) |&gt; \n      pull(predictions)\n  )\n\nWhen we visualize, we see that the forest from the package is also a good approximator of the conditional mean function. It is possible that the bias of this estimated forest arises from tuning parameters that did not grow sufficiently deep trees.\n\n\nCode\np_no_points +\n  geom_line(\n    data = forest_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are grf::regression_forest() predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\nOnce you have learned a forest yourself, you might try a regression forest using the baseball_population.csv data or another dataset of your choosing.\n\n\nForests as adaptive nearest neighbors\nA regression tree can be interpreted as an adaptive nearest-neighbor estimator: the prediction at predictor value \\(\\vec{x}\\) is the average outcome of all its neighbors, where neighbors are defined as all sampled data points that fall in the same leaf as \\(\\vec{x}\\). The estimator is adaptive because the definition of the neighborhood around \\(\\vec{x}\\) was learned from the data.\nRandom forests can likewise be interpreted as weighted adaptive nearest-neighbor estimators. For each unit \\(i\\), the predicted value is the average outcome of all other units where each unit \\(j\\) is weighted by the frequency with which it falls in the same leaf as unit \\(i\\). Seeing forest-based predictions as a weighted average of other units’ outcomes is a powerful perspective that has led to new advances in forests for uses that go beyond standard regression (Athey, Tibshirani, & Wager 2019).",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#gradient-boosted-trees",
    "href": "algorithms_for_prediction.html#gradient-boosted-trees",
    "title": "Algorithms for prediction",
    "section": "Gradient boosted trees",
    "text": "Gradient boosted trees\n\nTo read more on this topic, see Ch 17.2 of Efron & Hastie (2016)\n\nGradient boosted trees are similar to a random forest in that the result is an average of trees. A key difference is how the trees are produced. In a random forest, the trees are all independent prediction functions learned in parallel. In boosting, the trees are learned sequentially, with each tree correcting the errors of preceding trees. More precisely, the algorithm begins by predicting the sample mean for all observations and defining residuals as the difference between the true outcome \\(Y\\) and the predicted value. Then the algorithm estimates a regression tree to model the residuals. It adds a regularized version of the predicted residuals to the predicted value, then calculates residuals and fits a new tree to predict the new residuals. Over a series of rounds, the predictions become sequentially closer to the truth, as visualized below.\n\n\nCode\nlibrary(xgboost)\nxgboost_illustration &lt;- foreach(round = 1:6, .combine = \"rbind\") %do% {\n  xgboost.out &lt;- xgboost(\n    data = model.matrix(~ x + z, data = simulated_data),\n    label = simulated_data |&gt; pull(y),\n    nrounds = round\n  )\n  to_predict |&gt;\n    mutate(yhat = predict(xgboost.out, newdata = X_to_predict),\n           round = paste(\"After\",round,ifelse(round == 1, \"round\", \"rounds\"),\"of boosting\"))\n}\np_no_points +\n  geom_line(data = xgboost_illustration,\n            aes(y = yhat)) +\n  facet_wrap(~round) +\n  ggtitle(\"Solid lines are xgboost::xgboost() predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\nA difficulty in boosting is knowing when to stop: the predictions improve over time but ultimately will begin to overfit. While more trees is always better in a random forest, the same is not true of boosting.\nTo learn more about boosting in math, we recommend Ch 17.2 of Efron & Hastie (2016). To try boosting, you can install the xgboost package in R and follow the code below.\n\nlibrary(xgboost)\n\nTo fit a boosting model, first define a predictor matrix and an outcome vector.\n\nX &lt;- model.matrix(~ x + z, data = simulated_data)\nY &lt;- simulated_data |&gt; pull(y)\n\nThe code below uses these data to carry out 10 rounds of boosting (recall that the number of rounds is a key tuning parameter, and 10 is only chosen for convenience).\n\nxgboost.out &lt;- xgboost(\n  data = model.matrix(~ x + z, data = simulated_data),\n  label = simulated_data |&gt; pull(y),\n  nrounds = 10\n)\n\nFinally, we can define new data at which to predict\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict &lt;- model.matrix(~ x + z, data = to_predict)\n\nand then produce predicted values.\n\nxgboost_predicted &lt;- to_predict |&gt;\n  mutate(yhat = predict(xgboost.out, newdata = X_to_predict))\n\nVisualizing the result, we can see that boosting performed well in our simulated example.\n\np_no_points +\n  geom_line(\n    data = xgboost_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are xgboost::xgboost() predictions.\\nDashed lines are the true conditional mean.\")",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#closing-thoughts",
    "href": "algorithms_for_prediction.html#closing-thoughts",
    "title": "Algorithms for prediction",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nAlgorithms for prediction are often powerful tools from data science. Because they are \\(\\hat{Y}\\) tools, their application in social science requires us to ask questions involving \\(\\hat{Y}\\). One such category of questions that have been the focus of this page are questions about conditional means. Questions about conditional means are \\(\\hat{Y}\\) questions because the conditional mean \\(E(Y\\mid\\vec{X} = \\vec{x})\\) would be the best possible prediction of \\(Y\\) given \\(\\vec{X} = \\vec{x}\\) when “best” is defined by squared error loss. We have therefore focused on algorithms for prediction that seek to minimize the sum of squared errors, since these algorithms may often be useful in social science as tools to estimate conditional means.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "algorithms_for_prediction.html#footnotes",
    "href": "algorithms_for_prediction.html#footnotes",
    "title": "Algorithms for prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRaudenbush, S. W., and A.S. Bryk. (2002). Hierarchical linear models: Applications and data analysis methods. Advanced Quantitative Techniques in the Social Sciences Series/SAGE.↩︎\nWhile we write out \\(\\hat\\mu_0\\), algorithmic implementations of ridge regression often mean-center \\(Y\\) before applying the algorithm which is equivalent to having an unpenalized \\(\\hat\\mu_0\\).↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Algorithms for prediction"
    ]
  },
  {
    "objectID": "data_driven_selection.html",
    "href": "data_driven_selection.html",
    "title": "Data-driven selection of an estimator",
    "section": "",
    "text": "If you are taking notes, here is a PDF of the slides and a PDF of this page. If you are a Stata user, most of this page is R code. The one most important exercise is sample splitting, for which you can use sample_split.do.\nQuantitative social scientists have long faced the question of how to choose a model. Even within the scope of linear regression, one might wonder whether a model that interacts two predictors is better than one that includes them only additively. As computational advances have yielded new algorithms for prediction, the number of choices has exploded. Many models are possible. How should we choose?\nAn algorithm for prediction takes as its input a feature vector \\(\\vec{x}\\) and returns as its output a predicted value, \\(\\hat{y}\\). One way to choose among several algorithms is to find the one that produces predictions \\(\\hat{y}\\) that are as close as possible to the true outcomes \\(y\\). While this predictive metric might seem grounded in data science, this page will show how metrics of predictive performance can also help with a classical social science task: estimating subgroup means. By the end of the page, we will have motivated why one should care about metrics of predictive performance and learned tools to estimate predictive performance by sample splitting.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#predicting-for-individuals",
    "href": "data_driven_selection.html#predicting-for-individuals",
    "title": "Data-driven selection of an estimator",
    "section": "Predicting for individuals",
    "text": "Predicting for individuals\nContinuing with the example of baseball player salaries, we consider a model to predict salary this year as a linear function of team average salary from last year. We first prepare the environment and load data.\n\nlibrary(tidyverse)\n\n\nbaseball &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nThen we draw a sample of 5 players per team.\n\nset.seed(90095)\nlearning &lt;- baseball |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nWe estimate a linear regression model on this sample.\n\nlinear_regression &lt;- lm(\n  salary ~ team_past_salary, \n  data = learning\n)\n\nThe figure below visualizes the predictions from this linear regression, calculated for all players who were not part of the random learning sample.\n\n\n\n\n\n\n\n\n\nWhile one might have hoped to tell a story about high-quality prediction, the dominant story in the individual-level prediction plot is one of poor prediction: players’ salaries vary widely around the estimated regression line. To put that fact to a number, one might consider \\(R^2\\) which involves a ratio of two expected squared prediction errors, one from the prediction function \\(\\hat{f}\\) and one from a comparison model that predicts the mean for all cases.1\n\\[\nR^2\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                Y - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(Y - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{Variance of $Y$}\n}\n\\] In other words, subtracting the predicted values from the individual players’ salaries only reduces the expected squared error by 5.8%. If the goal is to predict for individuals, the model does not seem very good!",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#estimating-subgroup-means",
    "href": "data_driven_selection.html#estimating-subgroup-means",
    "title": "Data-driven selection of an estimator",
    "section": "Estimating subgroup means",
    "text": "Estimating subgroup means\nA social scientist might respond that the goal was never to accurately predict the salary of any individual baseball player. Rather, the data on individual players was in service of a more aggregate goal: estimating the mean salary on each team. Noting that the prediction is the same for every player on a team, the social scientist might propose the graph below, in which the unit of analysis is a team instead of a player.\n\n\n\n\n\n\n\n\n\nThe social scientist might argue that the model is quite good for team salaries. If we take the goal to be to estimate the team average salary, then we might create an analogous version of \\(R^2\\) focused on estimation of team-average salaries.2\n\\[\nR^2_\\text{Group}\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{If Predicted $\\text{E}(Y)$ for Everyone}\n}\n\\] By that metric, the model seems quite good, predicting away 57.1% of the expected squared error at the team level. Surprisingly, a model that was not very good at predicting for individuals might be quite good at predicting the team-average outcomes!\nOne might respond that prediction and estimation are simply different goals, with little to do with one another. But in fact the two are mathematically linked. Given two models to choose from, the one that predicts better (in squared error loss) will also be a better estimator of the subgroup means.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#prediction-and-estimation-are-connected",
    "href": "data_driven_selection.html#prediction-and-estimation-are-connected",
    "title": "Data-driven selection of an estimator",
    "section": "Prediction and estimation are connected",
    "text": "Prediction and estimation are connected\nTo formalize the problem of choosing an estimator, suppose we have two prediction functions \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\). Each function takes in a vector of features \\(\\vec{x}\\) and returns a predicted value, \\(\\hat{f}_1(\\vec{x})\\) or \\(\\hat{f}_2(\\vec{x})\\). We will assume for simplicity that each function has already been learned on a simple random sample from our population, and that the remaining units available to us are those that were not used in the learning process.\nSuppose we draw a random unit with features \\(\\vec{X}\\) and outcome \\(Y\\). For this unit, algorithm one would have a squared prediction error \\((Y - \\hat{f}_1(\\vec{X}))^2\\). We might score each algorithm’s performance by the average squared prediction error, with the average taken across units.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}} = \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] In our baseball example, the algorithm \\(\\hat{f}_1\\) makes an error when Mookie Betts has a salary of 21.2m but the algorithm only predicts 7.0m. The expected squared prediction error is the squared difference between these two values, taken on average over all players.\nOur social scientist has already replied that we rarely care about predicting the salary of an individual player. Instead, our questions are really about estimating subgroup means, such as the mean pay on each team. The social scientist might instead want to know about estimation error, \\[\n\\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} = \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] where \\(\\hat{f}(\\vec{X})\\) is the predicted salary of a player on team \\(\\vec{X}\\) and \\(\\text{E}(Y\\mid\\vec{X})\\) is the true population average salary on that team. This social scientist does not care about predicting for individual salaries \\(Y\\), but rather about accurately estimating the mean salary \\(\\text{E}(Y\\mid\\vec{X})\\) in each team.\nA little math (proof at the end of this section) can show that these two goals are actually closely linked. Expected squared prediction error equals expected squared estimation error plus expected within-group variance.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}  = \\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} + \\underbrace{\\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]}_{\\substack{\\text{Expected Within-}\\\\\\text{Group Variance}}}\n\\] Taking our baseball example, there are two sources of prediction error for Mookie Betts.\nFirst, salaries vary among the Dodger players (\\(\\text{V}(Y\\mid\\vec{X} = \\text{Dodgers})\\)). Because Mookie Betts and Freddie Freeman are both players on the Dodgers, they are identical from the perspective of the model (they have identical \\(\\vec{X}\\) values) and it has to make the same prediction for both of them. Just as there is variance within the Dodgers, there is variance within all MLB teams. The within-team variance averaged over teams (weighted by size) is the term at the right of the decomposition.\nSecond, the expected squared estimation error is the average squared difference between each player’s predicted salary and the true mean pay on that player’s team, \\(\\text{E}(Y\\mid\\vec{X})\\). In the case of Mookie Betts, this is the difference between the prediction for Mookie Betts and the true mean salary on his team, the Dodgers. Estimation error corresponds to our error if our goal is to estimate the mean salary on each team, instead of predicting the salary for each individual.\nNow suppose two prediction algorithms \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\) have different performance. For example, maybe the first algorithm is a better predictor: \\(\\text{ESPE}(\\hat{f}_1) &lt; \\text{ESPE}(\\hat{f}_2)\\). Regardless of which algorithm is used, the within-group variance component of the decomposition is unchanged. Therefore, if algorithm 1 is the better predictor, then it must also be the better estimator: \\(\\text{ESEE}(\\hat{f}_1) &lt; \\text{ESEE}(\\hat{f}_2)\\).\nIn fact, suppose an algorithm was omniscient and managed to predict the true conditional mean function for every observation, \\(\\hat{f}_\\text{Omniscient}(\\vec{X}) = \\text{E}(Y\\mid\\vec{X})\\). Then estimation error would be zero for this function. Prediction error would equal the expected within-group variance. The best possible prediction function (with squared error loss) is the conditional mean. This is one intuitive reason why an algorithm with good predictive performance is also a good estimator.\nThese facts motivate an idea for choosing an estimation function: to estimate conditional means well, choose the algorithm that minimizes squared prediction error.\nAppendix to section. A proof of the decomposition is provided below, but the ideas above are more important than the details of the proof.\n\\[\n\\begin{aligned}\n\\text{ESPE}(\\hat{f})\n&= \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&\\text{Add zero} \\\\\n&= \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X}) + \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&= \\underbrace{\n  \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right) ^ 2\\right]\n}_{=\\text{E}[\\text{V}(Y\\mid\\vec{X})]}\n  + \\underbrace{\n  \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right) ^ 2\\right]\n  }_{=\\text{ESEE}(\\hat{f})}\n  \\\\\n  &\\qquad + \\underbrace{\n  2\\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right)\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)\\right]\n  }_{\\substack{=\\text{Cov}[Y - \\text{E}(Y\\mid\\vec{X}), E(Y\\mid\\vec{X} - \\hat{f}(\\vec{X}))]=0\\\\\\text{covariance of within-group error and estimation error,}\\\\\\text{equals zero if the test case }Y\\text{ is not used to learn }\\hat{f}}} \\\\\n  &= \\text{ESEE}(\\hat{f}) + \\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]\n\\end{aligned}\n\\]",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#why-out-of-sample-prediction-matters",
    "href": "data_driven_selection.html#why-out-of-sample-prediction-matters",
    "title": "Data-driven selection of an estimator",
    "section": "Why out-of-sample prediction matters",
    "text": "Why out-of-sample prediction matters\nThe connection between prediction and estimation opens a powerful bridge: we can find good estimators by exploring which algorithms predict well. But it is important to remember that this bridge exists only for out-of-sample prediction error: error when predictions are made on a new sample that did not include the initial predictions.\n\nk-nearest neighbors estimator\nTo illustrate in-sample and out-of-sample prediction error, we consider a nearest neighbors estimator. When making a prediction for player \\(i\\), we might worry that we have too few sampled units on the team of player \\(i\\). We might solve this issue by averaging the sampled salaries within the team of player \\(i\\) and also the \\(k\\) nearest teams whose salaries in the past season were most similar.\nFor example, the Dodgers’ past-year average salary was $8.39m. The most similar team to them was the N.Y. Mets, who had an average salary of $8.34m. Because the past salaries are so similar, we might pool information: predict the Dodgers’ mean salary by the average of sampled players on both the Dodgers and the N.Y. Mets. If we wanted to pool more information, we might include the next-most similar team, the N.Y. Yankees with past salary $7.60m. We could pool more by also including the 3rd-nearest neighbor (Philadelphia, $6.50m), the 4th-nearest neighbor (San Diego, $6.39m), and so on. The more neighbors we include, the more pooled our estimate becomes.\n\n\nIn-sample performance\nHow many neighbors should we include? We first consider evaluating by in-sample performance: learn the estimator on a sample and evaluate predictive performance in that same sample. We repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error in that same sample\n\nThe blue line in the figure below shows results. In-sample mean squared prediction error is lowest when we pool over 0 neighbors. With in-sample evaluation, the predictions become gradually worse (higher mean squared error) as we pool information over more teams. If our goal were in-sample prediction, we should choose an estimator that does not pool information at all: the Dodgers’ population mean salary would be estimated by the mean among the sampled Dodgers only.\n\n\n\n\n\n\n\n\n\n\n\nOut-of-sample performance\nThe red line in the figure above shows a different performance metric: out-of-sample performance. This line shows what happens when we repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error on all units not included in that sample\n\nThe red line of out-of-sample performance looks very different than the blue line of in-sample performance, in two ways.\nFirst, the red line is always higher than the blue line. It is always harder to predict out-of-sample cases than to predict in-sample cases. This is unsurprising—the blue line was cheating by getting to see the outcomes of the very cases it was trying to predict!\nSecond, the red line exhibits a U-shaped relationship. Predictive performance improves (lower mean squared error) as we pool information over a few nearby teams. This is because the variance of the estimator is declining. After reaching an optimal value at around 10 neighbors, predictive performance begins to become worse (higher mean squared error).\nOne way to think about the red and blue lines is in terms of the signal and the noise. In any particular sample of 10 Dodger players, there is some amount of signal (true information about the Dodger population mean) and some amount of noise (randomness in the sample average arising from which 10 players we happened to sample). The distinction is irrelevant for in-sample prediction error, for which a close fit to both the signal and the noise yields low prediction error. But for out-of-sample prediction error, fitting to the signal improves performance while fitting to the noise harms performance. As one moves to the right in the graph, one is getting less of the signal and less of the noise. Thus, the blue line of in-sample performance gets consistently worse. The red line improves at first as the reduction in noise outweighs the reduction in signal, but then gets worse as the reduction in signal begins to outweigh the reduction in noise. In-sample prediction error is a poor metric because fitting closely to the noise can make this metric look misleadingly good. Out-of-sample error avoids this problem. The best value for nearest neighbors is the one that optimizes the tradeoff between signal and noise, where the red curve is minimized.\nAnother way to think about the lines is in terms of a bias-variance tradeoff. As we pool the Dodgers together with the N.Y. Mets and other teams, the variance of the estimator declines because the Dodger predicted salary is averaged over more teams. But the bias of the estimator increases: the N.Y. Mets are not the Dodgers, and including them in the average induces a bias. The minimum of the red curve is the amount of information pooling that optimizes the bias-variance tradeoff.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#sample-splitting",
    "href": "data_driven_selection.html#sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Sample splitting",
    "text": "Sample splitting\nThe illustration above showed that we ideally evaluate an estimator learned in a sample by performance when making predictions for the rest of the population (excluding that sample). But in practice, we often have access only to the sample and not to the rest of the population. To learn out-of-sample predictive performance, we need sample splitting.\nIn its simplest version, sample splitting proceeds in three steps:\n\nRandomly partition sampled cases into training and test sets\nLearn the prediction function among training cases\nEvaluate its performance among test cases\n\nVisually, we begin by randomly assigning cases into training or test sets.\n\n\n\n\n\n\n\n\n\nThen we separate these into two datasets. We use the train set to learn the model and the test set to evaluate the performance of the learned model.\n\n\n\n\n\n\n\n\n\nIn code, we can carry out a train-test split by first loading the baseball population,\n\nbaseball_population &lt;- read_csv(\"https://ilundberg.github.io/soc212b/data/baseball_population.csv\")\n\nand drawing a sample of 10 players per team.\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  ungroup()\n\nWe can then create the split. The code below first stratifies by team and then randomly samples 50% of cases to be used for the train set.\n\ntrain &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  slice_sample(prop = .5)\n\nThe code below takes all remaining cases not used for training to be used as the test set.\n\ntest &lt;- baseball_sample |&gt;\n  anti_join(train, by = join_by(player))\n\nWe can then learn a prediction function in the train set and evaluate performance in the test set. For example, the code below uses OLS.\n\nols &lt;- lm(salary ~ team, data = train)\ntest_mse &lt;- test |&gt;\n  # Make predictions\n  mutate(yhat = predict(ols, newdata = test)) |&gt;\n  # Calculate squared errors\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  # Summarize\n  summarize(mse = mean(squared_error))",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "href": "data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Tuning parameters by sample splitting",
    "text": "Tuning parameters by sample splitting\nOne way we might use sample splitting is for parameter tuning: to choose the value of some unknown tuning parameter such as the penalty \\(\\lambda\\) in ridge regression.\n\nlibrary(glmnet)\n\nLoaded glmnet 4.1-8\n\nridge_regression &lt;- glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nThe glmnet package makes estimates at many values of the penalty parameter \\(\\lambda\\). We can make predictions in the test set from models using these various values of \\(\\lambda\\).\n\npredicted &lt;- predict(\n  ridge_regression,\n  newx = model.matrix(~team, data = test)\n)\n\nWe can extract the penalty parameter values with ridge_regression$lambda, organizing them in a tibble for ease of access.\n\nlambda_tibble &lt;- tibble(\n  lambda = ridge_regression$lambda,\n  lambda_index = colnames(predicted)\n)\n\nFor each \\(\\lambda\\) value, the predictions are a column of predicted. The code below wrangles these predictions into a tidy format.\n\npredicted_tibble &lt;- as_tibble(predicted) |&gt;\n  # Append the actual value of the test outcome\n  mutate(y = test |&gt; pull(salary)) |&gt;\n  pivot_longer(cols = -y, names_to = \"lambda_index\", values_to = \"yhat\") |&gt;\n  left_join(lambda_tibble, by = join_by(lambda_index)) |&gt;\n  mutate(squared_error = (y - yhat) ^ 2)\n\nAt each \\(\\lambda\\) value, we can calculate mean squared error in the test set.\n\nmse &lt;- predicted_tibble |&gt;\n  group_by(lambda) |&gt;\n  summarize(mse = mean(squared_error))\n\nThe figure below visualizes these estimates. As the penalty parameter grows larger, predictive performance improves (lower error) to a point and then begins to get worse. The selected best value of the tuning parameter \\(\\lambda\\) is highlighted in blue.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#cross-validation",
    "href": "data_driven_selection.html#cross-validation",
    "title": "Data-driven selection of an estimator",
    "section": "Cross-validation",
    "text": "Cross-validation\nA downside of 50-50 sample splitting is that the train set is only half as large as the full available sample. We might prefer if it were larger, for instance 80% of the data used for training and only 20% used for testing. But then our estimates of performance in the test set might be noisy.\nOne solution to this problem is cross-validation, which proceeds in a series of steps:\n\nRandomize the sampled cases into a set of folds (e.g., 5 folds).\nTake fold 1 as the test set and estimate predictive performance.\nTake fold 2 as the test set and estimate predictive performance.\nIterate until all folds have served as the test set\nAverage predictive performance over the folds\n\n\n\n\n\n\n\n\n\n\nOptionally, repeat for many repetitions of randomly assigning cases to folds to reduce stochastic error.\nCross-validation is so common that it is already packaged into some of the learning algorithms we have considered in class. For example, the code below carries out cross-validation to automatically select the penalty parameter for ridge regression.\n\nridge_regression_cv &lt;- cv.glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nWe can use ridge_regression_cv$lambda.min to extract the chosen value of \\(\\lambda\\) that minimizes cross-validated mean squared error (2.7074931^{7}). We can also visualize the performance with a plot() function.\n\nplot(ridge_regression_cv)\n\n\n\n\n\n\n\n\nWe can make predictions at the chosen value of \\(\\lambda\\) by specifying the s argument in the predict() function.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat = predict(\n      ridge_regression_cv,\n      s = \"lambda.min\",\n      newx = model.matrix(~team, data = test)\n    )[,1]\n  )",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "href": "data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "title": "Data-driven selection of an estimator",
    "section": "Choosing an algorithm by sample splitting",
    "text": "Choosing an algorithm by sample splitting\nIn the example above, we used sample splitting to choose the optimal value of a penalty parameter (\\(\\lambda\\)) within a particular algorithm for prediction (ridge regression). One can also use a train-test split to choose among many very different estimators. For example, we might estimate OLS, a tree, and a forest.\n\nlibrary(rpart)\nlibrary(grf)\n\n\nols &lt;- lm(salary ~ team_past_salary, data = train)\ntree &lt;- rpart(salary ~ team_past_salary, data = train)\nforest &lt;- regression_forest(\n  X = model.matrix(~team_past_salary, data = train),\n  Y = train |&gt; pull(salary)\n)\n\nWe can make predictions for all of these estimators in the test set.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat_ols = predict(ols, newdata = test),\n    yhat_tree = predict(tree, newdata = test),\n    yhat_forest = predict(forest, newdata = model.matrix(~team_past_salary, data = test))$predictions\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"yhat\"), names_to = \"estimator\", values_to = \"yhat\"\n  )\n\nThe figure below visualizes the predicted values.\n\n\n\n\n\n\n\n\n\nWe can calculate mean squared error in the test set for each algorithm to determine which one is producing the best predictions.\n\nmse &lt;- predicted |&gt;\n  group_by(estimator) |&gt;\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  summarize(mse = mean(squared_error))",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "data_driven_selection.html#footnotes",
    "href": "data_driven_selection.html#footnotes",
    "title": "Data-driven selection of an estimator",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen estimating \\(R^2\\), we often use the training sample mean as an estimator of \\(\\text{E}(Y)\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎\nWhen estimating \\(R^2_\\text{Group}\\), we often use the subgroup training sample mean as an estimator of \\(\\text{E}(Y\\mid\\vec{X})\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Data-driven selection of an estimator"
    ]
  },
  {
    "objectID": "yhat_regression.html",
    "href": "yhat_regression.html",
    "title": "Regression for Y-hat",
    "section": "",
    "text": "slides\nThis class is about regression as a tool to approximate a conditional expectation function. From this perspective, the \\(\\hat\\beta\\) estimates are only a step toward the broader purpose of regression to produce \\(\\hat{Y}\\) values that achieve this approximation well. This perspective will ultimately allow us to consider machine learning estimators beyond regression.\nSome of this class relies on an ongoing project on description: ilundberg.github.io/description",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Regression for Y-hat"
    ]
  },
  {
    "objectID": "weighting_for_inference.html",
    "href": "weighting_for_inference.html",
    "title": "Weighting for causal and population inference",
    "section": "",
    "text": "This session will be about weighting to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by weighting"
    ]
  },
  {
    "objectID": "resampling.html",
    "href": "resampling.html",
    "title": "Statistical Uncertainty: Bootstrap and Beyond",
    "section": "",
    "text": "As researchers adopt algorithmic estimation methods for which analytical standard errors do not exist, methods to produce standard errors by resampling become all the more important. We will discuss the bootstrap for simple random samples and extensions to allow resampling-based standard error estimates in complex survey samples.",
    "crumbs": [
      "Problem Sets",
      "Descriptive Data Science with Probability Samples",
      "Statistical uncertainty by resampling"
    ]
  },
  {
    "objectID": "prediction_for_inference.html",
    "href": "prediction_for_inference.html",
    "title": "Prediction for causal and population inference",
    "section": "",
    "text": "This session will be about prediction to draw population inference from non-probability samples and causal inference from observational studies, both of which involve analogous assumptions and estimators. If you have a Census with features \\(\\vec{X}\\), ignorable sampling conditional on \\(\\vec{X}\\), and a good sample estimator of \\(E(Y\\mid\\vec{X})\\) then you can predict \\(E(Y\\mid\\vec{X})\\) and aggregate over the Census-known population distribution of \\(\\vec{X}\\). For causal inference, being assigned to treatment is analogous to being sampled to observe the potential outcome under treatment.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Estimation by prediction"
    ]
  },
  {
    "objectID": "nonparametric_identification.html",
    "href": "nonparametric_identification.html",
    "title": "Nonparametric Identification",
    "section": "",
    "text": "This page will cover nonparametric identification of causal and population parameters. To be written.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Nonparametric identification"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "Mediation",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Mediation"
    ]
  },
  {
    "objectID": "asking_a_research_question.html",
    "href": "asking_a_research_question.html",
    "title": "Asking a research question",
    "section": "",
    "text": "slides\nThis class is about how to ask a quantitative research question. The focus will be on summarizing outcomes over well-defined populations, for which a regression coefficient \\(\\hat\\beta\\) may or may not be a meaningful summary. We will focus on summarizing subgroups by the average value of \\(Y\\), or in small samples by summarizing subgroup means using predicted values (\\(\\hat{Y}\\)) from regression models.",
    "crumbs": [
      "Problem Sets",
      "Asking a Research Question"
    ]
  },
  {
    "objectID": "doubly_robust.html",
    "href": "doubly_robust.html",
    "title": "Doubly-robust estimation",
    "section": "",
    "text": "This session will combine prediction and weighting methods for an approach with properties superior to either on its own.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Doubly-robust estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to UCLA SOCIOL 212B (Winter 2025). See the syllabus.\nW 9–11:50am. Powell 320B.\nThis course is about answering social science questions using quantitative data. We will especially focus on how computational power is transforming the ways we can carry out quantitative research, covering both statistical and machine learning tools from the perspective of social science applications. The course especially emphasizes how to translate social science theories into quantities that can be estimated by algorithms designed for prediction. We will consider prediction in the service of both description and causal inference, building on ideas from SOCIOL 212A. The end product of the course is an extended abstract containing data analysis using the ideas from the course. For students continuing to 212C, the abstract can serve as the basis for the research project in that course. Students will leave the course prepared to connect social science theories to empirical evidence that can be produced by algorithms designed for prediction."
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Welcome!",
    "section": "Learning goals",
    "text": "Learning goals\nStudents will learn to\n\ndefine a precise quantitative research question\nconnect that question to predictions that can be made by statistical or machine learning algorithms\nmake a principled argument for the choice of a particular learning approach"
  },
  {
    "objectID": "index.html#schedule-of-topics-tentative",
    "href": "index.html#schedule-of-topics-tentative",
    "title": "Welcome!",
    "section": "Schedule of topics (tentative)",
    "text": "Schedule of topics (tentative)\nPart 1. Descriptive data science with probability samples.\n\nJan 8. Asking research questions without \\(\\hat\\beta\\) and regression for \\(\\hat{Y}\\)\nJan 15 and 22. Algorithms for prediction\nJan 29. Data-driven selection of an estimator\nFeb 5. Panel data (actually a Part 2 topic, presented out of order to align with afternoon CCPR workshop)\nFeb 12. Statistical uncertainty by resampling\n\nPart 2. Non-probability samples and observational causal inference\n\nFeb 19. Nonparametric identification\nFeb 26. Estimation by prediction\nMar 5. Estimation by weighting\nMar 12. Doubly-robust estimation"
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "Welcome!",
    "section": "Who should take this course?",
    "text": "Who should take this course?\nThe course is designed to support the development of quantitative social science research projects. The course is a good fit for PhD students in sociology, statistics, political science, economics, and other social sciences. PhD students from disciplines other than sociology should request a code from the instructor to enroll."
  },
  {
    "objectID": "index.html#prerequisite",
    "href": "index.html#prerequisite",
    "title": "Welcome!",
    "section": "Prerequisite",
    "text": "Prerequisite\nFamiliarity with basic probability and statistics (e.g., random variables, expectation, confidence intervals). Soc 212A is formally a prerequisite, but students who did not take Soc 212A are welcome to talk with me about whether Soc 212B would be a good fit for them."
  },
  {
    "objectID": "index.html#instructional-format",
    "href": "index.html#instructional-format",
    "title": "Welcome!",
    "section": "Instructional format",
    "text": "Instructional format\nLecture with in-class exercises. Bring computers to class."
  },
  {
    "objectID": "index.html#course-readings",
    "href": "index.html#course-readings",
    "title": "Welcome!",
    "section": "Course readings",
    "text": "Course readings\nReadings will be available online for free. See the course website for an updated schedule of readings and topics.\nMany readings from books with free PDFs available online:\n\nEfron, B., & T Hastie. 2016. Computer Age Statistical Inference: Algorithms, Evidence and Data Science. Cambridge: Cambridge University Press.\nHastie, T., R. Tibshirani, & J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer.\nHernán, M.A., & J.M. Robins. 2024. Causal Inference: What If? Boca Raton: Chapman & Hall / CRC."
  },
  {
    "objectID": "index.html#statistical-software",
    "href": "index.html#statistical-software",
    "title": "Welcome!",
    "section": "Statistical software",
    "text": "Statistical software\nYou can use any statistical software you prefer. I use R and will best be able to support you in R. In addition to R, we will attempt to provide Stata support where possible. Not all algorithms are available in Stata. If you are fluent in another software, you are welcome to use that. The focus of this course is on conceptual ideas, not a programming language."
  },
  {
    "objectID": "missing_data.html",
    "href": "missing_data.html",
    "title": "Missing data",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Missing data"
    ]
  },
  {
    "objectID": "panel_data.html",
    "href": "panel_data.html",
    "title": "Panel data",
    "section": "",
    "text": "Here are slides for today.\n\nThis session will cover a series of estimators that apply when\n\nmany units are observed over many time periods each\none or more units become treated at some point in the observation window\n\nThese estimators include difference in difference, interrupted time series, regression discontinuity, and synthetic control. In order to cover many estimators, today’s class will focus more on conceptual ideas than on coding. Thus, material is currently available in slide form rather than in written note form.",
    "crumbs": [
      "Problem Sets",
      "Non-Probability Samples and Observational Causal Inference",
      "Panel data"
    ]
  },
  {
    "objectID": "problem_sets.html",
    "href": "problem_sets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "This page will contain a problem set corresponding to each class meeting. The problem sets are very open-ended and are designed to connect the material from class to your ongoing project.\nFor every problem set, submit a PDF. If your code is not embedded in your PDF, then also submit a code file.\nWe chose together the following weekly pattern for problem sets:\nWe will be using identified peer reviews. The reason these are not anonymous is that we are a small class, and we will get to know one another’s projects. Your peer reviewer will not be grading you or assigning point values, but they will be commenting on your work. A good peer review is a short paragraph that comments on promising aspects of your peer’s work as well as offering suggestions for improvement or future directions."
  },
  {
    "objectID": "problem_sets.html#problem-set-1",
    "href": "problem_sets.html#problem-set-1",
    "title": "Problem Sets",
    "section": "Problem Set 1",
    "text": "Problem Set 1\n\n1. Your ongoing paper.\n1.1. (15 points) Write an abstract of your research paper.\n1.2. (5 points) What is one unit-specific quantity in your paper?\n1.3. (5 points) What is one target population in your paper?\n\n\n2. Regression for a conditional mean\n(25 points)\nIn class, we used OLS regression to predict the mean outcome in a subgroup. Do the same thing in a dataset of your choosing, which might be the dataset from your project. Define the outcome variable and the population subgroup you are describing."
  },
  {
    "objectID": "problem_sets.html#problem-set-2",
    "href": "problem_sets.html#problem-set-2",
    "title": "Problem Sets",
    "section": "Problem Set 2",
    "text": "Problem Set 2\n\n(20 points) In your project, do you have something analogous to many groups with a few observations per group? This could more generally be any case where the number of parameters to be estimated is somewhat large for the sample size available. If not in your project, think of an example in your general research area. Write a few sentences about why one might want penalized regression in that setting.\n\nThe remainder of this problem set is based in the idea of multilevel models, though you can alternatively estimate by LASSO or ridge regression if you prefer. You can complete parts (2) and (3) in your own dataset if you can define at least 10 groups with at least 20 units per group. Wherever we say “team”, substitute whatever group is in your dataset. You may also use the baseball_population.csv data, in which the group variable is team.\n\n(15 points) Draw a sample of 5 units per group. Estimate an unpenalized OLS regression and a penalized regression (multilevel, ridge, or LASSO). Produce a graph that compares the predicted values.\n(15 points) Repeat the exercise above but with a sample of 20 units per group. Visualize the estimates. How do your results change?\n\nNote that on (3) it is likely that the estimates will look different from (2), but it is also possible that they will be similar depending on your setting. For example, if your algorithm sets a very large penalty in your setting then all the group-specific deviations could be shrunk to zero in both (2) and (3). There are many right answers."
  },
  {
    "objectID": "problem_sets.html#problem-set-3",
    "href": "problem_sets.html#problem-set-3",
    "title": "Problem Sets",
    "section": "Problem Set 3",
    "text": "Problem Set 3\nUsing the simulation data from class or data from your own project:\n\n(15 points) Estimate a regression tree and visualize it. If you are using R, there are packages to visualize the tree. If you are using Stata, there may be packages or you are welcome to use the summary text output.\n(10 points) Write 3-4 sentences for someone who has never heard of this method. Pick some predictor vector value and explain how the tree produces a prediction for that predictor vector.\n(15 points) Estimate a random forest. Visualize the predicted values as a function of the predictors. If your data have only two predictors, this could be as in class with one on the \\(x\\)-axis and one for colors. If you have many predictors, you may need to find a creative way to visualize your predictions. You may choose to make predictions while holding one or more variables at a single value.\n(10 points) In a few sentences, give an example of a problem in your area of research (possibly your paper) where the functional form linking \\(\\vec{X}\\) to \\(E(Y\\mid\\vec{X})\\) is unknown, such that a random forest might be a way to learn the functional form from the data."
  },
  {
    "objectID": "problem_sets.html#problem-set-4",
    "href": "problem_sets.html#problem-set-4",
    "title": "Problem Sets",
    "section": "Problem Set 4",
    "text": "Problem Set 4\nWrite the abstract of your research paper. If you do not have results yet, pretend that your results are really amazing! You can make up any estimates that you want. As an added challenge, try writing the abstract twice with two sets of possible results. It would be ideal if all possible results are interesting.\nAs you write, minimize jargon so that you are writing for a New York Times reader. Emphasize big claims and don’t bury them in statistical terminology. The goal of this exercise is to help us ask and frame a high-impact question that will speak to a broad audience. Possibly, the exercise will lead you to change your question.\nYour peer reviewer should comment on things that may include:\n\nwhat about this abstract is especially compelling?\nwhat parts (if any) about this abstract are directed to a small subset of academia and perhaps should be broadened? This may be parts with jargon.\nwhat claims could be emphasized more?\n\nThe hope is that by writing an abstract (or abstracts) with findings that you could produce, we might come up with better ways of asking and presenting our questions."
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "Scale construction",
    "section": "",
    "text": "To be written. This is not a central topic of the course, but will have an appendix page since it may be relevant to student projects.",
    "crumbs": [
      "Problem Sets",
      "Additional topics",
      "Scale construction"
    ]
  },
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team!\n\n\n\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy! I am excited about causal inference and finding new research questions and ways to answer them. Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nSoonhong Cho\ntnsehdtm@gmail.com\n(he / him)\nSoonhong is a PhD candidate in political science at UCLA."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\n\n\n\nTerm\nMeaning\n\n\n\n\nUnit of analysis\nA row of your data\n\n\nUnit-specific quantity\nOutcome or potential outcome(s)\n\n\nTarget population\nSet of units over which to aggregate\n\n\n\n\nWhy? To ask \\(\\hat{Y}\\) questions, not \\(\\hat\\beta\\) questions"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-1",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-1",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nSome language notes:\n\nCausal language\n\nX verb Y\n\nNon-causal language\n\ndifferences in Y across X"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-2",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-2",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nMany submitted models like this:\n\\[E(Y \\mid X) = \\alpha + \\beta X\\]\n\nWhen \\(X\\) is binary, this model is like taking subgroup means.\n\n\n\\[\n\\begin{aligned}\nE(Y\\mid X = 0) &= \\alpha \\\\\nE(Y\\mid X = 1) &= \\alpha + \\beta\n\\end{aligned}\n\\]\n\n\nIt is not really a “model” at all"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-3",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-1-discussion-3",
    "title": "Algorithms for prediction",
    "section": "Problem Set 1: Discussion",
    "text": "Problem Set 1: Discussion\nOther things to discuss?"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#goals-for-today",
    "href": "slides/lec2/penalized_regression_slides.html#goals-for-today",
    "title": "Algorithms for prediction",
    "section": "Goals for today",
    "text": "Goals for today\n\nreview OLS to predict\nunderstand regularization to estimate\nmany parameters while avoiding high variance\nwalk through three regularized linear model algorithms\n\nmultilevel models\nridge regression\nLASSO regression"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\nAll 944 Major League Baseball Players, Opening Day 2023.\nCompiled by USA Today\nI appended each team’s win-loss record from 2022\nAvailable in baseball_population.csv"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-1",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-1",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\nLoad packages:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\n\n\nLoad data:\n\nbaseball_population &lt;- read_csv(\n  \"https://ilundberg.github.io/soc212b/data/baseball_population.csv\"\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-2",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-2",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\nbaseball_population |&gt; print(n = 3)\n\n# A tibble: 944 × 5\n  player               salary position team    team_past_record\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457\n2 Marte, Ketel       11600000 2B       Arizona            0.457\n3 Ahmed, Nick        10375000 SS       Arizona            0.457\n# ℹ 941 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-3",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-3",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\n\nCode\nbaseball_population |&gt;\n  group_by(position) |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  mutate(position = fct_reorder(position, -salary)) |&gt;\n  ggplot(aes(x = position, y = salary)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = label_currency(\n        scale = 1e-6, accuracy = .1, suffix = \" m\"\n      )(salary)\n    ), \n    y = 0, color = \"white\", fontface = \"bold\",\n    vjust = -1\n  ) +\n  scale_y_continuous(\n    name = \"Average Salary\",\n    labels = label_currency(scale = 1e-6, accuracy = 1, suffix = \" million\")\n  ) +\n  scale_x_discrete(\n    name = \"Position\",\n    labels = function(x) {\n      case_when(\n        x == \"C\" ~ \"C\\nCatcher\",\n        x == \"RHP\" ~ \"RHP\\nRight-\\nHanded\\nPitcher\",\n        x == \"LHP\" ~ \"LHP\\nLeft-\\nHanded\\nPitcher\",\n        x == \"1B\" ~ \"1B\\nFirst\\nBase\",\n        x == \"2B\" ~ \"2B\\nSecond\\nBase\",\n        x == \"SS\" ~ \"SS\\nShortstop\",\n        x == \"3B\" ~ \"3B\\nThird\\nBase\",\n        x == \"OF\" ~ \"OF\\nOutfielder\",\n        x == \"DH\" ~ \"DH\\nDesignated\\nHitter\"\n      )\n    }\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across positions\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-4",
    "href": "slides/lec2/penalized_regression_slides.html#data-baseball-salaries-4",
    "title": "Algorithms for prediction",
    "section": "Data: Baseball salaries",
    "text": "Data: Baseball salaries\n\n\nCode\nbaseball_population |&gt;\n  group_by(team) |&gt;\n  summarize(\n    salary = mean(salary),\n    team_past_record = unique(team_past_record)\n  ) |&gt;\n  ggplot(aes(x = team_past_record, y = salary)) +\n  geom_point() +\n  ggrepel::geom_text_repel(\n    aes(label = team),\n    size = 2\n  ) +\n  scale_x_continuous(\n    name = \"Team Past Record: Proportion Wins in 2022\"\n  ) +\n  scale_y_continuous(\n    name = \"Team Average Salary in 2023\",\n    labels = label_currency(\n      scale = 1e-6, \n      accuracy = 1, \n      suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  ggtitle(\"Baseball salaries vary across teams\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\n\nWe have the full population:\n\ntrue_dodger_mean &lt;- baseball_population |&gt;\n  # Restrict to the Dodgers\n  filter(team == \"L.A. Dodgers\") |&gt;\n  # Record the mean salary\n  summarize(mean_salary = mean(salary)) |&gt;\n  # Pull that estimate out of the data frame to just be a number\n  pull(mean_salary)\n\nThe true Dodger mean salary on Opening Day 2023 was $6,232,196."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-1",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-1",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nImagine we have\n\npredictors for everyone\nsalary for a sample of 5 players per team"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-2",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-2",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-3",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-3",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-4",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-4",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-5",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-5",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\n\nlearn model on everyone\ncreate data to predict: the Dodger population\npredict the outcome in this target population\naverage over units"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-6",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-6",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nDraw a sample of 5 players per team.\n\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup() |&gt;\n  print()\n\n# A tibble: 150 × 5\n   player                 salary position team    team_past_record\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Carroll, Corbin       1625000 OF       Arizona            0.457\n 2 Herrera, Jose          724300 C        Arizona            0.457\n 3 Melancon, Mark*       6000000 RHP      Arizona            0.457\n 4 Ahmed, Nick          10375000 SS       Arizona            0.457\n 5 Robinson, Kristian**   720000 OF       Arizona            0.457\n 6 Arcia, Orlando        2300000 SS       Atlanta            0.623\n 7 Lee, Dylan             730000 LHP      Atlanta            0.623\n 8 Pillar, Kevin         3000000 OF       Atlanta            0.623\n 9 Matzek, Tyler*        1200000 LHP      Atlanta            0.623\n10 Hilliard, Sam          750000 OF       Atlanta            0.623\n# ℹ 140 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-7",
    "href": "slides/lec2/penalized_regression_slides.html#task-predict-dodger-mean-salary-7",
    "title": "Algorithms for prediction",
    "section": "Task: Predict Dodger mean salary",
    "text": "Task: Predict Dodger mean salary\nOur 5 sampled Dodger players have an average salary of $7,936,238.\n\n\n# A tibble: 5 × 5\n  player           salary position team         team_past_record\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;\n1 Phillips, Evan  1300000 RHP      L.A. Dodgers            0.685\n2 Miller, Shelby  1500000 RHP      L.A. Dodgers            0.685\n3 Taylor, Chris  15000000 OF       L.A. Dodgers            0.685\n4 Betts, Mookie  21158692 OF       L.A. Dodgers            0.685\n5 Outman, James    722500 OF       L.A. Dodgers            0.685"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#task-create-data-to-predict",
    "href": "slides/lec2/penalized_regression_slides.html#task-create-data-to-predict",
    "title": "Algorithms for prediction",
    "section": "Task: Create data to predict",
    "text": "Task: Create data to predict\nWe want to predict salary for all Dodger players:\n\ndodgers_to_predict &lt;- baseball_population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  select(-salary)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-model",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-model",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Model",
    "text": "Ordinary Least Squares: Model\nModel salary as a function of team_past_record\n\nols &lt;- lm(\n  salary ~ team_past_record,\n  data = baseball_sample\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-predict",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-predict",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Predict",
    "text": "Ordinary Least Squares: Predict\n\nols_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict)) |&gt;\n  print(n = 3)\n\n# A tibble: 35 × 5\n  player           position team         team_past_record predicted_salary\n  &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;            &lt;dbl&gt;\n1 Freeman, Freddie 1B       L.A. Dodgers            0.685         5552999.\n2 Heyward, Jason   OF       L.A. Dodgers            0.685         5552999.\n3 Betts, Mookie    OF       L.A. Dodgers            0.685         5552999.\n# ℹ 32 more rows"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-average",
    "href": "slides/lec2/penalized_regression_slides.html#ordinary-least-squares-average",
    "title": "Algorithms for prediction",
    "section": "Ordinary Least Squares: Average",
    "text": "Ordinary Least Squares: Average\nAverage predictions over the target population\n\nols_estimate &lt;- ols_predicted |&gt;\n  summarize(ols_estimate = mean(predicted_salary)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n  ols_estimate\n         &lt;dbl&gt;\n1     5552999."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS",
    "text": "Performance of OLS\n\nWe usually have only one sample\nIn this case, we have the population\n\n\n\nPlan\n\ndraw repeated samples\nevaluate performance over repeated samples"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\nA function that\n\ntakes in a sample\nreturns an estimate"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-1",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-1",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\n\nols_estimator &lt;- function(\n    sample = baseball_sample, \n    to_predict = dodgers_to_predict\n) {\n  # Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team_past_record,\n    data = sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = to_predict))\n  # Average over the target population\n  ols_estimate_star &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate_star)\n}"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-2",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-2",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\nApply the estimator in repeated samples.\n\n\nCode\nmany_sample_estimates &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample\n  estimate &lt;- ols_estimator(baseball_sample)\n  return(estimate)\n}"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-3",
    "href": "slides/lec2/penalized_regression_slides.html#performance-of-ols-estimator-function-3",
    "title": "Algorithms for prediction",
    "section": "Performance of OLS: Estimator function",
    "text": "Performance of OLS: Estimator function\n\n\nCode\ntibble(y = many_sample_estimates) |&gt;\n  # Random jitter for x\n  mutate(x = runif(n(), -.1, .1)) |&gt;\n  ggplot(aes(x = x, y = many_sample_estimates)) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\",\n    labels = label_millions\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_continuous(\n    breaks = NULL, limits = c(-.5,.5),\n    name = \"Each dot is an OLS estimate\\nin one sample from the population\"\n  ) +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = -.5, y = true_dodger_mean, hjust = 0, vjust = -.5, label = \"True mean in\\nDodger subpopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#model-approximation-error",
    "href": "slides/lec2/penalized_regression_slides.html#model-approximation-error",
    "title": "Algorithms for prediction",
    "section": "Model approximation error",
    "text": "Model approximation error\n\n\nCode\npopulation_ols &lt;- lm(salary ~ team_past_record, data = baseball_population)\nforplot &lt;- baseball_population |&gt;\n  mutate(fitted = predict(population_ols)) |&gt;\n  group_by(team) |&gt;\n  summarize(\n    truth = mean(salary),\n    fitted = unique(fitted),\n    team_past_record = unique(team_past_record)\n  )\nforplot_dodgers &lt;- forplot |&gt; filter(team == \"L.A. Dodgers\")\nforplot |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = truth, color = team == \"L.A. Dodgers\")) +\n  geom_line(aes(y = fitted)) +\n  geom_segment(\n    data = forplot_dodgers,\n    aes(\n      yend = fitted - 3e5, y = truth + 3e5,\n    ), \n    arrow = arrow(length = unit(.05,\"in\"), ends = \"both\"), color = \"dodgerblue\"\n  ) +\n  geom_text(\n    data = forplot_dodgers,\n    aes(\n      x = team_past_record + .02, \n      y = .5 * (fitted + truth), \n      label = \"model\\napproximation\\nerror\"\n    ),\n    size = 2, color = \"dodgerblue\", hjust = 0\n  ) +\n  geom_text(\n    data = forplot_dodgers, \n    aes(y = truth, label = \"L.A.\\nDodgers\"),\n    color = \"dodgerblue\",\n    vjust = 1.8, size = 2\n  ) +\n  scale_color_manual(\n    values = c(\"gray\",\"dodgerblue\")\n  ) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(limits = c(.3,.8), name = \"Team Past Record: Proportion Wins in 2022\") +\n  theme_classic() +\n  theme(legend.position = \"none\", text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#model-approximation-error-1",
    "href": "slides/lec2/penalized_regression_slides.html#model-approximation-error-1",
    "title": "Algorithms for prediction",
    "section": "Model approximation error",
    "text": "Model approximation error\nHow to solve model approximation error? \nA model with more parameters \n\nols_team_categories &lt;- lm(\n  salary ~ team,\n  data = baseball_sample\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ols-with-many-parameters",
    "href": "slides/lec2/penalized_regression_slides.html#ols-with-many-parameters",
    "title": "Algorithms for prediction",
    "section": "OLS with many parameters",
    "text": "OLS with many parameters\n\n\nCode\n# Create many sample estimates with categorical teams\nmany_sample_estimates_categories &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ols &lt;- lm(\n    salary ~ team,\n    data = baseball_sample\n  )\n  # Predict for our target population\n  ols_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(ols, newdata = dodgers_to_predict))\n  # Average over the target population\n  ols_estimate &lt;- ols_predicted |&gt;\n    summarize(ols_estimate = mean(predicted_salary)) |&gt;\n    pull(ols_estimate)\n  # Return the estimate\n  return(ols_estimate)\n}\n# Visualize\ntibble(x = \"OLS linear in\\nteam past record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with categorical\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-1",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-1",
    "title": "Algorithms for prediction",
    "section": "Regularization",
    "text": "Regularization\nTwo estimators:\n\nnonparametric mean of 5 sampled players on each team\n(high variance)\n\nlinear prediction: linear fit on team past record\n(biased by model approximation error)\n\n\nWe might regularize the nonparametric toward the linear prediction"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-2",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-2",
    "title": "Algorithms for prediction",
    "section": "Regularization",
    "text": "Regularization\n\n\nCode\nestimates_by_w &lt;- foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n  tibble(\n    w = w_value,\n    estimate = (1 - w) * dodger_sample_mean + w * ols_estimate\n  )\n}\nestimates_by_w |&gt;\n  ggplot(aes(x = w, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n      mutate(\n        w = w + c(1,-1) * .13, \n        hjust = c(0,1),\n        label = c(\"Nonparametric Dodger Sample Mean\",\n                  \"Linear Prediction from OLS\")\n      ),\n    aes(label = label, hjust = hjust)\n  ) +\n  geom_segment(\n     data = estimates_by_w |&gt;\n      filter(w %in% c(0,1)) |&gt;\n       mutate(x = w + c(1,-1) * .12,\n              xend = w + c(1,-1) * .04),\n     aes(x = x, xend = xend),\n     arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  annotate(\n    geom = \"text\", x = .3, y = estimates_by_w$estimate[12],\n    label = \"Partial\\nPooling\\nEstimates\",\n    vjust = 1\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = c(.3,.4), \n    xend = c(.3,.55),\n    y = estimates_by_w$estimate[c(11,14)],\n    yend = estimates_by_w$estimate[c(8,14)],\n    arrow = arrow(length = unit(.08,\"in\"))\n  ) +\n  scale_x_continuous(\"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_y_continuous(\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    name = \"Dodger Mean Salary Estimates\",\n    expand = expansion(mult =.1)\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-how-much",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-how-much",
    "title": "Algorithms for prediction",
    "section": "Regularization: How much?",
    "text": "Regularization: How much?\n\nWant to minimize expected squared error.\n\\[\n\\text{Expected Squared Error}(\\hat\\mu_\\text{Dodgers}) = \\text{E}_S\\left(\\left(\\hat\\mu_\\text{Dodgers}(S) - \\mu_\\text{Dodgers}\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#regularization-how-much-1",
    "href": "slides/lec2/penalized_regression_slides.html#regularization-how-much-1",
    "title": "Algorithms for prediction",
    "section": "Regularization: How much?",
    "text": "Regularization: How much?\n\n\nCode\nrepeated_simulations &lt;- foreach(rep = 1:100, .combine = \"rbind\") %do% {\n  \n  a_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  \n  ols_fit &lt;- lm(salary ~ team_past_record, data = a_sample)\n  \n  ols_estimate &lt;- predict(\n    ols_fit, \n    newdata = baseball_population |&gt; \n      filter(team == \"L.A. Dodgers\") |&gt;\n      distinct(team_past_record)\n  )\n  \n  nonparametric_estimate &lt;- a_sample |&gt;\n    filter(team == \"L.A. Dodgers\") |&gt;\n    summarize(salary = mean(salary)) |&gt;\n    pull(salary)\n  \n  foreach(w_value = seq(0,1,.05), .combine = \"rbind\") %do% {\n    tibble(\n      w = w_value,\n      estimate = w * ols_estimate + (1 - w) * nonparametric_estimate\n    )\n  }\n}\n\naggregated &lt;- repeated_simulations |&gt;\n  group_by(w) |&gt;\n  summarize(mse = mean((estimate - true_dodger_mean) ^ 2)) |&gt;\n  mutate(best = mse == min(mse))\n\naggregated |&gt;\n  ggplot(aes(x = w, y = mse, color = best)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(name = \"Expected Squared Error\\nfor Dodger Mean Salary\") +\n  scale_x_continuous(name = \"Weight: Amount of Shrinkage Toward Linear Fit\") +\n  scale_color_manual(values = c(\"black\",\"dodgerblue\")) +\n  geom_vline(xintercept = c(0,1), linetype = \"dashed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", text = element_text(size = 18)) +\n  annotate(\n    geom = \"text\", \n    x = c(0.02,.98), \n    y = range(aggregated$mse),\n    hjust = c(0,1), vjust = c(0,1),\n    size = 3,\n    label = c(\n      \"Nonparametric estimator:\\nDodger mean salary\",\n      \"Model-based estimator:\\nOLS linear prediction\"\n    )\n  ) +\n  annotate(\n    geom = \"text\", \n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .2 * diff(range(aggregated$mse)),\n    vjust = -.1,\n    label = \"Best-Performing\\nEstimator\",\n    size = 3,\n    color = \"dodgerblue\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = aggregated$w[aggregated$best], \n    y = min(aggregated$mse) + .18 * diff(range(aggregated$mse)),\n    yend = min(aggregated$mse) + .05 * diff(range(aggregated$mse)),\n    arrow = arrow(length = unit(.08,\"in\")),\n    color = \"dodgerblue\"\n  )"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-idea",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-idea",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Idea",
    "text": "Multilevel models: Idea\n\nmany groups\na few observations per group\npartially pool between\n\ngroup-specific mean estimates (high variance)\na model pooled across groups (potentially biased)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-code",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-code",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Code",
    "text": "Multilevel models: Code\n\nlibrary(lme4)\n\n\nmultilevel &lt;- lmer(\n  salary ~ team_past_record + (1 | team), \n  data = baseball_sample\n)\n\nMake predictions\n\nmultilevel_predicted &lt;- dodgers_to_predict |&gt;\n  mutate(\n    fitted = predict(multilevel, newdata = dodgers_to_predict)\n  )"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Intuition",
    "text": "Multilevel models: Intuition\n\n\nCode\np &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel)[1], \n    slope = fixef(multilevel)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")\np"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition-1",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-intuition-1",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Intuition",
    "text": "Multilevel models: Intuition\n\n\nCode\nbigger_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 20) |&gt;\n  ungroup()\nmultilevel_big &lt;- lmer(formula(multilevel), data = bigger_sample)\nbigger_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(multilevel_big)\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = fixef(multilevel_big)[1], \n    slope = fixef(multilevel_big)[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    limits = layer_scales(p)$y$range$range\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Multilevel model on a sample of 20 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are multilevel model estimates.\\nDashed line is an unregularized fixed effect.\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-performance",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-performance",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: Performance",
    "text": "Multilevel models: Performance\n\n\nCode\nmany_sample_estimates_multilevel &lt;- foreach(\n  repetition = 1:100, .combine = \"c\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  multilevel &lt;- lmer(\n    salary ~ team_past_record + (1 | team),\n    data = baseball_sample\n  )\n  # Predict for our target population\n  mutilevel_predicted &lt;- dodgers_to_predict |&gt;\n    mutate(predicted_salary = predict(multilevel, newdata = dodgers_to_predict))\n  # Average over the target population\n  mutilevel_estimate &lt;- mutilevel_predicted |&gt;\n    summarize(mutilevel_estimate = mean(predicted_salary)) |&gt;\n    pull(mutilevel_estimate)\n  # Return the estimate\n  return(mutilevel_estimate)\n}\n# Visualize\ntibble(x = \"OLS with\\nlinear record\", y = many_sample_estimates) |&gt;\n  bind_rows(\n    tibble(x = \"OLS with\\nteam indicators\", y = many_sample_estimates_categories)\n  ) |&gt;\n  bind_rows(\n    tibble(x = \"Multilevel\\nmodel\", y = many_sample_estimates_multilevel)\n  ) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#multilevel-models-in-math",
    "href": "slides/lec2/penalized_regression_slides.html#multilevel-models-in-math",
    "title": "Algorithms for prediction",
    "section": "Multilevel models: In math",
    "text": "Multilevel models: In math\n\nPerson-level model. For player \\(i\\) on team \\(t\\),\n\\[\\begin{equation}\nY_{ti} \\sim \\text{Normal}\\left(\\mu_t, \\sigma^2\\right)\n\\end{equation}\\]\n\n\nGroup-level model. There is a model for \\(\\mu_t\\), mean salary on team \\(t\\).\n\\[\\mu_t \\sim \\text{Normal}(\\alpha + X_t\\beta, \\tau^2)\\] where \\(X_t\\) is team past record"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-1",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-1",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\nOften taught in data science classes\n\nmany coefficients\npenalize large coefficients\nminimize a loss function"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-2",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-2",
    "title": "Algorithms for prediction",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\]\nMinimize a loss function:\n\\[\n\\underbrace{\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t\\gamma_t^2}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-code",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-code",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Code",
    "text": "Ridge regression: Code\n\nlibrary(glmnet)\n\n\nridge &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose the ridge penalty (alpha = 0).\n  # Later, we will learn about the LASSO penalty (alpha = 1)\n  alpha = 0\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-estimates",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-estimates",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Estimates",
    "text": "Ridge regression: Estimates\n\n\nCode\np_ridge &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      ridge, \n      s = ridge$lambda[50],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(ridge, s = ridge$lambda[20])[1], \n    slope = coef(ridge, s = ridge$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_ridge"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage",
    "text": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  s = ridge$lambda[c(20,60,80)],\n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ncolnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\nbaseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  bind_cols(fitted) |&gt;\n  select(team, team_past_record, nonparametric, contains(\"Lambda\")) |&gt;\n  pivot_longer(cols = contains('Lambda')) |&gt;\n  distinct() |&gt;\n  mutate(name = fct_rev(name)) |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric), size = .8) +\n  geom_segment(\n    aes(y = nonparametric, yend = value),\n    arrow = arrow(length = unit(.04,\"in\"))\n  ) +\n  facet_wrap(~name) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"Ridge regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage-1",
    "href": "slides/lec2/penalized_regression_slides.html#ridge-regression-penalty-parameter-lambda-controls-shrinkage-1",
    "title": "Algorithms for prediction",
    "section": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage",
    "text": "Ridge regression: Penalty parameter \\(\\lambda\\) controls shrinkage\n\n\nCode\nfitted &lt;- predict(\n  ridge, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = ridge$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"Ridge Regression Penalty Term\",\n    limits = c(0,1e8)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = .85e7, xend = .2e7, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 1e7, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))\n\n\n\n\n\n\n\n\n\nWe will discuss how to choose the value of lambda more in two weeks when we discuss data-driven selection of an estimator.\nPerformance over repeated samples\nThe ridge regression estimator has intuitive performance across repeated samples: the variance of the estimates decreases as the value of the penalty parameter \\(\\lambda\\) rises. The biase of the estimates also increases as the value of \\(\\lambda\\) increases. The optimal value of \\(\\lambda\\) is a problem-specific question that requires one to balance the tradeoff between bias and variance.\n\n\nCode\nmany_sample_estimates_ridge &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  ridge &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 0\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    ridge, \n    s = ridge$lambda[c(20,60,80)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_ridge |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-1",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-1",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\nmultilevel &lt;- lmer(salary ~ 1 + (1 | team), data = baseball_sample)\n\n\n\nCode\nyhat_multilevel &lt;- baseball_sample |&gt;\n  mutate(yhat_multilevel = predict(multilevel)) |&gt;\n  distinct(team, yhat_multilevel)\nlambda_equivalent &lt;- as_tibble(VarCorr(multilevel)) |&gt;\n  select(grp, vcov) |&gt;\n  pivot_wider(names_from = \"grp\", values_from = \"vcov\") |&gt;\n  mutate(lambda_equivalent = Residual / team) |&gt;\n  pull(lambda_equivalent)\n\n\nI will also fit ridge regression with \\(\\lambda\\) set to 9.\n\n\nCode\nX &lt;- model.matrix(~ -1 + team, data = baseball_sample)\ny_centered &lt;- baseball_sample$salary - mean(baseball_sample$salary)\nbeta_ridge &lt;- solve(\n  t(X) %*% X + diag(rep(lambda_equivalent,ncol(X))), \n  t(X) %*% y_centered\n)\nyhat_ridge &lt;- beta_ridge + mean(baseball_sample$salary)\n\n\nFinally, we create a tibble with both the ridge regression and multilevel model estimates.\n\n\nCode\nboth_estimators &lt;- as_tibble(rownames_to_column(data.frame(yhat_ridge))) |&gt;\n  rename(team = rowname) |&gt;\n  mutate(team = str_remove(team,\"team\")) |&gt;\n  left_join(yhat_multilevel, by = join_by(team))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-2",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-2",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\n\nCode\np &lt;- both_estimators |&gt;\n  ggplot(aes(x = yhat_ridge, y = yhat_multilevel, label = team)) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_x_continuous(\n    name = \"Ridge Regression\",\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_y_continuous(\n    name = \"Multilevel Model\",,\n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  geom_point(alpha = 0) +\n  theme(text = element_text(size = 18))\np"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-3",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-3",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\n\nCode\np + \n  geom_point()"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-4",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-4",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\nMultilevel model:\n\\[\n\\begin{aligned}\nY_{ti} &\\sim \\text{Normal}(\\mu_t,\\sigma^2) \\\\\n\\mu_t &\\sim \\text{Normal}(\\mu_0, \\tau^2)\n\\end{aligned}\n\\] Estimates of \\(\\mu_t\\) given \\(\\hat\\mu_0\\), \\(\\hat\\tau^2\\), and \\(\\hat\\sigma^2\\):\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-5",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-5",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\underbrace{\\sum_t\\sum_i \\left(Y_{it}-\\mu_t\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t \\left(\\mu_t - \\hat\\mu_0\\right)^2}_\\text{Penalty}\n\\]\nRearranged to:\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-6",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-6",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\n\\[\n\\hat{\\vec\\mu}^\\text{Ridge} = \\underset{\\vec\\mu}{\\text{arg min}} \\sum_t\\left(\\sum_i \\left(Y_{it}-\\mu_t\\right)^2 + \\lambda\\left(\\hat\\mu_0 - \\mu_t\\right)^2\\right)\n\\]\n\nResult:\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-7",
    "href": "slides/lec2/penalized_regression_slides.html#connections-multilevel-model-and-ridge-regression-7",
    "title": "Algorithms for prediction",
    "section": "Connections: Multilevel model and ridge regression",
    "text": "Connections: Multilevel model and ridge regression\nMultilevel:\n\\[\n\\hat\\mu_t^\\text{Multilevel} = \\frac{\\sum_{i}Y_{ti} + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}\\hat\\mu_0}{n_t + \\frac{\\hat\\sigma^2}{\\hat\\tau^2}}\n\\]\nRidge:\n\\[\n\\hat\\mu_t^\\text{Ridge} = \\frac{\\sum_i Y_{it} + \\lambda\\hat\\mu_0}{n_t + \\lambda}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-regression",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-regression",
    "title": "Algorithms for prediction",
    "section": "LASSO regression",
    "text": "LASSO regression\nOutcome model similar to ridge:\n\\[\nY_{ti} = \\alpha + \\beta X_t + \\gamma_{t} + \\epsilon_{ti}\n\\]\nLoss function penalty has absolute value:\n\\[\n\\underbrace{\\sum_t\\sum_i\\left(Y_{ti} - \\left(\\alpha + \\beta X_t + \\gamma_{t}\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_t\\lvert\\gamma_t\\rvert}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-in-code",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-in-code",
    "title": "Algorithms for prediction",
    "section": "LASSO: In code",
    "text": "LASSO: In code\n\nlibrary(glmnet)\n\n\nlasso &lt;- glmnet(\n  x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n  y = baseball_sample$salary,\n  penalty.factor = c(0,rep(1,30)),\n  # Choose LASSO (alpha = 1) instead of ridge (alpha = 0)\n  alpha = 1\n)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates\n\n\nCode\np_lasso &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  mutate(nonparametric = mean(salary)) |&gt;\n  ungroup() |&gt;\n  mutate(\n    fitted = predict(\n      lasso, \n      s = lasso$lambda[20],\n      newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n    )\n  ) |&gt;\n  distinct(team, team_past_record, fitted, nonparametric) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = team_past_record)) +\n  geom_point(aes(y = nonparametric)) +\n  geom_abline(\n    intercept = coef(lasso, s = lasso$lambda[20])[1], \n    slope = coef(lasso, s = lasso$lambda[20])[2], \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    aes(y = nonparametric, yend = fitted),\n    arrow = arrow(length = unit(.05,\"in\"))\n  ) +\n  geom_point(aes(y = nonparametric)) +\n  #ggrepel::geom_text_repel(aes(y = fitted, label = team), size = 2) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_y_continuous(\n    name = \"Team Mean Salary in 2023\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  scale_x_continuous(name = \"Team Past Record: Proportion Wins in 2022\") +\n  ggtitle(\"LASSO regression on a sample of 5 players per team\",\n          subtitle = \"Points are nonparametric sample mean estimates.\\nArrow ends are ridge regression estimates.\\nDashed line is the unregularized component of the model.\")\np_lasso"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates-1",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates-1",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-estimates-2",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-estimates-2",
    "title": "Algorithms for prediction",
    "section": "LASSO estimates",
    "text": "LASSO estimates\n\n\nCode\nfitted &lt;- predict(\n  lasso, \n  newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n)\ndodgers_sample_mean &lt;- baseball_sample |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  summarize(salary = mean(salary)) |&gt;\n  pull(salary)\nols_estimate &lt;- predict(\n  lm(salary ~ team_past_record, data = baseball_sample),\n  newdata = baseball_sample |&gt; filter(team == \"L.A. Dodgers\") |&gt; slice_head(n = 1)\n)\ntibble(estimate = fitted[baseball_sample$team == \"L.A. Dodgers\",][1,],\n       lambda = lasso$lambda) |&gt;\n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary Estimate\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    ),\n    expand = expansion(mult = .2)\n  ) +\n  scale_x_continuous(\n    name = \"LASSO Regression Penalty Term\",\n    limits = c(0,2.5e6)\n  ) +\n  annotate(\n    geom = \"point\", x = 0, y = dodgers_sample_mean\n  ) +\n  annotate(geom = \"segment\", x = 2.5e5, xend = 1e5, y = dodgers_sample_mean,\n           arrow = arrow(length = unit(.05,\"in\"))) +\n  annotate(\n    geom = \"text\", x = 3e5, y = dodgers_sample_mean,\n    label = \"Mean salary of 5 sampled Dodgers\",\n    hjust = 0, size = 3\n  ) +\n  geom_hline(yintercept = ols_estimate, linetype = \"dashed\") +\n  annotate(\n    geom = \"text\",\n    x = 0, y = ols_estimate, \n    label = \"Dashed line = Prediction that does not allow any team-specific deviations\", \n    vjust = -.8, size = 3, hjust = 0\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18))"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#lasso-performance",
    "href": "slides/lec2/penalized_regression_slides.html#lasso-performance",
    "title": "Algorithms for prediction",
    "section": "LASSO: Performance",
    "text": "LASSO: Performance\n\n\nCode\nmany_sample_estimates_lasso &lt;- foreach(\n  repetition = 1:100, .combine = \"rbind\"\n) %do% {\n  # Draw a sample from the population\n  baseball_sample &lt;- baseball_population |&gt;\n    group_by(team) |&gt;\n    slice_sample(n = 5) |&gt;\n    ungroup()\n  # Apply the estimator to the sample# Learn a model in the sample\n  lasso &lt;- glmnet(\n    x = model.matrix(~ -1 + team_past_record + team, data = baseball_sample),\n    y = baseball_sample$salary,\n    penalty.factor = c(0,rep(1,30)),\n    alpha = 1\n  )\n  # Predict for our target population\n  fitted &lt;- predict(\n    lasso, \n    s = lasso$lambda[c(15,30,45)],\n    newx = model.matrix(~-1 + team_past_record + team, data = baseball_sample)\n  )\n  colnames(fitted) &lt;- c(\"Large Lambda Value\",\"Medium Lambda Value\",\"Small Lambda Value\")\n  \n  as_tibble(\n    as.matrix(fitted[baseball_sample$team == \"L.A. Dodgers\",][1,]),\n    rownames = \"estimator\"\n  ) |&gt;\n    rename(estimate = V1)\n}\n# Visualize\nmany_sample_estimates_lasso |&gt;\n  mutate(estimator = fct_rev(estimator)) |&gt;\n  ggplot(aes(x = estimator, y = estimate)) +\n  geom_jitter(width = .2, height = 0) +\n  scale_y_continuous(\n    name = \"Dodger Mean Salary\", \n    labels = label_currency(\n      scale = 1e-6, accuracy = .1, suffix = \" million\"\n    )\n  ) +\n  theme_minimal() +\n  theme(text = element_text(size = 18)) +\n  scale_x_discrete(\n    name = \"Estimator\"\n  ) +\n  ggtitle(NULL, subtitle = \"Each dot is an estimate in one sample from the population\") +\n  geom_hline(yintercept = true_dodger_mean, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = .4, y = true_dodger_mean, hjust = 0, vjust = .5, label = \"Truth in\\npopulation\", size = 3)"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap",
    "href": "slides/lec2/penalized_regression_slides.html#recap",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\nA recipe for prediction:\n\nmodel in learning data\npredict in target population\naverage"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap-1",
    "href": "slides/lec2/penalized_regression_slides.html#recap-1",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\nPerformance (expected squared error) can suffer from two sources:\n\nbias (model approximation error)\nvariance (too many parameters)\n\n\nRegularization balances these two concerns"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#recap-2",
    "href": "slides/lec2/penalized_regression_slides.html#recap-2",
    "title": "Algorithms for prediction",
    "section": "Recap",
    "text": "Recap\n\nOLS: No regularization\nRidge regression: Penalizes sum of squared coefficients\nLASSO regression: Penalizes sum of absolute coefficients\n\n\nMultilevel models regularize group-specific estimates depending on the within-group precision, a special case of ridge regression."
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nRidge regression: Applies well to many settings with a large number of parameters, regardless of whether they are groups.\n\\[\n\\hat{\\vec\\beta}^\\text{Ridge} = \\underset{\\vec\\beta}{\\text{arg min}} \\underbrace{\\sum_i \\left(Y_i - \\left(\\alpha + \\vec{X}'\\vec\\beta\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_p \\beta_p^2}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-1",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-1",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nLASSO regression: Applies well to many settings with a large number of parameters, regardless of whether they are groups.\n\\[\n\\hat{\\vec\\beta}^\\text{Ridge} = \\underset{\\vec\\beta}{\\text{arg min}} \\underbrace{\\sum_i \\left(Y_i - \\left(\\alpha + \\vec{X}'\\vec\\beta\\right)\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda\\sum_p \\lvert\\beta_p\\rvert}_\\text{Penalty}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-2",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-2",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nMultilevel models can have group-specific slopes\n\\[\n\\begin{aligned}\nY_{gi} &\\sim \\text{Normal}\\left(\\alpha_g + \\beta_g X_{gi}, \\sigma^2\\right) \\\\\n\\alpha_g &\\sim \\text{Normal}\\left(\\eta_0, \\tau^2\\right) \\\\\n\\beta_g &\\sim \\text{Normal}\\left(\\lambda_0, \\delta^2\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-3",
    "href": "slides/lec2/penalized_regression_slides.html#generalizations-of-our-estimators-3",
    "title": "Algorithms for prediction",
    "section": "Generalizations of our estimators",
    "text": "Generalizations of our estimators\nMultilevel models can have more than two levels\n\\[\n\\begin{aligned}\nY_{ijk} &\\sim \\text{Normal}\\left(\\alpha_{ij}, \\sigma^2\\right) \\\\\n\\alpha_{ij} &\\sim \\text{Normal}\\left(\\beta_{i}, \\tau^2\\right) \\\\\n\\beta_{i} &\\sim \\text{Normal}\\left(\\lambda, \\delta^2\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#reading-optional",
    "href": "slides/lec2/penalized_regression_slides.html#reading-optional",
    "title": "Algorithms for prediction",
    "section": "Reading (optional)",
    "text": "Reading (optional)\nMultilevel models\n\nGelman & Hill 2006 for worked examples\nMurphy 2012 ch 5.6.2 for a succinct mathematical presentation"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#reading-optional-1",
    "href": "slides/lec2/penalized_regression_slides.html#reading-optional-1",
    "title": "Algorithms for prediction",
    "section": "Reading (optional)",
    "text": "Reading (optional)\nRidge & LASSO: Hastie, Tibshirani, & Friedman 2017 Ch 3.4\nRidge regression\n\nEfron & Hastie 2016 Ch 7.3 (all of ch 7 is useful)\nMurphy 2012 ch 7.5 for a Bayes view with lots of math\n\nLASSO regression\n\nEfron & Hastie 2016 Ch 16"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#connection-to-project",
    "href": "slides/lec2/penalized_regression_slides.html#connection-to-project",
    "title": "Algorithms for prediction",
    "section": "Connection to project",
    "text": "Connection to project\nIn your project, do you have a variable that creates\n\nmany groups with\nfew observations per group?\n\nWhere else do you have many parameters to estimate?\nHow could penalized regression apply in your project?"
  },
  {
    "objectID": "slides/lec2/penalized_regression_slides.html#problem-set-2",
    "href": "slides/lec2/penalized_regression_slides.html#problem-set-2",
    "title": "Algorithms for prediction",
    "section": "Problem Set 2",
    "text": "Problem Set 2\nDue Monday 9pm: link"
  }
]